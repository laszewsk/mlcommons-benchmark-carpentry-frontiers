\section{Review of Benchmark Related to this Effort}
\label{sec:benchmarks}

This section provides an overview of key benchmarking efforts that motivated our paper. We start with HPC benchmarks and also address MLCommons benchmark efforts.

\subsection{HPC Benchmarking}
\label{sec:benchmarks-hpc}
\label{sec:hpc}

HPC benchmarking has a great impact on the activities that we report here and we can learn a lot from these efforts. Some of the most known efforts are TOP500 and Green500.


\subsubsection{TOP500}
\label{sec:benchmarks-hpc-top500}

The list of world's largest supercomputers has been released biannually
for nearly 4 decades now and thus offers a number of important lessons
in designing sustainable benchmarks. At the heart of the TOP500 scoring procedure, which yields a ranked list of 500 supercomputing installations, is the LINPACK benchmark~\cite{dongarra2003hpl}, which bears the name of the
namesake software library~\cite{dongarra1979linpack} for
solving systems of linear equations. This linear solver package was
designed in the 1970s and implemented in FORTRAN. The user guide for the
library was published in 1979 and included a list of only 24
computers~\cite{dongarra1979linpack}. The following decades brought in various aspects of scaling into the
software, the list sizes, and the machines submitted for inclusion in the ranking as well as data and reporting information.

\subsubsection{Green500}
\label{sec:benchmarks-hpc-green500}

Power and energy play a dominant role in the modern world of high-performance and distributed computing, with multi-megawatt data centers and computing facilities abound in many locations across the globe.
The issues of excessive power draw and energy consumption
data in the mid-2000s~\cite{feng2005pwrprofsciapps,
cameron2005hpcpowerdistcompsciapps} culminated in a special working
group of cross-industry members~\cite{specpower2008, specpower}, combining the TOP500 ranking with the available power
draw information from the supercomputers to yield the ranking
called Green500~\cite{green500}. Since then, it is published
alongside the TOP500 ranking and continues to underscore the
importance of efficient energy use at large HPC installations.

\subsubsection{HPC innovation}
\label{sec:benchmarks-hpc-innov}

Besides the recognition of development of tools and software to
facilitate the use of HPC systems and foster democratization, power
consumption monitoring has been integrated at the various levels of HPC
facilities, from the processing and networking elements to the data
center level infrastructure. Also, by utilizing different floating-point
precisions~\cite{abdelfattah2021mxpsurvey} the applications improve
their efficiency and benefit from a great impact on the system
performance due to direct targeting of the specific architectural
designs.

The creation of leaderboards has led to a better understanding of the overall HPC system, but insights can be limited by misalignment of algorithm scaling and leaderboard projections.
To counter misalignment, benchmarks should closely resemble the scientific task to be benchmarked.
In some cases, it is informative to include end-to-end performance, including data storage limitations.



\subsection{Machine Learning Benchmarks}
\label{sec:ai-benchmakrs}
\label{sec:benchmarks-mlcommons}

Benchmarking in scientific machine learning (ML) has emerged as a critical area to guide algorithm development, enable fair comparisons towards progress and innovation, and facilitate reproducibility. The development of ML benchmarks for science is especially critical because of the multi-disciplinary nature of the development, often including domain experts, computing hardware developers, and ML researchers.  That, coupled with the variety of tasks and workloads, makes {\it high quality} benchmarking critical to making progress. 

To obtain an overview how many academic benchmarks have been published in well known public domain archives, so we queried arXiv~\cite{www-arXiv} and Google Scholar~\cite{www-google-scholar}. Note that according to Google, Google Scholar does not include all entries from arXiv, but it does include most of them. However, it also includes many more resources, so we expect a larger number from Google Scholer. As of Oct 1, 2025, we find 106 entries on arXiv when searching for the topic {\em ``AI benchmark''}.
executing equivalent queries in Google Scholar yields 2,490 entries for {\em ``AI benchmark''}. It is evident from this that a complete survey of these papers is difficult to achieve through manual inspection. In an upcoming effort, we plan to explore how to automatically categorize these entries using LLMs while implementing an agentic AI framework for it.

The vast number and diversity of scientific tasks poses challenges to finding a well-defined, high-quality benchmark for any given task.
To improve discoverability, we have cataloged in this paper all MLCommons benchmarks that have a result submission.
Secondly, we have developed an ontology \cite{www-las-mlcommons-benchmark-collection,www-mlcommons-science-benchmarks-paper} that allows users to identify suitable benchmarks.

\subsubsection{MLCommons}

MLCommons \cite{www-mlcommons} provides one of the most comprehensive and standardized ecosystems of AI benchmarking. It addresses training, inference, scientific computing, and domain-specific benchmarks. Most prominently, the MLPerf benchmark suite—covering datacenter, edge, mobile, and training applications—establishes industry-wide baselines for performance, accuracy, power efficiency, and quality of service across diverse model classes such as computer vision, language, recommendation, speech, and reinforcement learning. Additionally, it offers specialized evaluations including MLPerf Tiny for microcontroller-class devices, MLPerf Storage for I/O workloads, and MLPerf Science for large-scale scientific AI. Furthermore, MLCommons promotes the reproducibility through initiatives such as Croissant ML, a standardized metadata schema for datasets, and MLCube, a portable container-based model packaging standard. Additional domain-specific working groups in medical AI, multilingual speech, and responsible AI have recently expanded the targeted domains. 

We have provided a comprehensive list of benchmarks in Tables \ref{tab:benchmarks-mlcommons} and \ref{tab:llm_benchmarks_long}.  The tables contain information about the benchmark name, model, task, domain, model type, metrics, hardware, and a brief note. The evaluations of the AILuminate benchmarks can be found on the MLCommons Web pages and include (a) Safety / Jailbreak Tests, (b) LLM Safety Evaluation, (c)  Responsible AI / Alignment (d)  LLM (Decoder) (e)  Safety Rate, Toxicity Score (f) Cloud LLM APIs (g) Robustness and Alignment.

\subsubsection{Ontology}

To improve discoverability of suitable benchmarks for a given task, we introduce a definition and AI Benchmark ontology of scientific machine learning benchmarks, where benchmarks are classified and mapped to their scientific domain and machine learning task type in~\cite{www-mlcommons-science-benchmarks-paper}. This work grew out of the Web page created at  \cite{www-las-mlcommons-benchmark-collection}, \cite{www-mlcommons-benchmarks} and provides an easy to use interactive mechanism to query the cataloged benchmarks.

New AI benchmarks are added through an open submission workflow overseen by the MLCommons Science Working Group. Each submission is evaluated against a rubric of currently six categories (Software Environment, Problem Specification, Dataset, Performance Metrics, Reference Solution, Documentation) that assigns an overall rating and potential endorsement. The scoring framework enables stakeholders, researchers, domain scientists, and hardware vendors to identify representative subsets of benchmarks that align with their specific priorities. The ontology supports adding new scientific domains, AI/ML motifs, and computing motifs.   

A subset of information collected by the Web page is shown in Table \ref{tab:ontology}. It not only includes some elementary information about the benchmarks but also a perceived rating displayed as a radar chart. Such radar charts include ratings from 1-5, where 5 is the best rating. Ratings are identified for documentation, specification, software, metrics, dataset, and reference solution. The Web page not only includes an automatically generated report of all benchmarks in PDF format, but also a convenient online publication of the benchmarks with convenient search capabilities.

\begin{comment}
To initiate this effort, we issued queries to ChatGPT and obtained an initial list of benchmarks, as shown in Table \ref{tab:scientific_ai_benchmarks}. In addition, we identified several benchmarks based on the interests of members of the MLCommons Science and HPC Working Groups. This includes a categorization of MLCommons benchmarks according to working group interests.
\end{comment}

\input{0055-table-mlcommons-5}

%\input{0053-table-domains-3}
%ML Benchmarks LaTeX Table \ref{tab:ml_benchmarks}


%\input{ontology/selected}
\input{ontology/selected-new}



% \input{0056-x-table-mlcommons-6}

\subsection{Technical aspects of AI Benchmarks}

In addition to discoverability challenges, there are also technical issues that need to be addressed in dealing with democratization and AI benchmark carpentry.


\subsubsection{Workflows}
\label{sec:benchmarks-mlcommons-desktop}

There are many workflow frameworks that can support the AI Benchmark Workflow. Two of them 
are the Compute Coordinator and the Experiment Executer; they can be used in conjunction or separately \cite{las-2022-templated}. The Compute Coordinator allows hybrid infrastructure access from the benchmark application, while the Experiment Executor allows the repeated execution of templated benchmarks. Both produce results in a structured fashion so they can be combined from multiple experiments and multiple infrastructures in order to support the FAIR principles.

\subsubsection{Containerization}
\label{sec:benchmarks-mlcommons-hpc}

Benchmarking on HPC and even smaller machines can be simplified by providing containerized environments which not only enable easy deployment, but also can harmonize execution by providing stable operating system and software environments. In addition to portable makefiles, the uniform generation of containers can be leveraged between applications.
Although docker is today widely used to containerize applications, on HPC systems we find that limited root access on many HPC systems led to the development of apptainers.
Hence, AI benchmarking carpentry should include the development of software in apptainers directly or converting Docker containers to apptainers.

\subsubsection{System-Dependent Software and Deployment Variability}

Benchmarking can be complex if the software, libraries and infrastructure differ across systems.
To support coordinated  benchmarking across different machines, we have introduced a 
templates hybrid reusable computational analytics workflow management framework with cloudmesh. This framework has been be applied to multiple Deep Learning MLCommons Applications. The details are explained in  \cite{las-2022-templated}. Utilizing such workflow systems promotes adaptation as deployment and execution is typically included in the workflow specifications. However, it can also address adaptation and modifications to future improvements and porting to different hardware as a working template is already provided..
    
\subsubsection{Logging and Monitoring}
\label{sec:benchmarks-mlcommons-logging}

A variety of logging frameworks exist for AI Benchmark logging. This includes logging tools such as MLPerf logging. While such tools provide elementary logging features, their outputs are not human readable and require post processing. This is also an issue when running applications in interactive mode during debugging phases. For this reason, we have provided Cloudmesh-stopwatch that not only allows human readable format, but also allows automatic MLPerf logging (if desired) with a single line change in the code. Cloudmesh stopwatch supports Python, shell, and batch script execution, and employs a consistent log format across all three. 

In general, we distinguish between four types of monitoring: (a) Infrastructure Monitoring, (b) Application  Monitoring, (c) Training Monitoring, and (d) Model-Level Monitoring. A wide range of tools exists for each type, making it essential to identify those that provide effective functionality while remaining easy to use. TensorBoard is one example.
