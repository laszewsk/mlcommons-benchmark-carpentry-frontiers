@misc{papatheodore2022summitfrontier,
  author       = {Tom Papatheodore},
  title        = {{OLCF: From Summit to Frontier}},
  howpublished = {{Presentation (Track 1, Talk 2) at {\em ATPESC 2022}}},
  month        = aug,
  year         = 2022,
  url={https://extremecomputingtraining.anl.gov/wp-content/uploads/sites/96/2022/11/ATPESC-2022-Track-1-Talk-2-Papatheodore-Summit-and-Frontier.pdf},
}

@article{brown2020language,
  title={{Language Models are Few-Shot Learners}}, 
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year={2020},
  eprint={2005.14165},
  howpublished={arXiv},
  primaryClass={cs.CL},
  journal={arXiv preprint arXiv:2005.14165},
  url={https://arxiv.org/abs/2005.14165}, 
}

@article{patterson2021carbon,
  title={{The Carbon Footprint of Large Neural Network Training}},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc V. and Liang, Chen and Munguia, Lluis and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021},
  url={https://arxiv.org/abs/2104.10350}
}

@misc{baeldung2023energy,
  title={{How Much Energy Does ChatGPT Use?}},
  author={{Baeldung Editors}},
  year={2023},
  url={https://www.baeldung.com/cs/chatgpt-large-language-models-power-consumption}
}

@misc{jegham2025hungryaibenchmarkingenergy,
  title={How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference}, 
  author={Jegham, Nidhal and Abdelatti, Marwan and Koh, Chan Young and Elmoubarki, Lassad and Hendawi, Abdeltawab},
  year={2025},
  eprint={2505.09598},
  archivePrefix={arXiv},
  primaryClass={cs.CY},
  url={https://arxiv.org/abs/2505.09598}
}

@misc{medium2023gpt4carbon,
  title={{The Carbon Footprint of GPT-4}},
  author={{Data Science on Medium}},
  year={2023},
  url={https://medium.com/data-science/the-carbon-footprint-of-gpt-4-d6c676eb21ae}
}

@misc{extremenetworks2023energy,
  title={{Confronting AI’s Growing Energy Appetite: Part 1}},
  author={{Extreme Networks}},
  year={2023},
  url={https://www.extremenetworks.com/resources/blogs/confronting-ai-growing-energy-appetite-part-1}
}

@misc{sciencefeedback2024energy,
  title={{Training and Using ChatGPT Uses a Lot of Energy, but Exact Numbers Are Tricky to Pin Down Without Data from OpenAI}},
  author={{Science Feedback}},
  year={2024},
  url={https://science.feedback.org/training-and-using-chatgpt-uses-a-lot-of-energy-but-exact-numbers-are-tricky-to-pin-down-without-data-from-openai}
}

@misc{epochai2024compute,
  title={{Why GPT-5 Used Less Training Compute Than GPT-4.5 (But GPT-6 Probably Won’t)}},
  author={{Epoch AI}},
  year={2024},
  url={https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont}
}

@misc{hackernoon2024dirtysecret,
  title={{AI’s Dirty Secret: The Energy Cost of Training the Next GPT-5}},
  author={Hackernoon Editors},
  year={2024},
  url={https://hackernoon.com/ais-dirty-secret-the-energy-cost-of-training-the-next-gpt-5}
}

@article{kaplan2020scaling,
  title={{Scaling Laws for Neural Language Models}},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020},
  url={https://arxiv.org/abs/2001.08361}
}

%------------------------------------------------------------
@inproceedings{Scogland11Green500,
  author    = {Scogland, Thomas R. W. and Subramaniam, Balaji and Feng, Wu{-}Chun},
  title     = {{Emerging Trends on the Evolving Green500: Year Three}},
  booktitle = {Proc.\ IEEE IPDPS Workshops},
  year      = {2011},
  pages     = {889--895},
  doi       = {10.1109/IPDPS.2011.229}
}

@misc{NVIDIA23Blog,
  author       = {{NVIDIA Corporation}},
  title        = {Energy Efficiency in High-Performance Computing:
                  Balancing Speed and Sustainability},
  howpublished = {\url{https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/}},
  year         = {2023}
}

@INPROCEEDINGS{Tschand24MLPerfPower,
  author={Tschand, Arya and Rajan, Arun Tejusve Raghunath and Idgunji, Sachin and Ghosh, Anirban and Holleman, Jeremy and Kiraly, Csaba and Ambalkar, Pawan and Borkar, Ritika and Chukka, Ramesh and Cockrell, Trevor and Curtis, Oliver and Fursin, Grigori and Hodak, Miro and Kassa, Hiwot and Lokhmotov, Anton and Miskovic, Dejan and Pan, Yuechao and Manmathan, Manu Prasad and Raymond, Liz and John, Tom St. and Suresh, Arjun and Taubitz, Rowan and Zhan, Sean and Wasson, Scott and Kanter, David and Reddi, Vijay Janapa},
  booktitle={{IEEE International Symposium on High Performance Computer Architecture}}, 
  series = {HPCA},
  title={{MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from $\mu$Watts to MWatts for Sustainable AI}},
  year={2025},
  volume={},
  number={},
  pages={1201-1216},
  keywords={Performance evaluation;Power demand;Power measurement;Standards organizations;Machine learning;Benchmark testing;Energy efficiency;Surges;Optimization;System analysis and design;mlperf;energy efficiency;sustainable ai;machine learning;computer architecture},
  doi={10.1109/HPCA61900.2025.00092}
}

@inproceedings{Peon23A100PowerCap,
  author    = {Dan Zhao and Siddharth Samsi and Joseph McDonald and
               Baolin Li and David Bestor and Michael Jones and
               Devesh Tiwari and Vijay Gadepally},
  title     = {Sustainable Supercomputing for {AI}: {GPU} Power Capping at {HPC} Scale},
  booktitle = {Proceedings of the Energy-Efficient {HPC} and {AI} Workshop at the International Conference for High Performance Computing, Networking, Storage and Analysis (SC~’23)},
  year      = {2023},
  doi       = {10.48550/arXiv.2402.18593},
  url       = {https://arxiv.org/abs/2402.18593},
  note      = {Demonstrates that capping NVIDIA A100 GPUs to 300\,W can
               cut energy by double-digit percentages with \textless1\,\%
               throughput loss.}
}
@techreport{Koomey21HyperscaleCost,
  author      = {Jonathan G.\ Koomey and Sarah E.\ Taylor},
  title       = {Cost Drivers for Hyperscale Data Centres:
                 Implications for Efficiency and Sustainability},
  institution = {Koomey Analytics},
  year        = {2021},
  type        = {White Paper},
  url         = {https://koomey.com/wp-content/uploads/2021/10/Koomey-Taylor-2021-Hyperscale-Cost-Drivers.pdf},
  note        = {Quantifies how a $\pm$10\;¢\,kWh$^{-1}$ electricity swing
                 alters the ROI of cooling technologies and renewable PPAs.}
}
@techreport{Freina24EnergySurvey,
  author      = {David Freina and Matthijs Jansen and Animesh Trivedi},
  title       = {A Survey of Energy Measurement Methodologies for Computer Systems},
  institution = {Vrije Universiteit Amsterdam},
  year        = {2024},
  type        = {Technical Report},
  url         = {https://atlarge-research.com/pdfs/2024-dfreina-litsurvey.pdf},
  note        = {Comprehensive taxonomy of hardware- and software-based
                 energy-measurement solutions, including accuracy limits
                 and calibration practices.}
}
@INPROCEEDINGS{Wang2010HPC-PUE,
  author={Wang, Lizhe and von Laszewski, Gregor and Dayal, Jay and Wang, Fugang},
  booktitle={2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing}, 
  title={Towards Energy Aware Scheduling for Precedence Constrained Parallel Tasks in a Cluster with DVFS}, 
  year={2010},
  volume={},
  number={},
  pages={368-377},
  keywords={Energy consumption;Processor scheduling;High performance computing;Dynamic voltage scaling;Power engineering computing;Frequency;Computational modeling;Grid computing;Concurrent computing;Costs;Cluster Computing;Green Computing;Task Scheduling},
  doi={10.1109/CCGRID.2010.19}}


%-----------------------------------------------------------------------



@misc{DOE_Frontier_Power2023,
  author   = {U.S. Department of Energy},
  title    = {Frontier {Power \& Cooling} Factsheet},
  year     = 2023,
  note     = {\url{https://www.olcf.ornl.gov/frontier-power}}
}


@misc{EIA_Electricity_Price_2025,
  author   = {U.S. Energy Information Administration},
  title    = {Average Price of Electricity to Ultimate Customers},
  year     = 2025,
  howpublished = {\url{https://www.eia.gov/electricity/monthly/}}
}

@misc{eia2024residential,
  title={Average Annual Electricity Consumption for U.S. Residential Customers},
  author={{U.S. Energy Information Administration}},
  year={2024},
  howpublished={Available at https://www.eia.gov/}
}

@misc{Eurostat_GHG_2024,
  author   = {Eurostat},
  title    = {Greenhouse Gas Emissions per Capita},
  year     = 2024,
  note     = {\url{https://ec.europa.eu/eurostat/statistics-explained/}}
}

@misc{Google_PUE_2023,
  author   = {Google LLC},
  title    = {Data Centre PUE Metrics (2023)},
  year     = 2023,
  note     = {\url{https://sustainability.google/reports/}}
}

@misc{Google_Sustainability_2024,
  author   = {Google LLC},
  title    = {2024 Environmental Report},
  year     = 2024,
  note     = {\url{https://sustainability.google/}}
}

@misc{Google_Geothermal_2023,
  author   = {Google LLC},
  title    = {Bringing 24/7 Geothermal to the Grid},
  year     = 2023,
  note     = {\url{https://www.blog.google/outreach-initiatives/sustainability/}}
}

@inproceedings{ING_SCALE_2024,
  author   = {van den Berg, J. and colleagues},
  title    = {SCALE: Carbon-Aware Scheduling for {Kubernetes} in the Cloud},
  booktitle= {Proc.\ ACM e-Energy},
  year     = 2024
}

@inproceedings{GREEN_Slurm_2025,
  author   = {Chen, L. and colleagues},
  title    = {GREEN: Carbon-Aware Job Placement for {Slurm} Clusters},
  booktitle= {USENIX NSDI},
  year     = 2025
}

@article{luccioni2023estimating,
  title={Estimating the carbon footprint of bloom, a 176b parameter language model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={Journal of machine learning research},
  volume={24},
  number={253},
  pages={1--15},
  year={2023}
}

@inproceedings{EU2019_424,
  author       = {European Commission},
  title     = {Commission Regulation (EU) 2019/424 of 15 March 2019 Laying Down Ecodesign Requirements for Servers and Data Storage Products},
  booktitle = {Official Journal of the European Union},
  year      = {2019},
  note      = {\url{https://eur-lex.europa.eu/eli/reg/2019/424/oj}}
}
@misc{EU_Lot9_Guidance,
  author       = {European Commission},
  title        = {Guidance Document on the Ecodesign Requirements for Servers and Data Storage Products (Lot 9)},
  howpublished = {\url{https://data.europa.eu/doi/10.2873/782959}},
  year         = {2021},
  note         = {Sections 3–4 detail conformity‐assessment reports}
}