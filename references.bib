@misc{carbontracker,
  howpublished =  {\url{https://github.com/lfwa/carbontracker}},
  note =          {Energy and CO2 prediction for deep‑learning
                   training},
  title =         {CarbonTracker},
  year =          {2025},
}

@misc{powerpackmontbl,
  howpublished =  {\url{https://gitlab.bsc.es/mont-blanc/PowerPACK}},
  note =          {Energy \& power profiling toolkit for MPI/OpenMP
                   mini-apps (Joules, Watts)},
  title =         {PowerPACK / Mont-Blanc},
  year =          {2025},
}

@misc{craypatenergyco,
  howpublished =
  {\url{https://support.hpe.com/hpesc/public/docDisplay?docId=a00111513en\_us}},
  note =          {Integrated energy-per-function profiling in HPE/Cray
                   Performance Analysis Tool},
  title =         {Cray PAT Energy Counters},
  year =          {2025},
}

@misc{ibmpowerapipmli,
  howpublished =  {\url{https://github.com/IBM/powerapi}},
  note =          {System \& per-process kWh reporting on Power-based
                   supercomputers},
  title =         {IBM PowerAPI (pmlib)},
  year =          {2025},
}

@misc{nvidiadcgmenerg,
  howpublished =  {\url{https://developer.nvidia.com/dcgm}},
  note =          {GPU Joules \& Watts via Data Center GPU Manager;
                   attachable to HPC benchmarks},
  title =         {NVIDIA DCGM Energy},
  year =          {2025},
}

@misc{intelvtunepower,
  howpublished =
  {\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html}},
  note =          {Package Watts \& energy per function for MPI/OpenMP
                   codes},
  title =         {Intel VTune Power Analysis},
  year =          {2025},
}

@misc{cloudmesh-gpu,
  author =        {von Laszewski, Gregor},
  month =         feb,
  note =          {[Online; accessed 2025-11-26]},
  title =         {Cloudmesh GPU Monitor},
  year =          {2022},
  url =           {https://github.com/cloudmesh/cloudmesh-gpu},
}

@misc{www-las-mlcommons-benchmark-coolection,
  author =        {Gregor von Laszewski and Ben Hawks and Marco Colombo and
                   Reece Shiraishi and Anjay Krishnan and Nhan Tran and
                   Geoffrey C. Fox},
  howpublished =  {GitHub},
  month =         jun,
  note =          {Online Collection:
                       \url={https://mlcommons-science.github.io/benchmark/}},
  title =         {MLCommons Science Working Group AI Benchmarks
                   Collection},
  year =          {2025},
  url =           {https://mlcommons-science.github.io/benchmark/
                  benchmarks.pdf},
}

@misc{trillion-parameter-consortium,
  author =        {{DOE}},
  month =         aug,
  note =          {[Online; accessed 2025-11-30]},
  title =         {Trillion Parameter Consortium},
  year =          {2023},
  url =           {https://tpc.dev/},
}

@misc{wikipedia:benchmarking,
  author =        {Wikipedia},
  month =         {6},
  note =          {[Online; accessed 2025-09-23]},
  title =         {Benchmark (computing)},
  year =          {2005},
  url =           {https://en.wikipedia.org/wiki/Benchmark_%28computing%29},
}

@misc{PerfKitBenchmarker,
  author =        {{Google Cloud Platform} and contributors},
  howpublished =  {GitHub},
  title =         {PerfKitBenchmarker},
  year =          {2025},
  url =           {https://github.com/GoogleCloudPlatform/PerfKitBenchmarker},
}

@misc{IO500,
  author =        {IO500 Steering Committee},
  howpublished =  {Web Page},
  title =         {IO500: A Benchmarking Suite for HPC Storage I/O
                   Performance},
  year =          {2025},
  url =           {https://io500.org},
}

@misc{softwarecarpentry2024,
  author =        {{Software Carpentry}},
  howpublished =  {\url{https://software-carpentry.org/}},
  note =          {Accessed: 2025-05-28},
  title =         {Software Carpentry},
  year =          {2024},
}

@misc{datacarpentry2025,
  author =        {{The Carpentries}},
  howpublished =  {\url{https://datacarpentry.org}},
  note =          {Accessed: 2025-10-23},
  title =         {Data Carpentry},
  year =          {2025},
}

@misc{HPCcarpentry2025,
  author =        {{The Carpentries / HPC Carpentry community}},
  howpublished =  {\url{https://hpc-carpentry.org}},
  note =          {Accessed: 2025-10-23},
  title =         {HPC Carpentry},
  year =          {2025},
}

@misc{specpower2008,
  author =        {SPEC},
  title =         {The {SPEC} Power Benchmark},
  year =          {2008},
  url =           {www.spec.org/power_ssj2008/},
}

@misc{specpower,
  howpublished =  {\url{https://spec.org/power\_ssj2008/}},
  note =          {Watts per transaction and operations per Watt for
                   enterprise servers},
  title =         {SPECpower\_ssj2008},
  year =          {2025},
}

@misc{www-arXiv,
  author =        {{Cornell University}},
  month =         oct,
  note =          {[Online; accessed 2025-10-01]},
  title =         {arXiv.org e-Print archive},
  year =          {2025},
  url =           {https://arxiv.org/},
}

@misc{www-google-scholar,
  key =           {Google scholar},
  month =         oct,
  note =          {[Online; accessed 2025-10-01]},
  title =         {Google Scholar},
  year =          {2025},
  url =           {https://scholar.google.com/},
}

@misc{www-las-mlcommons-benchmark-collection,
  author =        {Gregor von Laszewski and Ben Hawks and Marco Colombo and
                   Reece Shiraishi and Anjay Krishnan and Nhan Tran and
                   Geoffrey C. Fox},
  howpublished =  {GitHub},
  month =         jun,
  note =          {Online Collection:
                       \url={https://mlcommons-science.github.io/benchmark/}},
  title =         {MLCommons Science Working Group AI Benchmarks
                   Collection},
  year =          {2025},
  url =           {https://mlcommons-science.github.io/benchmark/
                  benchmarks.pdf},
}

@misc{www-mlcommons-science-benchmarks-paper,
  author =        {Ben Hawks and Gregor von Laszewski and
                   Matthew D. Sinclair and Marco Colombo and
                   Shivaram Venkataraman and Rutwik Jain and Yiwei Jiang and
                   Nhan Tran and Geoffrey Fox},
  howpublished =  {arXiv},
  title =         {{An MLCommons Scientific Benchmarks Ontology}},
  year =          {2025},
  url =           {https://arxiv.org/abs/2511.05614},
}

@misc{www-mlcommons-benchmarks,
  author =        {von Laszewski, Gregor and Nhan Tran and {others}},
  month =         oct,
  note =          {[Online; accessed 2025-10-01]},
  title =         {mlcommons-science/benchmark},
  year =          {2025},
  url =           {https://github.com/mlcommons-science/benchmark},
}

@misc{nguyen2023climatelearnbenchmarkingmachinelearning,
  author =        {Tung Nguyen and Jason Jewik and Hritik Bansal and
                   Prakhar Sharma and Aditya Grover},
  howpublished =  {arXiv},
  title =         {ClimateLearn: Benchmarking Machine Learning for
                   Weather and Climate Modeling},
  year =          {2023},
  url =           {https://arxiv.org/abs/2307.01909},
}

@misc{allenai:arc,
  author =        {Peter Clark and Isaac Cowhey and Oren Etzioni and
                   Tushar Khot and Ashish Sabharwal and
                   Carissa Schoenick and Oyvind Tafjord},
  howpublished =  {arXiv:1803.05457v1},
  title =         {Think you have Solved Question Answering? Try ARC,
                   the AI2 Reasoning Challenge},
  year =          {2018},
  doi =           {10.48550/arXiv.1803.05457},
}

@misc{fang2024domainagnosticmoleculargenerationchemical,
  author =        {Yin Fang and Ningyu Zhang and Zhuo Chen and
                   Lingbing Guo and Xiaohui Fan and Huajun Chen},
  howpublished =  {arXiv},
  title =         {Domain-Agnostic Molecular Generation with Chemical
                   Feedback},
  year =          {2024},
  url =           {https://arxiv.org/abs/2301.11259},
}

@misc{hu2021opengraphbenchmarkdatasets,
  author =        {Weihua Hu and Matthias Fey and Marinka Zitnik and
                   Yuxiao Dong and Hongyu Ren and Bowen Liu and
                   Michele Catasta and Jure Leskovec},
  howpublished =  {arXiv},
  title =         {{Open Graph Benchmark: Datasets for Machine Learning
                   on Graphs}},
  year =          {2021},
  url =           {https://arxiv.org/abs/2005.00687},
}

@misc{tian2024scicoderesearchcodingbenchmark,
  author =        {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and
                   Xinan Chen and Cunwei Fan and Xuefei Guo and
                   Roland Haas and Pan Ji and Kittithat Krongchon and
                   Yao Li and Shengyan Liu and Di Luo and Yutao Ma and
                   Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and
                   Bohao Wu and Yanyu Xiong and Shengzhu Yin and
                   Minhui Zhu and Kilian Lieret and Yanxin Lu and
                   Genglin Liu and Yufeng Du and Tianhua Tao and
                   Ofir Press and Jamie Callan and Eliu Huerta and
                   Hao Peng},
  howpublished =  {arXiv},
  title =         {SciCode: A Research Coding Benchmark Curated by
                   Scientists},
  year =          {2024},
  url =           {https://arxiv.org/abs/2407.13168},
}

@misc{krause2024calochallenge2022communitychallenge,
  author =        {Claudius Krause and Michele Faucci Giannelli and
                   Gregor Kasieczka and Benjamin Nachman and
                   Dalila Salamani and David Shih and Anna Zaborowska and
                   Oz Amram and Kerstin Borras and Matthew R. Buckley and
                   Erik Buhmann and Thorsten Buss and
                   Renato Paulo Da Costa Cardoso and Anthony L. Caterini and
                   Nadezda Chernyavskaya and Federico A. G. Corchia and
                   Jesse C. Cresswell and Sascha Diefenbacher and
                   Etienne Dreyer and Vijay Ekambaram and Engin Eren and
                   Florian Ernst and Luigi Favaro and Matteo Franchini and
                   Frank Gaede and Eilam Gross and Shih-Chieh Hsu and
                   Kristina Jaruskova and Benno Käch and
                   Jayant Kalagnanam and Raghav Kansal and Taewoo Kim and
                   Dmitrii Kobylianskii and Anatolii Korol and
                   William Korcari and Dirk Krücker and Katja Krüger and
                   Marco Letizia and Shu Li and Qibin Liu and
                   Xiulong Liu and Gabriel Loaiza-Ganem and
                   Thandikire Madula and Peter McKeown and
                   Isabell-A. Melzer-Pellmann and Vinicius Mikuni and
                   Nam Nguyen and Ayodele Ore and
                   Sofia Palacios Schweitzer and Ian Pang and
                   Kevin Pedro and Tilman Plehn and Witold Pokorski and
                   Huilin Qu and Piyush Raikwar and John A. Raine and
                   Humberto Reyes-Gonzalez and Lorenzo Rinaldi and
                   Brendan Leigh Ross and Moritz A. W. Scham and
                   Simon Schnake and Chase Shimmin and Eli Shlizerman and
                   Nathalie Soybelman and Mudhakar Srivatsa and
                   Kalliopi Tsolaki and Sofia Vallecorsa and
                   Kyongmin Yeo and Rui Zhang},
  howpublished =  {arXiv},
  title =         {CaloChallenge 2022: A Community Challenge for Fast
                   Calorimeter Simulation},
  year =          {2024},
  url =           {https://arxiv.org/abs/2410.21611},
}

@misc{takamoto2024pdebenchextensivebenchmarkscientific,
  author =        {Makoto Takamoto and Timothy Praditia and
                   Raphael Leiteritz and Dan MacKinlay and
                   Francesco Alesiani and Dirk Pflüger and
                   Mathias Niepert},
  howpublished =  {arXiv},
  title =         {PDEBENCH: An Extensive Benchmark for Scientific
                   Machine Learning},
  year =          {2024},
  url =           {https://arxiv.org/abs/2210.07182},
}

@misc{pramanick2025spiqadatasetmultimodalquestion,
  author =        {Shraman Pramanick and Rama Chellappa and
                   Subhashini Venugopalan},
  howpublished =  {arXiv},
  title =         {SPIQA: A Dataset for Multimodal Question Answering on
                   Scientific Papers},
  year =          {2025},
  url =           {https://arxiv.org/abs/2407.09413},
}

@misc{nguyen2024seafloor,
  author =        {Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and
                   Xi Peng},
  howpublished =  {arXiv},
  title =         {SeafloorAI: A Large-scale Vision-Language Dataset for
                   Seafloor Geological Survey},
  year =          {2024},
  url =           {https://arxiv.org/abs/2411.00172},
}

@misc{duarte2022fastml,
  author =        {Javier Duarte and Nhan Tran and Ben Hawks and
                   Christian Herwig and Jules Muhizi and
                   Shvetank Prakash and Vijay Janapa Reddi},
  howpublished =  {arXiv},
  title =         {FastML Science Benchmarks: Accelerating Real-Time
                   Scientific Edge Machine Learning},
  year =          {2022},
  url =           {https://arxiv.org/abs/2207.07958},
}

@misc{farrell2021mlperfhpcholisticbenchmark,
  author =        {Steven Farrell and Murali Emani and Jacob Balma and
                   Lukas Drescher and Aleksandr Drozd and Andreas Fink and
                   Geoffrey Fox and David Kanter and Thorsten Kurth and
                   Peter Mattson and Dawei Mu and Amit Ruhela and
                   Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and
                   Aristeidis Tsaris and Jan Balewski and Ben Cumming and
                   Takumi Danjo and Jens Domke and Takaaki Fukai and
                   Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and
                   Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and
                   Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and
                   Maxime Martinasso and Satoshi Matsuoka and
                   Henrique Mendonça and Kazuki Minami and Prabhat Ram and
                   Takashi Sawada and Mallikarjun Shankar and
                   Tom St. John and Akihiro Tabuchi and
                   Venkatram Vishwanath and Mohamed Wahib and
                   Masafumi Yamazaki and Junqi Yin},
  howpublished =  {arXiv},
  title =         {MLPerf HPC: A Holistic Benchmark Suite for Scientific
                   Machine Learning on HPC Systems},
  year =          {2021},
  url =           {https://arxiv.org/abs/2110.11466},
}

@misc{campolongo2025buildingmachinelearningchallenges,
  author =        {Elizabeth G. Campolongo and Yuan-Tang Chou and
                   Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and
                   Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and
                   Mark S. Neubauer and Josephine Namayanja and
                   Aneesh Subramanian and Philip Harris and
                   Advaith Anand and David E. Carlyn and Subhankar Ghosh and
                   Christopher Lawrence and Eric Moreno and Ryan Raikman and
                   Jiaman Wu and Ziheng Zhang and Bayu Adhi and
                   Mohammad Ahmadi Gharehtoragh and
                   Saúl Alonso Monsalve and Marta Babicz and
                   Furqan Baig and Namrata Banerji and William Bardon and
                   Tyler Barna and Tanya Berger-Wolf and
                   Adji Bousso Dieng and Micah Brachman and Quentin Buat and
                   David C. Y. Hui and Phuong Cao and Franco Cerino and
                   Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and
                   Deming Chen and Eric Chen and Chia-Jui Chou and
                   Zih-Chen Ciou and Miles Cochran-Branson and
                   Artur Cordeiro Oudot Choi and Michael Coughlin and
                   Matteo Cremonesi and Maria Dadarlat and Peter Darch and
                   Malina Desai and Daniel Diaz and Steven Dillmann and
                   Javier Duarte and Isla Duporge and Urbas Ekka and
                   Saba Entezari Heravi and Hao Fang and Rian Flynn and
                   Geoffrey Fox and Emily Freed and Hang Gao and
                   Jing Gao and Julia Gonski and Matthew Graham and
                   Abolfazl Hashemi and Scott Hauck and James Hazelden and
                   Joshua Henry Peterson and Duc Hoang and Wei Hu and
                   Mirco Huennefeld and David Hyde and Vandana Janeja and
                   Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and
                   Maksim Kholiavchenko and Elham E. Khoda and
                   Sangin Kim and Aditya Kumar and Bo-Cheng Lai and
                   Trung Le and Chi-Wei Lee and JangHyeon Lee and
                   Shaocheng Lee and Suzan van der Lee and Charles Lewis and
                   Haitong Li and Haoyang Li and Henry Liao and Mia Liu and
                   Xiaolin Liu and Xiulong Liu and Vladimir Loncar and
                   Fangzheng Lyu and Ilya Makarov and
                   Abhishikth Mallampalli Chen-Yu Mao and
                   Alexander Michels and Alexander Migala and
                   Farouk Mokhtar and Mathieu Morlighem and Min Namgung and
                   Andrzej Novak and Andrew Novick and Amy Orsborn and
                   Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and
                   Zhiyuan Pei and Ana Peixoto and George Percivall and
                   Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and
                   Melissa Quinnan and Arghya Ranjan and Dylan Rankin and
                   Christina Reissel and Benedikt Riedel and
                   Dan Rubenstein and Argyro Sasli and Eli Shlizerman and
                   Arushi Singh and Kim Singh and Eric R. Sokol and
                   Arturo Sorensen and Yu Su and Mitra Taheri and
                   Vaibhav Thakkar and Ann Mariam Thomas and
                   Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and
                   Arjun Verma and Ricco C. Venterea and He Wang and
                   Jianwu Wang and Sam Wang and Shaowen Wang and
                   Gordon Watts and Jason Weitz and Andrew Wildridge and
                   Rebecca Williams and Scott Wolf and Yue Xu and
                   Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and
                   Ying Zhao and Yibo Zhong},
  howpublished =  {arXiv},
  title =         {Building Machine Learning Challenges for Anomaly
                   Detection in Science},
  year =          {2025},
  url =           {https://arxiv.org/abs/2503.02112},
}

@misc{luo2025benchmarkingaiscientistsomics,
  author =        {Erpai Luo and Jinmeng Jia and Yifan Xiong and
                   Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and
                   Xuegong Zhang},
  howpublished =  {arXiv},
  title =         {Benchmarking AI scientists in omics data-driven
                   biological research},
  year =          {2025},
  url =           {https://arxiv.org/abs/2505.08341},
}

@misc{hendrycks2021measuring,
  author =        {Hendrycks, Dan and Burns, Collin and
                   Kadavath, Saurav},
  journal =       {arXiv preprint arXiv:2009.03300},
  title =         {{Measuring Massive Multitask Language Understanding}},
  year =          {2021},
  url =           {https://arxiv.org/abs/2009.03300},
}

@misc{rein2023gpqagraduatelevelgoogleproofqa,
  author =        {Rein, David and Hou, Betty Li and
                   Stickland, Asa Cooper},
  title =         {GPQA: A Graduate-Level Google-Proof Q and A
                   Benchmark},
  year =          {2023},
  url =           {https://arxiv.org/abs/2311.12022},
}

@misc{mudur2025feabenchevaluatinglanguagemodels,
  author =        {Nayantara Mudur and Hao Cui and
                   Subhashini Venugopalan and Paul Raccuglia and
                   Michael P. Brenner and Peter Norgaard},
  howpublished =  {arXiv},
  title =         {FEABench: Evaluating Language Models on Multiphysics
                   Reasoning Ability},
  year =          {2025},
  url =           {https://arxiv.org/abs/2504.06260},
}

@misc{weitz2025neuralarchitecturecodesignfast,
  author =        {Jason Weitz and Dmitri Demler and Luke McDermott and
                   Nhan Tran and Javier Duarte},
  howpublished =  {arXiv},
  title =         {Neural Architecture Codesign for Fast Physics
                   Applications},
  year =          {2025},
  url =           {https://arxiv.org/abs/2501.05515},
}

@misc{khrabrov2024nabla2dftuniversalquantumchemistry,
  author =        {Kuzma Khrabrov and Anton Ber and Artem Tsypin and
                   Konstantin Ushenin and Egor Rumiantsev and
                   Alexander Telepov and Dmitry Protasov and
                   Ilya Shenbin and Anton Alekseev and Mikhail Shirokikh and
                   Sergey Nikolenko and Elena Tutubalina and
                   Artur Kadurin},
  howpublished =  {arXiv},
  title =         {Delta-Squared DFT: A Universal Quantum Chemistry
                   Dataset of Drug-Like Molecules and a Benchmark for
                   Neural Network Potentials},
  year =          {2024},
  url =           {https://arxiv.org/abs/2406.14347},
}

@misc{zhong2024spiqa,
  author =        {Zhong, Xiaoyan and Gao, Yijian and
                   Gururangan, Suchin},
  title =         {SPIQA: Scientific Paper Image Question Answering},
  year =          {2024},
  url =           {https://arxiv.org/abs/2407.09413},
}

@misc{rein2023gpqagraduatelevelgoogleproofqa2,
  author =        {David Rein and Betty Li Hou and Asa Cooper Stickland and
                   Jackson Petty and Richard Yuanzhe Pang and
                   Julien Dirani and Julian Michael and
                   Samuel R. Bowman},
  howpublished =  {arXiv},
  title =         {GPQA: A Graduate-Level Google-Proof Q and A
                   Benchmark},
  year =          {2023},
  url =           {https://arxiv.org/abs/2311.12022},
}

@misc{jin2020diseasedoespatienthave,
  author =        {Di Jin and Eileen Pan and Nassim Oufattole and
                   Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
  howpublished =  {arXiv},
  title =         {What Disease does this Patient Have? A Large-scale
                   Open Domain Question Answering Dataset from Medical
                   Exams},
  year =          {2020},
  url =           {https://arxiv.org/abs/2009.13081},
}

@misc{diguglielmo2025endtoendworkflowmachinelearningbased,
  author =        {Giuseppe Di Guglielmo and Botao Du and Javier Campos and
                   Alexandra Boltasseva and Akash V. Dixit and
                   Farah Fahim and Zhaxylyk Kudyshev and Santiago Lopez and
                   Ruichao Ma and Gabriel N. Perdue and Nhan Tran and
                   Omer Yesilyurt and Daniel Bowring},
  howpublished =  {arXiv},
  title =         {End-to-end workflow for machine learning-based qubit
                   readout with QICK and hls4ml},
  year =          {2025},
  url =           {https://arxiv.org/abs/2501.14663},
}

@misc{luo2024cfdbenchlargescalebenchmarkmachine,
  author =        {Luo, Yining and Chen, Yingfa and Zhang, Zhen},
  title =         {CFDBench: A Large-Scale Benchmark for Machine
                   Learning Methods in Fluid Dynamics},
  year =          {2024},
  url =           {https://arxiv.org/abs/2310.05963},
}

@misc{cui2025curieevaluatingllmsmultitask,
  author =        {Hao Cui and Zahra Shamsi and Gowoon Cheon and
                   Xuejian Ma and Shutong Li and Maria Tikhanovskaya and
                   Peter Norgaard and Nayantara Mudur and
                   Martyna Plomecka and Paul Raccuglia and Yasaman Bahri and
                   Victor V. Albert and Pranesh Srinivasan and
                   Haining Pan and Philippe Faist and Brian Rohr and
                   Ekin Dogus Cubuk and Muratahan Aykol and
                   Amil Merchant and Michael J. Statt and Dan Morris and
                   Drew Purves and Elise Kleeman and Ruth Alcantara and
                   Matthew Abraham and Muqthar Mohammad and
                   Ean Phing VanLee and Chenfei Jiang and
                   Elizabeth Dorfman and Eun-Ah Kim and
                   Michael P Brenner and Viren Jain and Sameera Ponda and
                   Subhashini Venugopalan},
  howpublished =  {arXiv},
  title =         {CURIE: Evaluating LLMs On Multitask Scientific Long
                   Context Understanding and Reasoning},
  year =          {2025},
  url =           {https://arxiv.org/abs/2503.13517},
}

@misc{parpillon2024smartpixelsinpixelai,
  author =        {Benjamin Parpillon and Chinar Syal and Jieun Yoo and
                   Jennet Dickinson and Morris Swartz and
                   Giuseppe Di Guglielmo and Alice Bean and
                   Douglas Berry and Manuel Blanco Valentin and
                   Karri DiPetrillo and Anthony Badea and Lindsey Gray and
                   Petar Maksimovic and Corrinne Mills and
                   Mark S. Neubauer and Gauri Pradhan and Nhan Tran and
                   Dahai Wen and Farah Fahim},
  howpublished =  {arXiv},
  title =         {Smart Pixels: In-pixel AI for on-sensor data
                   filtering},
  year =          {2024},
  url =           {https://arxiv.org/abs/2406.14860},
}

@misc{https://doi.org/10.5281/zenodo.5046389,
  author =        {Aarrestad, Thea and Govorkova, Ekaterina and
                   Ngadiuba, Jennifer and Puljak, Ema and
                   Pierini, Maurizio and Wozniak, Kinga Anna},
  publisher =     {Zenodo},
  title =         {{Unsupervised New Physics detection at 40 MHz:
                   Training Dataset}},
  year =          {2021},
  doi =           {10.5281/ZENODO.5046389},
  url =           {https://zenodo.org/record/5046389},
}

@misc{bowles2024betterclassicalsubtleart,
  author =        {Joseph Bowles and Shahnawaz Ahmed and Maria Schuld},
  howpublished =  {arXiv},
  title =         {Better than classical? The subtle art of benchmarking
                   quantum machine learning models},
  year =          {2024},
  url =           {https://arxiv.org/abs/2403.07059},
}

@misc{odagiu2024ultrafastjetclassificationfpgas,
  author =        {Patrick Odagiu and Zhiqiang Que and Javier Duarte and
                   Johannes Haller and Gregor Kasieczka and
                   Artur Lobanov and Vladimir Loncar and Wayne Luk and
                   Jennifer Ngadiuba and Maurizio Pierini and
                   Philipp Rincke and Arpita Seksaria and Sioni Summers and
                   Andre Sznajder and Alexander Tapper and
                   Thea K. Aarrestad},
  howpublished =  {arXiv},
  title =         {Ultrafast jet classification on FPGAs for the HL-LHC},
  year =          {2024},
  doi =           {https://doi.org/10.1088/2632-2153/ad5f10},
  url =           {https://arxiv.org/abs/2402.01876},
}

@misc{liu2021braggnnfastxraybragg,
  author =        {Zhengchun Liu and Hemant Sharma and Jun-Sang Park and
                   Peter Kenesei and Antonino Miceli and Jonathan Almer and
                   Rajkumar Kettimuthu and Ian Foster},
  howpublished =  {arXiv},
  title =         {BraggNN: Fast X-ray Bragg Peak Analysis Using Deep
                   Learning},
  year =          {2021},
  url =           {https://arxiv.org/abs/2008.08198},
}

@misc{kafkes2021boostrdatasetacceleratorcontrol,
  author =        {Diana Kafkes and Jason St. John},
  howpublished =  {arXiv},
  title =         {BOOSTR: A Dataset for Accelerator Control Systems},
  year =          {2021},
  url =           {https://arxiv.org/abs/2101.08359},
}

@misc{kvapil2025intelligentexperimentsrealtimeai,
  author =        {J. Kvapil and G. Borca-Tasciuc and H. Bossi and
                   K. Chen and Y. Chen and Y. Corrales Morales and
                   H. Da Costa and C. Da Silva and C. Dean and J. Durham and
                   S. Fu and C. Hao and P. Harris and O. Hen and
                   H. Jheng and Y. Lee and P. Li and X. Li and Y. Lin and
                   M. X. Liu and V. Loncar and J. P. Mitrevski and
                   A. Olvera and M. L. Purschke and J. S. Renck and
                   G. Roland and J. Schambach and Z. Shi and N. Tran and
                   N. Wuerfel and B. Xu and D. Yu and H. Zhang},
  howpublished =  {arXiv},
  title =         {Intelligent experiments through real-time AI: Fast
                   Data Processing and Autonomous Detector Control for
                   sPHENIX and future EIC detectors},
  year =          {2025},
  url =           {https://arxiv.org/abs/2501.04845},
}

@misc{abud2021deep,
  author =        {A. Abed Abud and B. Abi and R. Acciarri and
                   M. A. Acero and G. Adamov and D. Adams and
                   M. Adinolfi and A. Aduszkiewicz and Z. Ahmad and
                   J. Ahmed and T. Alion and S. Alonso Monsalve and
                   M. Alrashed and C. Alt and A. Alton and P. Amedo and
                   J. Anderson and C. Andreopoulos and M. P. Andrews and
                   F. Andrianala and S. Andringa and N. Anfimov and
                   A. Ankowski and M. Antonova and S. Antusch and
                   A. Aranda-Fernandez and A. Ariga and L. O. Arnold and
                   M. A. Arroyave and J. Asaadi and A. Aurisano and
                   V. Aushev and D. Autiero and M. Ayala-Torres and
                   F. Azfar and H. Back and J. J. Back and C. Backhouse and
                   P. Baesso and I. Bagaturia and L. Bagby and
                   S. Balasubramanian and P. Baldi and B. Baller and
                   B. Bambah and F. Barao and G. Barenboim and
                   G. J. Barker and W. Barkhouse and C. Barnes and
                   G. Barr and J. Barranco Monarca and N. Barros and
                   J. L. Barrow and A. Basharina-Freshville and
                   A. Bashyal and V. Basque and E. Belchior and
                   J. B. R. Battat and F. Battisti and F. Bay and
                   J. L. Bazo Alba and J. F. Beacom and E. Bechetoille and
                   B. Behera and L. Bellantoni and G. Bellettini and
                   V. Bellini and O. Beltramello and D. Belver and
                   N. Benekos and F. Bento Neves and S. Berkman and
                   P. Bernardini and R. M. Berner and H. Berns and
                   S. Bertolucci and M. Betancourt and
                   A. Betancur Rodr{\'i}guez and M. Bhattacharjee and
                   S. Bhuller and B. Bhuyan and S. Biagi and J. Bian and
                   M. Biassoni and K. Biery and B. Bilki and M. Bishai and
                   A. Bitadze and A. Blake and F. D. M. Blaszczyk and
                   G. C. Blazey and E. Blucher and J. Boissevain and
                   S. Bolognesi and T. Bolton and L. Bomben and
                   M. Bonesini and M. Bongrand and F. Bonini and
                   A. Booth and C. Booth and S. Bordoni and A. Borkum and
                   T. Boschi and N. Bostan and P. Bour and C. Bourgeois and
                   S. B. Boyd and D. Boyden and J. Bracinik and D. Braga and
                   D. Brailsford and A. Brandt and J. Bremer and C. Brew and
                   E. Brianne and S. J. Brice and C. Brizzolari and
                   C. Bromberg and G. Brooijmans and J. Brooke and
                   A. Bross and G. Brunetti and M. Brunetti and
                   N. Buchanan and H. Budd and D. Caiulo and
                   P. Calafiura and J. Calcutt and M. Calin and
                   S. Calvez and E. Calvo and A. Caminata and
                   M. Campanelli and K. Cankocak and D. Caratelli and
                   G. Carini and B. Carlus and P. Carniti and
                   I. Caro Terrazas and H. Carranza and T. Carroll and
                   J. F. Casta\~{n}o Forero and A. Castillo and
                   C. Castromonte and E. Catano-Mur and C. Cattadori and
                   F. Cavalier and F. Cavanna and S. Centro and
                   G. Cerati and A. Cervelli and A. Cervera Villanueva and
                   M. Chalifour and A. Chappell and E. Chardonnet and
                   N. Charitonidis and A. Chatterjee and
                   S. Chattopadhyay and H. Chen and M. Chen and Y. Chen and
                   Z. Chen and D. Cherdack and C. Chi and S. Childress and
                   A. Chiriacescu and G. Chisnall and K. Cho and
                   S. Choate and D. Chokheli and S. Choubey and
                   A. Christensen and D. Christian and G. Christodoulou and
                   A. Chukanov and E. Church and P. Clarke and
                   T. E. Coan and A. G. Cocco and J. A. B. Coelho and
                   E. Conley and R. Conley and J. M. Conrad and
                   M. Convery and S. Copello and L. Corwin and
                   L. Cremaldi and L. Cremonesi and
                   J. I. Crespo-Anad\'{o}n and E. Cristaldo and R. Cross and
                   A. Cudd and C. Cuesta and Y. Cui and D. Cussans and
                   M. Dabrowski and O. Dalager and H. da Motta and
                   L. Da Silva Peres and C. David and Q. David and
                   G. S. Davies and S. Davini and J. Dawson and K. De and
                   R. M. De Almeida and P. Debbins and I. De Bonis and
                   M. P. Decowski and A. de Gouv{\^e}a and
                   P. C. De Holanda and I. L. De Icaza Astiz and
                   A. Deisting and P. De Jong and A. Delbart and
                   D. Delepine and M. Delgado and A. Dell'Acqua and
                   P. De Lurgio and J. R. T. de Mello Neto and
                   D. M. DeMuth and S. Dennis and C. Densham and
                   G. W. Deptuch and A. De Roeck and V. De Romeri and
                   G. De Souza and R. Dharmapalan and F. Diaz and
                   J. S. D\'{i}az and S. Di Domizio and L. Di Giulio and
                   P. Ding and L. Di Noto and C. Distefano and R. Diurba and
                   M. Diwan and Z. Djurcic and N. Dokania and S. Dolan and
                   M. J. Dolinski and L. Domine and D. Douglas and
                   D. Douillet and G. Drake and F. Drielsma and
                   D. Duchesneau and K. Duffy and P. Dunne and T. Durkin and
                   H. Duyang and O. Dvornikov and D. A. Dwyer and
                   A. S. Dyshkant and M. Eads and A. Earle and
                   D. Edmunds and J. Eisch and L. Emberger and S. Emery and
                   A. Ereditato and C. O. Escobar and G. Eurin and
                   J. J. Evans and E. Ewart and A. C. Ezeribe and
                   K. Fahey and A. Falcone and C. Farnese and Y. Farzan and
                   J. Felix and M. Fernandes Carneiro da Silva and
                   E. Fernandez-Martinez and P. Fernandez Menendez and
                   F. Ferraro and L. Fields and F. Filthaut and
                   A. Fiorentini and R. S. Fitzpatrick and W. Flanagan and
                   B. Fleming and R. Flight and D. V. Forero and
                   J. Fowler and W. Fox and J. Franc and K. Francis and
                   D. Franco and J. Freeman and J. Freestone and
                   J. Fried and A. Friedland and S. Fuess and I. Furic and
                   A. P. Furmanski and A. Gago and H. Gallagher and
                   A. Gallas and A. Gallego-Ros and N. Gallice and
                   V. Galymov and E. Gamberini and T. Gamble and
                   R. Gandhi and R. Gandrajula and F. Gao and S. Gao and
                   D. Garcia-Gamez and M. \'{A} Garc\'{i}a-Peris and
                   S. Gardiner and D. Gastler and G. Ge and B. Gelli and
                   A. Gendotti and S. Gent and Z. Ghorbani-Moghaddam and
                   D. Gibin and I. Gil-Botella and S. Gilligan and
                   C. Girerd and A. K. Giri and D. Gnani and O. Gogota and
                   M. Gold and S. Gollapinni and K. Gollwitzer and
                   R. A. Gomes and L. V. Gomez Bermeo and
                   L. S. Gomez Fajardo and F. Gonnella and
                   J. A. Gonzalez-Cuevas and D. Gonzalez-Diaz and
                   M. Gonzalez-Lopez and M. C. Goodman and O. Goodwin and
                   S. Goswami and C. Gotti and E. Goudzovski and
                   C. Grace and M. Graham and R. Gran and E. Granados and
                   P. Granger and A. Grant and C. Grant and D. Gratieri and
                   P. Green and L. Greenler and J. Greer and
                   W. C. Griffith and M. Groh and J. Grudzinski and
                   K. Grzelak and W. Gu and V. Guarino and R. Guenette and
                   E. Guerard and A. Guglielmi and B. Guo and
                   K. K. Guthikonda and R. Gutierrez and P. Guzowski and
                   M. M. Guzzo and S. Gwon and A. Habig and H. Hadavand and
                   R. Haenni and A. Hahn and J. Haiston and
                   P. Hamacher-Baumann and T. Hamernik and P. Hamilton and
                   J. Han and D. A. Harris and J. Hartnell and J. Harton and
                   T. Hasegawa and C. Hasnip and R. Hatcher and
                   K. W. Hatfield and A. Hatzikoutelis and C. Hayes and
                   E. Hazen and A. Heavey and K. M. Heeger and J. Heise and
                   K. Hennessy and S. Henry and
                   M. A. Hernandez Morquecho and K. Herner and L. Hertel and
                   V Hewes and A. Higuera and T. Hill and S. J. Hillier and
                   A. Himmel and J. Hoff and C. Hohl and A. Holin and
                   E. Hoppe and G. A. Horton-Smith and M. Hostert and
                   A. Hourlier and B. Howard and R. Howell and J. Huang and
                   J. Huang and J. Hugon and G. Iles and N. Ilic and
                   A. M. Iliescu and R. Illingworth and A. Ioannisian and
                   L. Isenhower and R. Itay and A. Izmaylov and
                   S. Jackson and V. Jain and E. James and B. Jargowsky and
                   F. Jediny and D. Jena and Y. S. Jeong and
                   C. Jes\'{u}s-Valls and X. Ji and L. Jiang and
                   S. Jim\'{e}nez and A. Jipa and R. Johnson and
                   B. Jones and S. B. Jones and M. Judah and C. K. Jung and
                   T. Junk and Y. Jwa and M. Kabirnezhad and A. Kaboth and
                   I. Kadenko and I. Kakorin and F. Kamiya and
                   N. Kaneshige and G. Karagiorgi and G. Karaman and
                   A. Karcher and M. Karolak and Y. Karyotakis and
                   S. Kasai and S. P. Kasetti and L. Kashur and
                   N. Kazaryan and E. Kearns and P. Keener and
                   K. J. Kelly and E. Kemp and O. Kemularia and
                   W. Ketchum and S. H. Kettell and M. Khabibullin and
                   A. Khotjantsev and A. Khvedelidze and D. Kim and
                   B. King and B. Kirby and M. Kirby and J. Klein and
                   K. Koehler and L. W. Koerner and S. Kohn and
                   P. P. Koller and L. Kolupaeva and M. Kordosky and
                   T. Kosc and U. Kose and V. A. Kosteleck\'{y} and
                   K. Kothekar and F. Krennrich and I. Kreslo and
                   Y. Kudenko and V. A. Kudryavtsev and S. Kulagin and
                   J. Kumar and P. Kumar and P. Kunze and N. Kurita and
                   C. Kuruppu and V. Kus and T. Kutter and A. Lambert and
                   B. Land and K. Lande and C. E. Lane and K. Lang and
                   T. Langford and J. Larkin and P. Lasorak and D. Last and
                   C. Lastoria and A. Laundrie and A. Lawrence and
                   I. Lazanu and R. LaZur and T. Le and S. Leardini and
                   J. Learned and P. LeBrun and T. LeCompte and
                   G. Lehmann Miotto and R. Lehnert and
                   M. A. Leigui de Oliveira and M. Leitner and L. Li and
                   S. W. Li and T. Li and Y. Li and H. Liao and
                   C. S. Lin and Q. Lin and S. Lin and A. Lister and
                   B. R. Littlejohn and J. Liu and S. Lockwitz and
                   T. Loew and M. Lokajicek and I. Lomidze and K. Long and
                   K. Loo and D. Lorca and T. Lord and J. M. LoSecco and
                   W. C. Louis and X. -G. Lu and K. B. Luk and X. Luo and
                   N. Lurkin and T. Lux and V. P. Luzio and
                   D. MacFarlane and A. A. Machado and P. Machado and
                   C. T. Macias and J. R. Macier and A. Maddalena and
                   A. Madera and P. Madigan and S. Magill and K. Mahn and
                   A. Maio and A. Major and J. A. Maloney and
                   G. Mandrioli and R. C. Mandujano and J. Maneira and
                   L. Manenti and S. Manly and A. Mann and
                   K. Manolopoulos and M. Manrique Plata and
                   V. N. Manyam and L. Manzanillas and M. Marchan and
                   A. Marchionni and W. Marciano and D. Marfatia and
                   C. Mariani and J. Maricic and R. Marie and F. Marinho and
                   A. D. Marino and D. Marsden and M. Marshak and
                   C. M. Marshall and J. Marshall and J. Marteau and
                   J. Martin-Albo and N. Martinez and
                   D. A. Martinez Caicedo and S. Martynenko and K. Mason and
                   A. Mastbaum and M. Masud and S. Matsuno and
                   J. Matthews and C. Mauger and N. Mauri and
                   K. Mavrokoridis and I. Mawby and R. Mazza and
                   A. Mazzacane and E. Mazzucato and T. McAskill and
                   E. McCluskey and N. McConkey and K. S. McFarland and
                   C. McGrew and A. McNab and A. Mefodiev and P. Mehta and
                   P. Melas and O. Mena and S. Menary and H. Mendez and
                   D. P. M{\'e}ndez and A. Menegolli and G. Meng and
                   M. D. Messier and W. Metcalf and T. Mettler and
                   M. Mewes and H. Meyer and T. Miao and G. Michna and
                   T. Miedema and J. Migenda and V. Mikola and
                   R. Milincic and W. Miller and J. Mills and C. Milne and
                   O. Mineev and O. G. Miranda and S. Miryala and
                   C. S. Mishra and S. R. Mishra and A. Mislivec and
                   D. Mladenov and I. Mocioiu and K. Moffat and N. Moggi and
                   R. Mohanta and T. A. Mohayai and N. Mokhov and
                   J. Molina and L. Molina Bueno and A. Montanari and
                   C. Montanari and D. Montanari and
                   L. M. Montano Zetina and J. Moon and M. Mooney and
                   A. F. Moor and D. Moreno and C. Morris and C. Mossey and
                   E. Motuk and C. A. Moura and J. Mousseau and W. Mu and
                   L. Mualem and J. Mueller and M. Muether and S. Mufson and
                   F. Muheim and A. Muir and M. Mulhearn and D. Munford and
                   H. Muramatsu and S. Murphy and J. Musser and
                   J. Nachtman and S. Nagu and M. Nalbandyan and
                   R. Nandakumar and D. Naples and S. Narita and
                   D. Navas-Nicol\'{a}s and A. Navrer-Agasson and
                   N. Nayak and M. Nebot-Guinot and K. Negishi and
                   J. K. Nelson and J. Nesbit and M. Nessi and
                   D. Newbold and M. Newcomer and D. Newhart and
                   H. Newton and R. Nichol and F. Nicolas-Arnaldos and
                   E. Niner and K. Nishimura and A. Norman and
                   A. Norrick and R. Northrop and P. Novella and
                   J. A. Nowak and M. Oberling and J. P. Ochoa-Ricoux and
                   A. Olivares Del Campo and A. Olivier and
                   A. Olshevskiy and Y. Onel and Y. Onishchuk and J. Ott and
                   L. Pagani and S. Pakvasa and G. Palacio and
                   O. Palamara and S. Palestini and J. M. Paley and
                   M. Pallavicini and C. Palomares and
                   J. L. Palomino-Gallo and E. Pantic and V. Paolone and
                   V. Papadimitriou and R. Papaleo and A. Papanestis and
                   S. Paramesvaran and S. Parke and Z. Parsa and
                   M. Parvu and S. Pascoli and L. Pasqualini and
                   J. Pasternak and J. Pater and C. Patrick and
                   L. Patrizii and R. B. Patterson and S. J. Patton and
                   T. Patzak and A. Paudel and B. Paulos and L. Paulucci and
                   Z. Pavlovic and G. Pawloski and D. Payne and V. Pec and
                   S. J. M. Peeters and E. Pennacchio and A. Penzo and
                   O. L. G. Peres and J. Perry and D. Pershey and
                   G. Pessina and G. Petrillo and C. Petta and R. Petti and
                   F. Piastra and L. Pickering and F. Pietropaolo and
                   R. Plunkett and R. Poling and X. Pons and
                   N. Poonthottathil and S. Pordes and J. Porter and
                   M. Potekhin and R. Potenza and B. V. K. S. Potukuchi and
                   J. Pozimski and M. Pozzato and S. Prakash and
                   T. Prakash and S. Prince and D. Pugnere and X. Qian and
                   M. C. Queiroga Bazetto and J. L. Raaf and V. Radeka and
                   J. Rademacker and B. Radics and A. Rafique and
                   E. Raguzin and M. Rai and M. Rajaoalisoa and
                   I. Rakhno and A. Rakotonandrasana and
                   L. Rakotondravohitra and Y. A. Ramachers and
                   R. Rameika and M. A. Ramirez Delgado and B. Ramson and
                   A. Rappoldi and G. Raselli and P. Ratoff and S. Raut and
                   R. F. Razakamiandra and J. S. Real and B. Rebel and
                   M. Reggiani-Guzzo and T. Rehak and J. Reichenbacher and
                   S. D. Reitzner and H. Rejeb Sfar and A. Renshaw and
                   S. Rescia and F. Resnati and A. Reynolds and
                   C. Riccio and G. Riccobene and L. C. J. Rice and
                   J. Ricol and A. Rigamonti and Y. Rigaut and D. Rivera and
                   L. Rochester and M. Roda and P. Rodrigues and
                   M. J. Rodriguez Alonso and E. Rodriguez Bonilla and
                   J. Rodriguez Rondon and S. Rosauro-Alcaraz and
                   M. Rosenberg and P. Rosier and B. Roskovec and
                   M. Rossella and J. Rout and P. Roy and S. Roy and
                   A. Rubbia and C. Rubbia and F. C. Rubio and
                   B. Russell and D. Ruterbories and R. Saakyan and
                   S. Sacerdoti and T. Safford and R. Sahay and N. Sahu and
                   P. Sala and N. Samios and O. Samoylov and
                   M. C. Sanchez and D. A. Sanders and D. Sankey and
                   S. Santana and M. Santos-Maldonado and N. Saoulidou and
                   P. Sapienza and C. Sarasty and I. Sarcevic and
                   G. Savage and V. Savinov and A. Scaramelli and
                   A. Scarff and A. Scarpelli and T. Schaffer and
                   H. Schellman and P. Schlabach and D. Schmitz and
                   K. Scholberg and A. Schukraft and E. Segreto and
                   J. Sensenig and I. Seong and A. Sergi and
                   D. Sgalaberna and M. H. Shaevitz and S. Shafaq and
                   M. Shamma and R. Sharankova and H. R. Sharma and
                   R. Sharma and R. Kumar and T. Shaw and
                   C. Shepherd-Themistocleous and S. Shin and D. Shooltz and
                   R. Shrock and L. Simard and F. Simon and N. Simos and
                   J. Sinclair and G. Sinev and J. Singh and J. Singh and
                   V. Singh and R. Sipos and F. W. Sippach and G. Sirri and
                   A. Sitraka and K. Siyeon and K. Skarpaas VIII and
                   A. Smith and E. Smith and P. Smith and J. Smolik and
                   M. Smy and E. L. Snider and P. Snopok and
                   M. Soares Nunes and H. Sobel and M. Soderberg and
                   C. J. Solano Salinas and S. S\"{o}ldner-Rembold and
                   N. Solomey and V. Solovov and W. E. Sondheim and
                   M. Sorel and J. Soto-Oton and A. Sousa and
                   K. Soustruznik and F. Spagliardi and M. Spanu and
                   J. Spitz and N. J. C. Spooner and K. Spurgeon and
                   R. Staley and M. Stancari and L. Stanco and
                   R. Stanley and R. Stein and H. M. Steiner and
                   J. Stewart and B. Stillwell and J. Stock and
                   F. Stocker and T. Stokes and M. Strait and T. Strauss and
                   S. Striganov and A. Stuart and J. G. Suarez and
                   H. Sullivan and D. Summers and A. Surdo and V. Susic and
                   L. Suter and C. M. Sutera and R. Svoboda and
                   B. Szczerbinska and A. M. Szelc and R. Talaga and
                   H. A. Tanaka and B. Tapia Oregui and A. Tapper and
                   S. Tariq and E. Tatar and R. Tayloe and A. M. Teklu and
                   M. Tenti and K. Terao and C. A. Ternes and
                   F. Terranova and G. Testera and A. Thea and
                   J. L. Thompson and C. Thorn and S. C. Timm and
                   J. Todd and A. Tonazzo and D. Torbunov and M. Torti and
                   M. Tortola and F. Tortorici and D. Totani and
                   M. Toups and C. Touramanis and J. Trevor and
                   S. Trilov and W. H. Trzaska and Y. T. Tsai and
                   Z. Tsamalaidze and K. V. Tsang and N. Tsverava and
                   S. Tufanli and C. Tull and E. Tyley and M. Tzanov and
                   M. A. Uchida and J. Urheim and T. Usher and
                   S. Uzunyan and M. R. Vagins and P. Vahle and
                   G. A. Valdiviesso and E. Valencia and Z. Vallari and
                   J. W. F. Valle and S. Vallecorsa and R. Van Berg and
                   R. G. Van de Water and F. Varanini and D. Vargas and
                   G. Varner and J. Vasel and S. Vasina and G. Vasseur and
                   N. Vaughan and K. Vaziri and S. Ventura and
                   A. Verdugo and S. Vergani and M. A. Vermeulen and
                   M. Verzocchi and M. Vicenzi and H. Vieira de Souza and
                   C. Vignoli and C. Vilela and B. Viren and T. Vrba and
                   T. Wachala and A. V. Waldron and M. Wallbank and
                   H. Wang and J. Wang and M. H. L. S. Wang and Y. Wang and
                   Y. Wang and K. Warburton and D. Warner and M. Wascko and
                   D. Waters and A. Watson and P. Weatherly and A. Weber and
                   M. Weber and H. Wei and A. Weinstein and D. Wenman and
                   M. Wetstein and A. White and L. H. Whitehead and
                   D. Whittington and M. J. Wilking and C. Wilkinson and
                   Z. Williams and F. Wilson and R. J. Wilson and
                   J. Wolcott and T. Wongjirad and A. Wood and K. Wood and
                   E. Worcester and M. Worcester and C. Wret and W. Wu and
                   W. Wu and Y. Xiao and E. Yandel and G. Yang and
                   K. Yang and S. Yang and T. Yang and A. Yankelevich and
                   N. Yershov and K. Yonehara and T. Young and B. Yu and
                   H. Yu and J. Yu and W. Yuan and R. Zaki and
                   J. Zalesak and L. Zambelli and B. Zamorano and
                   A. Zani and L. Zazueta and G. Zeit and G. P. Zeller and
                   J. Zennamo and K. Zeug and C. Zhang and M. Zhao and
                   E. Zhivun and G. Zhu and P. Zilberman and
                   E. D. Zimmerman and M. Zito and S. Zucchelli and
                   J. Zuklin and V. Zutshi and R. Zwaska},
  howpublished =  {arXiv},
  title =         {{Deep Underground Neutrino Experiment (DUNE) Near
                   Detector Conceptual Design Report}},
  year =          {2021},
  url =           {https://arxiv.org/abs/2103.13910},
}

@misc{glazer2024frontiermathbenchmarkevaluatingadvanced,
  author =        {Elliot Glazer and Ege Erdil and Tamay Besiroglu and
                   Diego Chicharro and Evan Chen and Alex Gunning and
                   Caroline Falkman Olsson and Jean-Stanislas Denain and
                   Anson Ho and Emily de Oliveira Santos and
                   Olli J\"{a}rviniemi and Matthew Barnett and
                   Robert Sandler and Matej Vrzala and Jaime Sevilla and
                   Qiuyu Ren and Elizabeth Pratt and Lionel Levine and
                   Grant Barkley and Natalie Stewart and Bogdan Grechuk and
                   Tetiana Grechuk and Shreepranav Varma Enugandla and
                   Mark Wildon},
  howpublished =  {arXiv},
  title =         {{FrontierMath: A Benchmark for Evaluating Advanced
                   Mathematical Reasoning in AI}},
  year =          {2024},
  url =           {https://arxiv.org/abs/2411.04872},
}

@misc{www-aime,
  author =        {{vals.ai}},
  month =         mar,
  note =          {[Online accessed 2025-06-24]},
  title =         {{Public Enterprise LLM Benchmarks: AIME}},
  year =          {2025},
  url =           {https://www.vals.ai/benchmarks/aime},
}

@misc{wei2024lowlatencyopticalbasedmode,
  author =        {Yumou Wei and Ryan F. Forelli and Chris Hansen and
                   Jeffrey P. Levesque and Nhan Tran and Joshua C. Agar and
                   Giuseppe Di Guglielmo and Michael E. Mauel and
                   Gerald A. Navratil},
  howpublished =  {arXiv},
  title =         {Low latency optical-based mode tracking with machine
                   learning deployed on FPGAs on a tokamak},
  year =          {2024},
  doi =           {https://doi.org/10.1063/5.0190354},
  url =           {https://arxiv.org/abs/2312.00128},
}

@misc{NVIDIA_NsightSys_Doc,
  author =        {{NVIDIA Corporation}},
  howpublished =  {\url{https://developer.nvidia.com/nsight-systems}},
  note =          {Accessed: [Current Date]},
  title =         {{NVIDIA Nsight Systems Documentation}},
  year =          {2024},
}

@misc{NVIDIA_NsightComp_Doc,
  author =        {{NVIDIA Corporation}},
  howpublished =
  {\url{https://docs.nvidia.com/nsight-compute/NsightCompute/index.html}},
  note =          {Accessed: [Current Date]},
  title =         {{NVIDIA Nsight Compute Documentation}},
  year =          {2024},
}

@misc{Intel_VTune_Doc,
  author =        {{Intel Corporation}},
  howpublished =
  {\url{https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/}},
  note =          {Accessed: [Current Date]},
  title =         {{Intel VTune Profiler Documentation}},
  year =          {2024},
}

@misc{AMD_ROCM_Doc,
  author =        {{AMD}},
  howpublished =  {\url{https://rocm.docs.amd.com/en/latest/}},
  note =          {Accessed: [Current Date]},
  title =         {{ROCm Documentation}},
  year =          {2024},
}

@misc{Triton_Paper,
  author =        {OpenAI},
  howpublished =  {\url{https://openai.com/research/triton}},
  note =          {The Triton profiler is part of the Triton compiler
                   and language.},
  title =         {{Triton: An Intermediate Language for Tiled Tensor
                   Computations}},
  year =          {2019},
}

@misc{NVIDIA_NCCL_Doc,
  author =        {{NVIDIA Corporation}},
  howpublished =
  {\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html}},
  note =          {Accessed: [Current Date]},
  title =         {{NVIDIA Collective Communications Library (NCCL)
                   Documentation}},
  year =          {2024},
}

@misc{coral2,
  author =        {{Lawrence Livermore National Labs}},
  howpublished =  {\url{https://asc.llnl.gov/coral-2-benchmarks}},
  title =         {{CORAL-2 Benchmarks}},
  year =          {2020},
}

@misc{olcf6-bmks,
  author =        {{Oak Ridge National Labs}},
  howpublished =
  {\url{https://www.olcf.ornl.gov/draft-olcf-6-technical-requirements/benchmarks/}},
  title =         {{OLCF-6 Benchmarks}},
  year =          {2024},
}

@misc{Narang2017-deepBench,
  author =        {Narang, Sharan and Diamos, Greg},
  howpublished =  {\url{https://svail.github.io/DeepBench-update/}},
  title =         {{An update to DeepBench with a focus on deep learning
                   inference}},
  year =          {2017},
}

@misc{openfold2,
  author =        {Derevyanko, Georgy and Lamoureux, Guillame and
                   Outeiral, Carlos and Oda, Toshiyuki and Fuchs, Fabian and
                   Mahajan, Sai Pooja and Moult, John and Haas, Juergen and
                   Maragakis, Paul and Ruzmetov, Talant and
                   AlQuraishi, Mohammed},
  howpublished =  {\url{https://lupoglaz.github.io/OpenFold2/}},
  title =         {{OpenFold2: Replicating AlphaFold2 in the Dark}},
  year =          {2023},
}

@misc{DRAMthermalissues,
  author =        {Karen Heyman},
  howpublished =
  {\url{https://semiengineering.com/dram-thermal-issues-reach-crisis-point/}},
  title =         {{DRAM Thermal Issues Reach Crisis Point}},
  year =          {2022},
}

@misc{tacc,
  author =        {TACC},
  howpublished =  {\url{https://www.tacc.utexas.edu/}},
  title =         {{Texas Advanced Computing Center}},
  year =          {2021},
}

@misc{sciencefeedback2024energy,
  author =        {{Science Feedback}},
  title =         {{Training and Using ChatGPT Uses a Lot of Energy, but
                   Exact Numbers Are Tricky to Pin Down Without Data
                   from OpenAI}},
  year =          {2024},
  url =           {https://science.feedback.org/training-and-using-chatgpt-
                  uses-a-lot-of-energy-but-exact-numbers-are-tricky-to-pin-
                  down-without-data-from-openai},
}

@misc{EIA_Electricity_Price_2025,
  author =        {U.S. Energy Information Administration},
  howpublished =  {\url{https://www.eia.gov/electricity/monthly/}},
  title =         {Average Price of Electricity to Ultimate Customers},
  year =          {2025},
}

@misc{eia2024residential,
  author =        {{U.S. Energy Information Administration}},
  howpublished =  {Available at https://www.eia.gov/},
  title =         {Average Annual Electricity Consumption for U.S.
                   Residential Customers},
  year =          {2024},
}

@misc{jegham2025hungryaibenchmarkingenergy,
  author =        {Jegham, Nidhal and Abdelatti, Marwan and
                   Koh, Chan Young and Elmoubarki, Lassad and
                   Hendawi, Abdeltawab},
  title =         {How Hungry is AI? Benchmarking Energy, Water, and
                   Carbon Footprint of LLM Inference},
  year =          {2025},
  url =           {https://arxiv.org/abs/2505.09598},
}

@misc{baeldung2023energy,
  author =        {{Baeldung Editors}},
  title =         {{How Much Energy Does ChatGPT Use?}},
  year =          {2023},
  url =           {https://www.baeldung.com/cs/chatgpt-large-language-models-
                  power-consumption},
}

@misc{medium2023gpt4carbon,
  author =        {{Data Science on Medium}},
  title =         {{The Carbon Footprint of GPT-4}},
  year =          {2023},
  url =           {https://medium.com/data-science/the-carbon-footprint-of-gpt-
                  4-d6c676eb21ae},
}

@misc{extremenetworks2023energy,
  author =        {{Extreme Networks}},
  title =         {{Confronting AI’s Growing Energy Appetite: Part 1}},
  year =          {2023},
  url =           {https://www.extremenetworks.com/resources/blogs/confronting-
                  ai-growing-energy-appetite-part-1},
}

@misc{epochai2024compute,
  author =        {{Epoch AI}},
  title =         {{Why GPT-5 Used Less Training Compute Than GPT-4.5
                   (But GPT-6 Probably Won’t)}},
  year =          {2024},
  url =           {https://epoch.ai/gradient-updates/why-gpt5-used-less-
                  training-compute-than-gpt45-but-gpt6-probably-wont},
}

@misc{hackernoon2024dirtysecret,
  author =        {Hackernoon Editors},
  title =         {{AI’s Dirty Secret: The Energy Cost of Training the
                   Next GPT-5}},
  year =          {2024},
  url =           {https://hackernoon.com/ais-dirty-secret-the-energy-cost-of-
                  training-the-next-gpt-5},
}

@misc{sert2,
  howpublished =  {\url{https://spec.org/sert2/}},
  note =          {Server Efficiency Rating Tool with calibrated energy
                   measurements},
  title =         {SPEC SERT\textsuperscript{2}},
  year =          {2025},
}

@misc{tpcenergy,
  howpublished =  {\url{https://www.tpc.org/}},
  note =          {Energy add‑on kit for TPC database benchmarks},
  title =         {TPC-Energy},
  year =          {2025},
}

@misc{joulesort,
  howpublished =  {\url{https://sortbenchmark.org/}},
  note =          {Records sorted per Joule; storage I/O energy
                   efficiency},
  title =         {JouleSort Benchmark},
  year =          {2025},
}

@misc{hpcgpower,
  howpublished =  {\url{https://hpcg-benchmark.org/}},
  note =          {Energy efficiency (GFLOPS/W) for the High Performance
                   Conjugate Gradient benchmark},
  title =         {HPCG-Power},
  year =          {2025},
}

@misc{hplmxphplai,
  howpublished =  {\url{https://top500.org/news/hpl-ai-benchmark/}},
  note =          {Mixed-precision LINPACK benchmark with GFLOPS/W
                   metric},
  title =         {HPL-MxP (HPL-AI)},
  year =          {2025},
}

@misc{mlperfpower,
  howpublished =  {\url{https://mlcommons.org/en/power/}},
  note =          {Joules, average Watts, Joules per sample/epoch for ML
                   workloads},
  title =         {MLPerf Power: Training and Inference},
  year =          {2025},
}

@misc{mlperftiny,
  howpublished =  {\url{https://mlcommons.org/en/tiny/}},
  note =          {Microjoules per inference on micro‑controllers},
  title =         {MLPerf Tiny: Energy Mode},
  year =          {2025},
}

@misc{coremarkpro,
  howpublished =  {\url{https://www.eembc.org/coremarkpro/}},
  note =          {Iterations per second per Watt for embedded/SoC
                   devices},
  title =         {CoreMark-PRO Power},
  year =          {2025},
}

@misc{procyon,
  howpublished =  {\url{https://benchmarks.ul.com/procyon}},
  note =          {Images per Watt and fps/W on desktop and mobile
                   devices},
  title =         {UL Procyon AI Inference Power Test},
  year =          {2025},
}

@misc{candlepowerstud,
  howpublished =  {\url{https://doi.org/10.1145/3337821.3337924}},
  note =          {Deep learning cancer benchmark with Joules/epoch \&
                   GFLOPS/W metrics},
  title =         {CANDLE Power Study (SC19)},
  year =          {2025},
}

@misc{luleshminifeene,
  howpublished =
  {\url{https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom55337.2022.00045}},
  note =          {Energy/Joules per iteration for proxy-apps (Gerofi et
                   al., 2022)},
  title =         {LULESH/miniFE Energy Benchmark},
  year =          {2025},
}

@misc{exasmrpowerbenc,
  howpublished =  {\url{https://doi.org/10.1016/j.jpdc.2021.05.001}},
  note =          {Energy vs accuracy trade-off for neutron transport
                   mini-app},
  title =         {ExaSMR Power Benchmark},
  year =          {2025},
}

@misc{eehpcwgenergybe,
  howpublished =  {\url{https://eehpcwg.llnl.gov/}},
  note =          {Community draft specification for node \& job energy
                   benchmarking},
  title =         {EE-HPC-WG Energy Benchmark (draft)},
  year =          {2025},
}

@misc{hpcai500energyt,
  howpublished =  {\url{https://www.hpc-ai.org/}},
  note =          {Upcoming GFLOPS/W extension to HPC-AI500 mixed AI/HPC
                   benchmark},
  title =         {HPC-AI500 Energy Track (planned)},
  year =          {2025},
}

@misc{parsec31energye,
  howpublished =  {\url{https://parsec.cs.gatech.edu/}},
  note =          {Research prototype adding power metrics to PARSEC
                   benchmark suite},
  title =         {PARSEC-3.1 Energy Extension},
  year =          {2025},
}

@misc{specptdaemonser,
  howpublished =  {\url{https://spec.org/ptdaemon/}},
  note =          {Calibrated power logging used with SPEC benchmarks on
                   HPC systems},
  title =         {SPEC PTDaemon / SERT Energy for HPC},
  year =          {2025},
}

@misc{scaphandre,
  howpublished =  {\url{https://github.com/hubblo-org/scaphandre}},
  note =          {Process \& node power telemetry agent for Linux
                   clusters (Watts, kWh)},
  title =         {Scaphandre},
  year =          {2025},
}

@misc{kepler,
  howpublished =
  {\url{https://github.com/sustainable-computing-io/kepler}},
  note =          {Watts and Joules per container/pod using eBPF/RAPL},
  title =         {Kepler: Kubernetes-based Energy Profiler},
  year =          {2025},
}

@misc{codecarbon,
  howpublished =  {\url{https://codecarbon.io/}},
  note =          {Process‑level kWh and kg CO2e estimation library},
  title =         {CodeCarbon},
  year =          {2025},
}

@article{Stevens2023-auroraGPT,
  author =        {Stevens, Rick},
  journal =       {{Trillion Parameter Consortium Seminar}},
  series =        {TPC},
  title =         {{Argonne's "AuroraGPT" Project}},
  year =          {2023},
}

@techreport{Dongarra1989LinpackReport,
  author =        {Jack J. Dongarra},
  institution =   {University of Tennessee, Knoxville / Oak Ridge
                   National Laboratory},
  number =        {Technical Report CS-89-85},
  title =         {Performance of Various Computers Using Standard
                   Linear Equations Software},
  year =          {1989},
  url =           {http://www.netlib.org/benchmark/performance.ps},
}

@article{Dongarra2016HPCG,
  author =        {Dongarra, Jack J. and Heroux, Michael A. and
                   Luszczek, Piotr},
  journal =       {International Journal of High Performance Computing
                   Applications},
  number =        {1},
  pages =         {3--8},
  title =         {High‐performance conjugate‐gradient benchmark: A
                   new metric for ranking high‐performance computing
                   systems},
  volume =        {30},
  year =          {2016},
  doi =           {10.1177/1094342015593158},
  url =           {https://doi.org/10.1177/1094342015593158},
}

@article{wilson2014software,
  author =        {Wilson, Greg},
  journal =       {F1000Research},
  publisher =     {F1000Research},
  title =         {Software Carpentry: lessons learned},
  volume =        {3},
  year =          {2014},
  doi =           {10.12688/f1000research.3-62.v2},
  url =           {https://doi.org/10.12688/f1000research.3-62.v2},
}

@article{reid2025hpc,
  author =        {Reid, Andrew and Keller, Trevor and O’Cais, Alan and
                   Rasel, Annajiat Alim and Purwanto, Wirawan and
                   Herriman, Jane and Muite, Benson and
                   Hermanns, Marc-Andr{\'e}},
  journal =       {Journal of Computational Science},
  number =        {1},
  title =         {HPC Carpentry: Recent Progress and Incubation Toward
                   an Official Carpentries Lesson Program},
  volume =        {16},
  year =          {2025},
}

@article{las-frontiers-edu,
  author =        {von Laszewski, Gregor and Fleischer, J. P. and
                   Knuuti, Robert and Fox, Geoffrey C. and
                   Kolessar, Jake and Butler, Thomas S. and Fox, Judy},
  journal =       {Frontiers in High Performance Computing},
  month =         {October},
  number =        {1233877},
  pages =         {31},
  title =         {Opportunities for enhancing MLCommons efforts while
                   leveraging insights from educational MLCommons
                   earthquake benchmarks efforts},
  volume =        {1},
  year =          {2023},
  doi =           {10.3389/fhpcp.2023.1233877},
  issn =          {2813-7337},
  url =           {https://www.frontiersin.org/journals/high-performance-
                  computing/articles/10.3389/fhpcp.2023.1233877},
}

@article{dongarra2003hpl,
  author =        {Jack J. Dongarra and Piotr Luszczek and
                   Antoine Petitet},
  journal =       {Concurrency and Computation: Practice and Experience},
  month =         {August 10},
  note =          {ISSN 1532-0634},
  number =        {9},
  pages =         {803-820},
  title =         {The {LINPACK} Benchmark: Past, Present, and Future},
  volume =        {15},
  year =          {2003},
  doi =           {10.1002/cpe.728},
}

@book{dongarra1979linpack,
  address =       {Philadelphia, PA, USA},
  author =        {Jack J. Dongarra and J. Bunch and Cleve Moler and
                   G. W. Stewart},
  publisher =     {Society of Industrial and Applied Mathematics},
  title =         {{LINPACK} User's Guide},
  year =          {1979},
}

@inproceedings{feng2005pwrprofsciapps,
  address =       {Denver, CO, USA},
  author =        {W.-C. Feng and R. Ge and Kirk W. Cameron},
  booktitle =     {19th IEEE International Parallel and Distributed
                   Processing Symposium (IPDPS 05)},
  title =         {Power and Energy Profiling of Scientific Applications
                   on Distributed Systems},
  year =          {2005},
}

@article{cameron2005hpcpowerdistcompsciapps,
  author =        {Kirk W. Cameron and R. Ge and X. Feng},
  journal =       {IEEE Computer},
  number =        {11},
  pages =         {40-47},
  title =         {High-performance, Power-aware, Distributed Computing
                   for Scientific Applications},
  volume =        {38},
  year =          {2005},
}

@article{green500,
  address =       {Washington, DC, USA},
  author =        {Feng, Wu-chun and Cameron, Kirk},
  journal =       {Computer},
  month =         {dec},
  number =        {12},
  pages =         {50–55},
  publisher =     {IEEE Computer Society Press},
  title =         {The Green500 List: Encouraging Sustainable
                   Supercomputing},
  volume =        {40},
  year =          {2007},
  doi =           {10.1109/MC.2007.445},
  issn =          {0018-9162},
  url =           {https://doi.org/10.1109/MC.2007.445},
}

@article{abdelfattah2021mxpsurvey,
  author =        {Ahmad Abdelfattah and Hartwig Anzt and Erik G Boman and
                   Erin Carson and Terry Cojean and Jack Dongarra and
                   Alyson Fox and Mark Gates and Nicholas J Higham and
                   Xiaoye S Li and Jennifer Loe and Piotr Luszczek and
                   Srikara Pranesh and Siva Rajamanickam and
                   Tobias Ribizel and Barry F Smith and
                   Kasia Swirydowicz and Stephen Thomas and
                   Stanimire Tomov and Yaohung M Tsai and
                   Ulrike Meier Yang},
  journal =       {The International Journal of High Performance
                   Computing Applications},
  number =        {4},
  pages =         {344-369},
  title =         {A survey of numerical linear algebra methods
                   utilizing mixed-precision arithmetic},
  volume =        {35},
  year =          {2021},
  abstract =      {The efficient utilization of mixed-precision
                   numerical linear algebra algorithms can offer
                   attractive acceleration to scientific computing
                   applications. Especially with the hardware
                   integration of low-precision special-function units
                   designed for machine learning applications, the
                   traditional numerical algorithms community urgently
                   needs to reconsider the floating point formats used
                   in the distinct operations to efficiently leverage
                   the available compute power. In this work, we provide
                   a comprehensive survey of mixed-precision numerical
                   linear algebra routines, including the underlying
                   concepts, theoretical background, and experimental
                   results for both dense and sparse linear algebra
                   problems.},
  doi =           {10.1177/10943420211003313},
  url =           {https://doi.org/10.1177/10943420211003313},
}

@article{www-mlcommons,
  journal =       {Web page},
  key =           {MLCommons},
  month =         apr,
  note =          {\url{https://mlcommons.org/} [Accessed April 13,
                   2023]},
  title =         {{Machine learning innovation to benefit everyone}},
  year =          {2023},
  url =           {https://mlcommons.org/},
}

@inproceedings{10.1007/978-3-031-23220-6_4,
  address =       {Cham},
  author =        {Thiyagalingam, Jeyan and von Laszewski, Gregor and
                   Yin, Junqi and Emani, Murali and Papay, Juri and
                   Barrett, Gregg and Luszczek, Piotr and
                   Tsaris, Aristeidis and Kirkpatrick, Christine and
                   Wang, Feiyi and Gibbs, Tom and Vishwanath, Venkatram and
                   Shankar, Mallikarjun and Fox, Geoffrey and Hey, Tony},
  booktitle =     {High Performance Computing. ISC High Performance 2022
                   International Workshops},
  editor =        {Anzt, Hartwig and Bienz, Amanda and Luszczek, Piotr and
                   Baboulin, Marc},
  pages =         {47--64},
  publisher =     {Springer International Publishing},
  title =         {AI Benchmarking for Science: Efforts from the
                   MLCommons Science Working Group},
  year =          {2022},
  abstract =      {With machine learning (ML) becoming a transformative
                   tool for science, the scientific community needs a
                   clear catalogue of ML techniques, and their relative
                   benefits on various scientific problems, if they were
                   to make significant advances in science using AI.
                   Although this comes under the purview of
                   benchmarking, conventional benchmarking initiatives
                   are focused on performance, and as such, science,
                   often becomes a secondary criteria.},
  isbn =          {978-3-031-23220-6},
}

@inproceedings{zhang2024empowering,
  author =        {Hang Zhang and Jiawei Sun and Renqi Chen and Wei Liu and
                   Zhonghang Yuan and Xinzhe Zheng and Zhefan Wang and
                   Zhiyuan Yang and Hang Yan and Han-Sen Zhong and
                   Xiqing Wang and Wanli Ouyang and Fan Yang and
                   Nanqing Dong},
  booktitle =     {The Thirty-eight Conference on Neural Information
                   Processing Systems Datasets and Benchmarks Track},
  title =         {Empowering and Assessing the Utility of Large
                   Language Models in Crop Science},
  year =          {2024},
  url =           {https://openreview.net/forum?id=hMj6jZ6JWU},
}

@inproceedings{neurips2024_0db7f135,
  author =        {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and
                   Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and
                   Kong, Linghe},
  booktitle =     {Advances in Neural Information Processing Systems},
  editor =        {A. Globerson and L. Mackey and D. Belgrave and A. Fan and
                   U. Paquet and J. Tomczak and C. Zhang},
  pages =         {7296--7310},
  publisher =     {Curran Associates, Inc.},
  title =         {UrbanDataLayer: A Unified Data Pipeline for Urban
                   Science},
  volume =        {37},
  year =          {2024},
  url =           {https://proceedings.neurips.cc/paper_files/paper/2024/file/
                  0db7f135f6991e8cec5e516ecc66bfba-Paper-
                  Datasets_and_Benchmarks_Track.pdf},
}

@article{karargyris2023federated,
  author =        {Karargyris, Alexandros and Umeton, Renato and
                   Sheller, Micah J. and Aristizabal, Alejandro and
                   George, Johnu and Wuest, Anna and Pati, Sarthak and
                   Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and
                   Narayana Moorthy, Prakash and Chowdhury, Alexander and
                   Guo, Junyi and Nalawade, Sahil and Rosenthal, Jacob and
                   Kanter, David and Xenochristou, Maria and
                   Beutel, Daniel J. and Chung, Verena and
                   Bergquist, Timothy and Eddy, James and Abid, Abubakar and
                   Tunstall, Lewis and Sanseviero, Omar and
                   Dimitriadis, Dimitrios and Qian, Yiming and
                   Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong and
                   Bala, Srini and Bittorf, Victor and
                   Puchala, Sreekar Reddy and Ricciuti, Biagio and
                   Samineni, Soujanya and Sengupta, Eshna and
                   Chaudhari, Akshay and Coleman, Cody and
                   Desinghu, Bala and Diamos, Gregory and Dutta, Debo and
                   Feddema, Diane and Fursin, Grigori and Huang, Xinyuan and
                   Kashyap, Satyananda and Lane, Nicholas and
                   Mallick, Indranil and Mascagni, Pietro and
                   Mehta, Virendra and Moraes, Cassiano Ferro and
                   Natarajan, Vivek and Nikolov, Nikola and
                   Padoy, Nicolas and Pekhimenko, Gennady and
                   Reddi, Vijay Janapa and Reina, G. Anthony and
                   Ribalta, Pablo and Singh, Abhishek and
                   Thiagarajan, Jayaraman J. and Albrecht, Jacob and
                   Wolf, Thomas and Miller, Geralyn and Fu, Huazhu and
                   Shah, Prashant and Xu, Daguang and Yadav, Poonam and
                   Talby, David and Awad, Mark M. and Howard, Jeremy P. and
                   Rosenthal, Michael and Marchionni, Luigi and
                   Loda, Massimo and Johnson, Jason M. and
                   Bakas, Spyridon and Mattson, Peter and
                   FeTS Consortium and BraTS-2020 Consortium and
                   AI4SafeChole Consortium},
  journal =       {Nature Machine Intelligence},
  month =         jul,
  number =        {7},
  pages =         {799--810},
  title =         {Federated benchmarking of medical artificial
                   intelligence with MedPerf},
  volume =        {5},
  year =          {2023},
  doi =           {10.1038/s42256-023-00652-2},
  url =           {https://doi.org/10.1038/s42256-023-00652-2},
}

@inproceedings{neurips2024_a8063075,
  author =        {Zou, Deyu and Liu, Shikun and Miao, Siqi and
                   Fung, Victor and Chang, Shiyu and Li, Pan},
  booktitle =     {Advances in Neural Information Processing Systems},
  editor =        {A. Globerson and L. Mackey and D. Belgrave and A. Fan and
                   U. Paquet and J. Tomczak and C. Zhang},
  pages =         {92499--92528},
  publisher =     {Curran Associates, Inc.},
  title =         {GeSS: Benchmarking Geometric Deep Learning under
                   Scientific Applications with Distribution Shifts},
  volume =        {37},
  year =          {2024},
  url =           {https://proceedings.neurips.cc/paper_files/paper/2024/file/
                  a8063075b00168dc39bc81683619f1a8-Paper-
                  Datasets_and_Benchmarks_Track.pdf},
}

@article{chanussot2021oc20,
  author =        {Chanussot, Lowik and Das, Abhishek and
                   Goyal, Siddharth and Lavril, Thibaut and
                   Shuaibi, Muhammed and Riviere, Morgane and
                   Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and
                   Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and
                   Wood, Brandon and Yoon, Junwoong and Parikh, Devi and
                   Zitnick, C. Lawrence and Ulissi, Zachary},
  journal =       {ACS Catalysis},
  number =        {10},
  pages =         {6059--6072},
  title =         {The Open Catalyst 2020 (OC20) Dataset and Community
                   Challenges},
  volume =        {11},
  year =          {2021},
  doi =           {10.1021/acscatal.0c04525},
  url =           {https://pubs.acs.org/doi/10.1021/acscatal.0c04525},
}

@article{tran2023oc22,
  author =        {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and
                   Wood, Brandon M. and Goyal, Siddharth and
                   Das, Abhishek and Heras-Domingo, Javier and
                   Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and
                   Sriram, Anuroop and Therrien, F\'{e}lix and
                   Abed, Jehad and Voznyy, Oleksandr and
                   Sargent, Edward H. and Ulissi, Zachary and
                   Zitnick, C. Lawrence},
  journal =       {ACS Catalysis},
  number =        {5},
  pages =         {3066--3084},
  title =         {The Open Catalyst 2022 (OC22) Dataset and Challenges
                   for Oxide Electrocatalysts},
  volume =        {13},
  year =          {2023},
  doi =           {10.1021/acscatal.2c05426},
  url =           {https://pubs.acs.org/doi/10.1021/acscatal.2c05426},
}

@inproceedings{neurips2024_c4e3b55e,
  author =        {Chen, Pin and Peng, Luoxuan and Jiao, Rui and
                   Mo, Qing and Wang, Zhen and Huang, Wenbing and
                   Liu, Yang and Lu, Yutong},
  booktitle =     {Advances in Neural Information Processing Systems},
  editor =        {A. Globerson and L. Mackey and D. Belgrave and A. Fan and
                   U. Paquet and J. Tomczak and C. Zhang},
  pages =         {108902--108928},
  publisher =     {Curran Associates, Inc.},
  title =         {Learning Superconductivity from Ordered and
                   Disordered Material Structures},
  volume =        {37},
  year =          {2024},
  url =           {https://proceedings.neurips.cc/paper_files/paper/2024/file/
                  c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-
                  Datasets_and_Benchmarks_Track.pdf},
}

@inproceedings{neurips2024_4f9a5acd,
  author =        {Ohana, Ruben and McCabe, Michael and Meyer, Lucas and
                   Morel, Rudy and Agocs, Fruzsina J. and
                   Beneitez, Miguel and Berger, Marsha and
                   Burkhart, Blakesley and Dalziel, Stuart B. and
                   Fielding, Drummond B. and Fortunato, Daniel and
                   Goldberg, Jared A. and Hirashima, Keiya and
                   Jiang, Yan-Fei and Kerswell, Rich R. and
                   Maddu, Suryanarayana and Miller, Jonah and
                   Mukhopadhyay, Payel and Nixon, Stefan S. and
                   Shen, Jeff and Watteaux, Romain and
                   Blancard, Bruno R\'{e}galdo-Saint and
                   Rozet, Fran\c{c}ois and Parker, Liam H. and
                   Cranmer, Miles and Ho, Shirley},
  booktitle =     {{Advances in Neural Information Processing Systems}},
  editor =        {A. Globerson and L. Mackey and D. Belgrave and A. Fan and
                   U. Paquet and J. Tomczak and C. Zhang},
  pages =         {44989--45037},
  publisher =     {Curran Associates, Inc.},
  title =         {The Well: a Large-Scale Collection of Diverse Physics
                   Simulations for Machine Learning},
  volume =        {37},
  year =          {2024},
  url =           {https://proceedings.neurips.cc/paper_files/paper/2024/file/
                  4f9a5acd91ac76569f2fe291b1f4772b-Paper-
                  Datasets_and_Benchmarks_Track.pdf},
}

@article{roberts2023satin,
  author =        {Roberts, Jonathan and Han, Kai and Albanie, Samuel},
  journal =       {ICCV Workshop: Towards the Next Generation of
                   Computer Vision Datasets},
  month =         {3},
  title =         {Satin: A multi-task metadataset for classifying
                   satellite imagery using vision-language models},
  year =          {2023},
  doi =           {10.48550/arXiv.2304.11619},
}

@article{lightman2023lets,
  author =        {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and
                   Edwards, Harri and Baker, Bowen and Lee, Teddy and
                   Leike, Jan and Schulman, John and Sutskever, Ilya and
                   Cobbe, Karl},
  journal =       {arXiv preprint arXiv:2305.20050},
  title =         {Let's Verify Step by Step},
  year =          {2023},
  doi =           {10.48550/arXiv.2305.20050},
}

@inproceedings{neurips2024_c00d37d6,
  author =        {Peterson, Ralph E and Tanelus, Aramis and
                   Ick, Christopher and Mimica, Bartul and
                   Francis, Niegil and Ivan, Violet J and Choudhri, Aman and
                   Falkner, Annegret L and Murthy, Mala and
                   Schneider, David M and Sanes, Dan H and
                   Williams, Alex H},
  booktitle =     {Advances in Neural Information Processing Systems},
  editor =        {A. Globerson and L. Mackey and D. Belgrave and A. Fan and
                   U. Paquet and J. Tomczak and C. Zhang},
  pages =         {106370--106382},
  publisher =     {Curran Associates, Inc.},
  title =         {Vocal Call Locator Benchmark (VCL) for localizing
                   rodent vocalizations from multi-channel audio},
  volume =        {37},
  year =          {2024},
  url =           {https://proceedings.neurips.cc/paper_files/paper/2024/file/
                  c00d37d6b04d73b870b963a4d70051c1-Paper-
                  Datasets_and_Benchmarks_Track.pdf},
}

@inproceedings{neurips2024_c6c31413,
  author =        {Bushuiev, Roman and Bushuiev, Anton and
                   de Jonge, Niek F. and Young, Adamo and
                   Kretschmer, Fleming and Samusevich, Raman and
                   Heirman, Janne and Wang, Fei and Zhang, Luke and
                   D\"{u}hrkop, Kai and Ludwig, Marcus and
                   Haupt, Nils A. and Kalia, Apurva and Brungs, Corinna and
                   Schmid, Robin and Greiner, Russell and Wang, Bo and
                   Wishart, David S. and Liu, Li-Ping and Rousu, Juho and
                   Bittremieux, Wout and Rost, Hannes and Mak, Tytus D. and
                   Hassoun, Soha and Huber, Florian and
                   van der Hooft, Justin J.J. and Stravs, Michael A. and
                   B\"{o}cker, Sebastian and Sivic, Josef and
                   Pluskal, Tom\'{a}\v{s}},
  booktitle =     {Advances in Neural Information Processing Systems},
  editor =        {A. Globerson and L. Mackey and D. Belgrave and A. Fan and
                   U. Paquet and J. Tomczak and C. Zhang},
  pages =         {110010--110027},
  publisher =     {Curran Associates, Inc.},
  title =         {MassSpecGym: A benchmark for the discovery and
                   identification of molecules},
  volume =        {37},
  year =          {2024},
  url =           {https://proceedings.neurips.cc/paper_files/paper/2024/file/
                  c6c31413d5c53b7d1c343c1498734b0f-Paper-
                  Datasets_and_Benchmarks_Track.pdf},
}

@inproceedings{qin2023extremely,
  author =        {Shuyu Qin and Joshua Agar and Nhan Tran},
  booktitle =     {AI for Accelerated Materials Design - NeurIPS 2023
                   Workshop},
  title =         {Extremely Noisy 4D-TEM Strain Mapping Using Cycle
                   Consistent Spatial Transforming Autoencoders},
  year =          {2023},
  url =           {https://openreview.net/forum?id=7yt3N0o0W9},
}

@inproceedings{quench2024,
  address =       {Purdue University, IN},
  author =        {Maira Khan and Steve Krave and Vittorio Marinozzi and
                   Jennifer Ngadiuba and Stoyan Stoynev and Nhan Tran},
  booktitle =     {Fast Machine Learning for Science Conference 2024},
  month =         oct,
  publisher =     {indico.cern.ch},
  title =         {Benchmarking and Interpreting Real Time Quench
                   Detection Algorithms},
  year =          {2024},
  url =           {https://indico.cern.ch/event/1387540/contributions/6153618/
                  attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf},
}

@article{jain2013materials,
  author =        {Jain, Anubhav and Ong, Shyue Ping and
                   Hautier, Geoffroy and Chen, Wei and
                   Richards, William Davidson and Dacek, Stephen and
                   Cholia, Shreyas and Gunter, Dan and Skinner, David and
                   Ceder, Gerbrand and Persson, Kristin A.},
  journal =       {APL Materials},
  number =        {1},
  title =         {{The Materials Project: A Materials Genome Approach}},
  volume =        {1},
  year =          {2013},
  doi =           {10.1063/1.4812323},
  url =           {https://materialsproject.org/},
}

@inproceedings{las-2022-templated,
  author =        {von Laszewski, Gregor and Fleischer, J.P. and
                   Fox, Geoffrey C. and Papay, Juri and Jackson, Sam and
                   Thiyagalingam, Jeyan},
  booktitle =     {2023 IEEE 19th International Conference on e-Science
                   (e-Science)},
  pages =         {1-6},
  title =         {Templated Hybrid Reusable Computational Analytics
                   Workflow Management with Cloudmesh, Applied to the
                   Deep Learning MLCommons Cloudmask Application},
  year =          {2023},
  doi =           {10.1109/e-Science58273.2023.10254942},
}

@article{TensorFlow_System,
  author =        {Abadi, Martín and others},
  journal =       {OSDI},
  note =          {TensorBoard Profiler is a component of the TensorFlow
                   ecosystem.},
  pages =         {265--283},
  publisher =     {USENIX Association},
  title =         {{TensorFlow: A System for Large-Scale Machine
                   Learning}},
  volume =        {16},
  year =          {2016},
}

@article{PyTorch_System,
  author =        {Paszke, Adam and others},
  journal =       {Advances in Neural Information Processing Systems},
  note =          {PyTorch Profiler is part of the PyTorch library.},
  title =         {{PyTorch: An Imperative Style, High-Performance Deep
                   Learning Library}},
  volume =        {32},
  year =          {2019},
}

@manual{JAX_Profiler,
  author =        {Google},
  howpublished =
  {\url{https://jax.readthedocs.io/en/latest/profiling.html}},
  note =          {Accessed: 2025-10-22},
  title =         {JAX Profiler},
  year =          {2025},
}

@manual{NVIDIA_DLProf,
  author =        {NVIDIA},
  howpublished =
  {\url{https://developer.nvidia.com/deep-learning-profiler}},
  note =          {Accessed: 2025-10-22},
  title =         {NVIDIA Deep Learning Profiler (DLProf)},
  year =          {2025},
}

@manual{NVIDIA:CUDA:ProfilerGuide,
  author =        {{NVIDIA Corporation}},
  edition =       {Current Edition Number, e.g., 12.0},
  note =          {Documentation for nvprof. Available at
  \url{https://docs.nvidia.com/cuda/profiler-users-guide/index.html}},
  organization =  {{NVIDIA Corporation}},
  title =         {{NVIDIA CUDA Profiler User's Guide}},
  year =          {Current Year, e.g., 2024},
}

@article{HPCToolkit_Paper,
  author =        {Mellor-Crummey, John and others},
  journal =       {Concurrency and Computation: Practice and Experience},
  number =        {6},
  pages =         {680--708},
  title =         {{HPCToolkit: Tools for Performance Analysis of
                   Optimized Parallel Programs}},
  volume =        {24},
  year =          {2012},
}

@inproceedings{TAU_Paper,
  author =        {Shende, Sameer and Malony, Allen D.},
  booktitle =     {International Conference on Parallel and Distributed
                   Computing and Systems},
  pages =         {489--493},
  title =         {{The TAU Parallel Performance System}},
  year =          {1998},
}

@inproceedings{Perfetto_Google,
  author =        {Poulakos, George and others},
  booktitle =     {Proceedings of the 35th International Conference on
                   Software Engineering (ICSE 2023)},
  note =          {Based on the Perfetto project's scope for system
                   tracing.},
  title =         {{Perfetto: A System-Wide Tracing Tool for Modern
                   Applications}},
  year =          {2023},
}

@inproceedings{PAPI_Paper,
  author =        {Shirley Browne and Jack J. Dongarra and Nathan Garner and
                   Kevin S. London and Philip Mucci},
  booktitle =     {{Proceedings Supercomputing 2000, November 4-10,
                   2000, Dallas, Texas, {USA.} {IEEE} Computer Society,
                   {CD-ROM}}},
  editor =        {Jed Donnelley},
  pages =         {42},
  publisher =     {{IEEE} Computer Society},
  title =         {{A Scalable Cross-Platform Infrastructure for
                   Application Performance Tuning Using Hardware
                   Counters}},
  year =          {2000},
  biburl =        {https://dblp.org/rec/conf/sc/BrowneDGLM00.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  doi =           {10.1109/SC.2000.10029},
  timestamp =     {Fri, 24 Mar 2023 00:04:25 +0100},
  url =           {https://doi.org/10.1109/SC.2000.10029},
}

@inproceedings{XLA_Paper,
  author =        {Wang, Dehao and others},
  booktitle =     {Proceedings of the First Workshop on Systems for ML},
  note =          {XLA Profiler is a feature of the XLA compiler.},
  title =         {{XLA: Optimizing Compiler for TensorFlow}},
  year =          {2017},
}

@manual{TorchDynamo_TorchInductor,
  author =        {Meta (Facebook)},
  howpublished =  {\url{https://pytorch.org/blog/torchdynamo/}},
  note =          {Accessed: 2025-10-22},
  title =         {TorchDynamo \& TorchInductor Debug Tools},
  year =          {2025},
}

@manual{AWS_SageMaker_Debugger,
  author =        {Amazon Web Services},
  howpublished =
  {\url{https://docs.aws.amazon.com/sagemaker/latest/dg/debugger.html}},
  note =          {Accessed: 2025-10-22},
  title =         {AWS SageMaker Debugger},
  year =          {2025},
}

@manual{Azure_Profiler,
  author =        {Microsoft},
  howpublished =
  {\url{https://learn.microsoft.com/en-us/azure/machine-learning/how-to-monitor-profiling}},
  note =          {Accessed: 2025-10-22},
  title =         {Azure Machine Learning Profiler},
  year =          {2025},
}

@manual{Weights_Biases,
  author =        {Weights \& Biases},
  howpublished =  {\url{https://wandb.ai/site}},
  note =          {Accessed: 2025-10-22},
  title =         {Weights \& Biases},
  year =          {2025},
}

@manual{Comet_ML,
  author =        {Comet ML},
  howpublished =  {\url{https://www.comet.com/site/}},
  note =          {Accessed: 2025-10-22},
  title =         {Comet ML},
  year =          {2025},
}

@manual{MLflow,
  author =        {Databricks},
  howpublished =  {\url{https://mlflow.org/}},
  note =          {Accessed: 2025-10-22},
  title =         {MLflow},
  year =          {2025},
}

@manual{Torch_Tensorflow_Memory,
  author =        {Meta / Google},
  howpublished =
  {\url{https://pytorch.org/docs/stable/notes/memory.html}},
  note =          {Accessed: 2025-10-22},
  title =         {Torch/TensorFlow Memory Tools},
  year =          {2025},
}

@manual{Python_Profilers,
  author =        {Python Software Foundation},
  howpublished =  {\url{https://docs.python.org/3/library/profile.html}},
  note =          {Accessed: 2025-10-22},
  title =         {Python Profilers (cProfile, py-spy)},
  year =          {2025},
}

@inproceedings{CheBeckmann2013,
  author =        {Che, Shuai and Beckmann, Bradford M. and
                   Reinhardt, Stephen K. and Skadron, Kevin},
  booktitle =     {{IEEE International Symposium on Workload
                   Characterization}},
  month =         {9},
  pages =         {185-195},
  series =        {{IISWC}},
  title =         {{Pannotia: Understanding Irregular GPGPU Graph
                   Applications}},
  year =          {2013},
  doi =           {10.1109/IISWC.2013.6704684},
}

@article{WangPan2017-gunrock,
  address =       {New York, NY, USA},
  author =        {Wang, Yangzihao and Pan, Yuechao and Davidson, Andrew and
                   Wu, Yuduo and Yang, Carl and Wang, Leyuan and
                   Osama, Muhammad and Yuan, Chenshan and Liu, Weitang and
                   Riffel, Andy T. and Owens, John D.},
  journal =       {ACM Trans. Parallel Comput.},
  month =         {Aug},
  number =        {1},
  publisher =     {Association for Computing Machinery},
  title =         {{Gunrock: GPU Graph Analytics}},
  volume =        {4},
  year =          {2017},
  abstract =      {For large-scale graph analytics on the GPU, the
                   irregularity of data access and control flow, and the
                   complexity of programming GPUs, have presented two
                   significant challenges to developing a programmable
                   high-performance graph library. “Gunrock,” our
                   graph-processing system designed specifically for the
                   GPU, uses a high-level, bulk-synchronous,
                   data-centric abstraction focused on operations on a
                   vertex or edge frontier. Gunrock achieves a balance
                   between performance and expressiveness by coupling
                   high-performance GPU computing primitives and
                   optimization strategies with a high-level programming
                   model that allows programmers to quickly develop new
                   graph primitives with small code size and minimal GPU
                   programming knowledge. We characterize the
                   performance of various optimization strategies and
                   evaluate Gunrock’s overall performance on different
                   GPU architectures on a wide range of graph primitives
                   that span from traversal-based algorithms and ranking
                   algorithms, to triangle counting and
                   bipartite-graph-based algorithms. The results show
                   that on a single GPU, Gunrock has on average at least
                   an order of magnitude speedup over Boost and
                   PowerGraph, comparable performance to the fastest GPU
                   hardwired primitives and CPU shared-memory graph
                   libraries, such as Ligra and Galois, and better
                   performance than any other GPU high-level graph
                   library.},
  doi =           {10.1145/3108140},
  issn =          {2329-4949},
  url =           {https://doi.org/10.1145/3108140},
}

@article{kim2018qmcpack,
  author =        {Kim, Jeongnim and Baczewski, Andrew D and
                   Beaudet, Todd D and Benali, Anouar and
                   Bennett, M Chandler and Berrill, Mark A and
                   Blunt, Nick S and Borda, Edgar Josu{\'e} Landinez and
                   Casula, Michele and Ceperley, David M and others},
  journal =       {Journal of Physics: Condensed Matter},
  number =        {19},
  pages =         {195901},
  publisher =     {IOP Publishing},
  title =         {{QMCPACK: An Open Source ab initio Quantum Monte
                   Carlo Package for the Electronic Structure of Atoms,
                   Molecules and Solids}},
  volume =        {30},
  year =          {2018},
}

@inproceedings{WuTaylor2019-candle,
  address =       {New York, NY, USA},
  author =        {Wu, Xingfu and Taylor, Valerie and Wozniak, Justin M. and
                   Stevens, Rick and Brettin, Thomas and Xia, Fangfang},
  booktitle =     {{Proceedings of the 48th International Conference on
                   Parallel Processing}},
  publisher =     {Association for Computing Machinery},
  series =        {ICPP},
  title =         {{Performance, Energy, and Scalability Analysis and
                   Improvement of Parallel Cancer Deep Learning CANDLE
                   Benchmarks}},
  year =          {2019},
  abstract =      {Training scientific deep learning models requires the
                   significant compute power of high-performance
                   computing systems. In this paper, we analyze the
                   performance characteristics of the benchmarks from
                   the exploratory research project CANDLE (Cancer
                   Distributed Learning Environment) with a focus on the
                   hyperparameters epochs, batch sizes, and learning
                   rates. We present the parallel methodology that uses
                   the distributed deep learning framework Horovod to
                   parallelize the CANDLE benchmarks. We then use
                   scaling strategies for both epochs and batch size
                   with linear learning rate scaling to investigate how
                   they impact the execution time and accuracy as well
                   as the power, energy, and scalability of the parallel
                   CANDLE benchmarks under conditions of strong scaling
                   and weak scaling on the IBM Power9 heterogeneous
                   system Summit at Oak Ridge National Laboratory and
                   the Cray XC40 Theta at Argonne National Laboratory.
                   This study provides insights into how to set the
                   proper numbers of epochs, batch sizes, and compute
                   resources for these benchmarks to preserve the high
                   accuracy and to reduce the execution time of the
                   benchmarks. We identify the data-loading performance
                   bottleneck and then improve the performance and
                   energy for better scalability. Results with the
                   modified benchmarks on Summit indicate up to 78.25\%
                   in performance improvement and up to 78\% in energy
                   saving under strong scaling on up to 384 GPUs, and up
                   to 79.5\% in performance improvement and up to
                   77.11\% in energy saving under weak scaling on up to
                   3,072 GPUs. On Theta, we achieve up to 45.22\%
                   performance improvement and up to 41.78\% in energy
                   saving under strong scaling on up to 384 nodes.
                   Moreover, the modification dramatically reduces the
                   broadcast overhead.},
  doi =           {10.1145/3337821.3337905},
  isbn =          {9781450362955},
  url =           {https://doi.org/10.1145/3337821.3337905},
}

@article{BanburyReddi2021-tinyMLPerf,
  author =        {Colby R. Banbury and Vijay Janapa Reddi and
                   Peter Torelli and Jeremy Holleman and Nat Jeffries and
                   Csaba Kir{\'{a}}ly and Pietro Montino and
                   David Kanter and Sebastian Ahmed and Danilo Pau and
                   Urmish Thakker and Antonio Torrini and Pete Warden and
                   Jay Cordaro and Giuseppe Di Guglielmo and
                   Javier M. Duarte and Stephen Gibellini and
                   Videet Parekh and Honson Tran and Nhan Tran and
                   Wenxu Niu and Xuesong Xu},
  howpublished =  {arXiv},
  journal =       {CoRR},
  title =         {MLPerf Tiny Benchmark},
  volume =        {abs/2106.07597},
  year =          {2021},
  biburl =        {https://dblp.org/rec/journals/corr/abs-2106-07597.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  timestamp =     {Wed, 16 Jun 2021 10:42:19 +0200},
  url =           {https://arxiv.org/abs/2106.07597},
}

@inproceedings{BaruahShivdikar2021-gnnMark,
  author =        {Baruah, Trinayan and Shivdikar, Kaustubh and
                   Dong, Shi and Sun, Yifan and Mojumder, Saiful A and
                   Jung, Kihoon and Abellán, José L. and Ukidave, Yash and
                   Joshi, Ajay and Kim, John and Kaeli, David},
  booktitle =     {{IEEE International Symposium on Performance Analysis
                   of Systems and Software}},
  pages =         {13-23},
  series =        {ISPASS},
  title =         {{GNNMark: A Benchmark Suite to Characterize Graph
                   Neural Network Training on GPUs}},
  year =          {2021},
  doi =           {10.1109/ISPASS51385.2021.00013},
}

@inproceedings{DongKaeli2017-dnnmark,
  address =       {New York, NY, USA},
  author =        {Dong, Shi and Kaeli, David},
  booktitle =     {{Proceedings of the General Purpose GPUs}},
  pages =         {63--72},
  publisher =     {ACM},
  series =        {{GPGPU}},
  title =         {{DNNMark: A Deep Neural Network Benchmark Suite for
                   GPUs}},
  year =          {2017},
  doi =           {10.1145/3038228.3038239},
  isbn =          {978-1-4503-4915-4},
  url =           {http://doi.acm.org/10.1145/3038228.3038239},
}

@article{MattsonCheng2019-mlperfTrain,
  author =        {Peter Mattson and Christine Cheng and Cody Coleman and
                   Greg Diamos and Paulius Micikevicius and
                   David A. Patterson and Hanlin Tang and Gu{-}Yeon Wei and
                   Peter Bailis and Victor Bittorf and David Brooks and
                   Dehao Chen and Debojyoti Dutta and Udit Gupta and
                   Kim M. Hazelwood and Andrew Hock and Xinyuan Huang and
                   Bill Jia and Daniel Kang and David Kanter and
                   Naveen Kumar and Jeffery Liao and Guokai Ma and
                   Deepak Narayanan and Tayo Oguntebi and
                   Gennady Pekhimenko and Lillian Pentecost and
                   Vijay Janapa Reddi and Taylor Robie and Tom St. John and
                   Carole{-}Jean Wu and Lingjie Xu and Cliff Young and
                   Matei Zaharia},
  howpublished =  {arXiv},
  journal =       {CoRR},
  title =         {{MLPerf Training Benchmark}},
  volume =        {abs/1910.01500},
  year =          {2019},
  biburl =        {https://dblp.org/rec/journals/corr/abs-1910-01500.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  timestamp =     {Mon, 04 Nov 2019 08:16:51 +0100},
  url =           {http://arxiv.org/abs/1910.01500},
}

@article{MattsonReddi2020-mlPerf,
  author =        {Mattson, Peter and Reddi, Vijay Janapa and
                   Cheng, Christine and Coleman, Cody and Diamos, Greg and
                   Kanter, David and Micikevicius, Paulius and
                   Patterson, David and Schmuelling, Guenther and
                   Tang, Hanlin and others},
  journal =       {IEEE Micro},
  number =        {2},
  pages =         {8--16},
  publisher =     {IEEE},
  title =         {{MLPerf: An Industry Standard Benchmark Suite for
                   Machine Learning Performance}},
  volume =        {40},
  year =          {2020},
}

@inproceedings{Reddi2020mlperf-Infer,
  author =        {Reddi, Vijay Janapa and Cheng, Christine and
                   Kanter, David and Mattson, Peter and
                   Schmuelling, Guenther and Wu, Carole-Jean and
                   Anderson, Brian and Breughe, Maximilien and
                   Charlebois, Mark and Chou, William and Chukka, Ramesh and
                   Coleman, Cody and Davis, Sam and Deng, Pan and
                   Diamos, Greg and Duke, Jared and Fick, Dave and
                   Gardner, J. Scott and Hubara, Itay and
                   Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and
                   John, Tom St. and Kanwar, Pankaj and Lee, David and
                   Liao, Jeffery and Lokhmotov, Anton and
                   Massa, Francisco and Meng, Peng and
                   Micikevicius, Paulius and Osborne, Colin and
                   Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and
                   Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and
                   Tang, Hanlin and Thomson, Michael and Wei, Frank and
                   Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and
                   Yu, Bing and Yuan, George and Zhong, Aaron and
                   Zhang, Peizhao and Zhou, Yuchen},
  booktitle =     {{2020 ACM/IEEE 47th Annual International Symposium on
                   Computer Architecture}},
  pages =         {446-459},
  series =        {ISCA},
  title =         {{MLPerf Inference Benchmark}},
  year =          {2020},
  doi =           {10.1109/ISCA45697.2020.00045},
}

@article{ReddiCheng2021-mlPerfVision,
  author =        {Reddi, Vijay Janapa and Cheng, Christine and
                   Kanter, David and Mattson, Peter and
                   Schmuelling, Guenther and Wu, Carole-Jean},
  journal =       {IEEE Micro},
  number =        {3},
  pages =         {10-18},
  title =         {{The Vision Behind MLPerf: Understanding AI Inference
                   Performance}},
  volume =        {41},
  year =          {2021},
  doi =           {10.1109/MfM.2021.3066343},
}

@article{fan2021predicting,
  author =        {Fan, Zhao and Ma, Evan},
  journal =       {Nature communications},
  number =        {1},
  pages =         {1--13},
  publisher =     {Nature Publishing Group},
  title =         {Predicting orientation-dependent plastic
                   susceptibility from static structure in amorphous
                   solids via deep learning},
  volume =        {12},
  year =          {2021},
}

@article{jumper2021highly,
  author =        {Jumper, John and Evans, Richard and
                   Pritzel, Alexander and Green, Tim and
                   Figurnov, Michael and Ronneberger, Olaf and
                   Tunyasuvunakool, Kathryn and Bates, Russ and
                   {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and
                   others},
  journal =       {Nature},
  number =        {7873},
  pages =         {583--589},
  publisher =     {Nature Publishing Group},
  title =         {Highly accurate protein structure prediction with
                   AlphaFold},
  volume =        {596},
  year =          {2021},
}

@article{kates2019predicting,
  author =        {Kates-Harbeck, Julian and Svyatkovskiy, Alexey and
                   Tang, William},
  journal =       {Nature},
  number =        {7753},
  pages =         {526--531},
  publisher =     {Nature Publishing Group},
  title =         {Predicting disruptive instabilities in controlled
                   fusion plasmas through deep learning},
  volume =        {568},
  year =          {2019},
}

@article{ThiyagalingamShankar2022-mlSci,
  author =        {Thiyagalingam, Jeyan and Shankar, Mallikarjun and
                   Fox, Geoffrey and Hey, Tony},
  journal =       {Nature Reviews Physics},
  number =        {6},
  pages =         {413--420},
  publisher =     {Nature Publishing Group UK London},
  title =         {{Scientific Machine Learning Benchmarks}},
  volume =        {4},
  year =          {2022},
}

@inproceedings{ThiyagalingamVonLaszewski2022-aiForSciMLCommons,
  address =       {Cham},
  author =        {Thiyagalingam, Jeyan and von Laszewski, Gregor and
                   Yin, Junqi and Emani, Murali and Papay, Juri and
                   Barrett, Gregg and Luszczek, Piotr and
                   Tsaris, Aristeidis and Kirkpatrick, Christine and
                   Wang, Feiyi and Gibbs, Tom and Vishwanath, Venkatram and
                   Shankar, Mallikarjun and Fox, Geoffrey and Hey, Tony},
  booktitle =     {High Performance Computing. ISC High Performance 2022
                   International Workshops},
  editor =        {Anzt, Hartwig and Bienz, Amanda and Luszczek, Piotr and
                   Baboulin, Marc},
  pages =         {47--64},
  publisher =     {Springer International Publishing},
  title =         {AI Benchmarking for Science: Efforts from the
                   MLCommons Science Working Group},
  year =          {2022},
  abstract =      {With machine learning (ML) becoming a transformative
                   tool for science, the scientific community needs a
                   clear catalogue of ML techniques, and their relative
                   benefits on various scientific problems, if they were
                   to make significant advances in science using AI.
                   Although this comes under the purview of
                   benchmarking, conventional benchmarking initiatives
                   are focused on performance, and as such, science,
                   often becomes a secondary criteria.},
  isbn =          {978-3-031-23220-6},
}

@article{WangZhang2018-deepmd,
  author =        {Wang, Han and Zhang, Linfeng and Han, Jiequn and
                   E, Weinan},
  journal =       {{Computer Physics Communications}},
  pages =         {178-184},
  title =         {{DeePMD-kit: A deep learning package for many-body
                   potential energy representation and molecular
                   dynamics}},
  volume =        {228},
  year =          {2018},
  abstract =      {Recent developments in many-body potential energy
                   representation via deep learning have brought new
                   hopes to addressing the accuracy-versus-efficiency
                   dilemma in molecular simulations. Here we describe
                   DeePMD-kit, a package written in Python/C++ that has
                   been designed to minimize the effort required to
                   build deep learning based representation of potential
                   energy and force field and to perform molecular
                   dynamics. Potential applications of DeePMD-kit span
                   from finite molecules to extended systems and from
                   metallic systems to chemically bonded systems.
                   DeePMD-kit is interfaced with TensorFlow, one of the
                   most popular deep learning frameworks, making the
                   training process highly automatic and efficient. On
                   the other end, DeePMD-kit is interfaced with
                   high-performance classical molecular dynamics and
                   quantum (path-integral) molecular dynamics packages,
                   i.e., LAMMPS and the i-PI, respectively. Thus, upon
                   training, the potential energy and force field models
                   can be used to perform efficient molecular
                   simulations for different purposes. As an example of
                   the many potential applications of the package, we
                   use DeePMD-kit to learn the interatomic potential
                   energy and forces of a water model using data
                   obtained from density functional theory. We
                   demonstrate that the resulted molecular dynamics
                   model reproduces accurately the structural
                   information contained in the original model. Program
                   summary Program Title: DeePMD-kit Program Files doi:
                   http://dx.doi.org/10.17632/hvfh9yvncf.1 Licensing
                   provisions: LGPL Programming language: Python/C++
                   Nature of problem: Modeling the many-body atomic
                   interactions by deep neural network models. Running
                   molecular dynamics simulations with the models.
                   Solution method: The Deep Potential for Molecular
                   Dynamics (DeePMD) method is implemented based on the
                   deep learning framework TensorFlow. Supports for
                   using a DeePMD model in LAMMPS and i-PI, for
                   classical and quantum (path integral) molecular
                   dynamics are provided. Additional comments including
                   Restrictions and Unusual features: The code defines a
                   data protocol such that the energy, force, and virial
                   calculated by different third-party molecular
                   simulation packages can be easily processed and used
                   as model training data.},
  doi =           {https://doi.org/10.1016/j.cpc.2018.03.016},
  issn =          {0010-4655},
  url =           {https://www.sciencedirect.com/science/article/pii/
                  S0010465518300882},
}

@article{ZengZhang2023-deepmd2,
  author =        {Zeng, Jinzhe and Zhang, Duo and Lu, Denghui and
                   Mo, Pinghui and Li, Zeyu and Chen, Yixiao and
                   Rynik, Marián and Huang, Li’ang and Li, Ziyao and
                   Shi, Shaochen and Wang, Yingze and Ye, Haotian and
                   Tuo, Ping and Yang, Jiabin and Ding, Ye and Li, Yifan and
                   Tisi, Davide and Zeng, Qiyu and Bao, Han and Xia, Yu and
                   Huang, Jiameng and Muraoka, Koki and Wang, Yibo and
                   Chang, Junhan and Yuan, Fengbo and
                   Bore, Sigbjørn Løland and Cai, Chun and
                   Lin, Yinnian and Wang, Bo and Xu, Jiayan and
                   Zhu, Jia-Xin and Luo, Chenxing and Zhang, Yuzhi and
                   Goodall, Rhys E. A. and Liang, Wenshuo and
                   Singh, Anurag Kumar and Yao, Sikai and
                   Zhang, Jingchao and Wentzcovitch, Renata and
                   Han, Jiequn and Liu, Jie and Jia, Weile and
                   York, Darrin M. and E, Weinan and Car, Roberto and
                   Zhang, Linfeng and Wang, Han},
  journal =       {The Journal of Chemical Physics},
  month =         {08},
  number =        {5},
  pages =         {054801},
  title =         {{DeePMD-kit v2: A software package for deep potential
                   models}},
  volume =        {159},
  year =          {2023},
  abstract =      {{DeePMD-kit is a powerful open-source software
                   package that facilitates molecular dynamics
                   simulations using machine learning potentials known
                   as Deep Potential (DP) models. This package, which
                   was released in 2017, has been widely used in the
                   fields of physics, chemistry, biology, and material
                   science for studying atomistic systems. The current
                   version of DeePMD-kit offers numerous advanced
                   features, such as DeepPot-SE, attention-based and
                   hybrid descriptors, the ability to fit tensile
                   properties, type embedding, model deviation, DP-range
                   correction, DP long range, graphics processing unit
                   support for customized operators, model compression,
                   non-von Neumann molecular dynamics, and improved
                   usability, including documentation, compiled binary
                   packages, graphical user interfaces, and application
                   programming interfaces. This article presents an
                   overview of the current major version of the
                   DeePMD-kit package, highlighting its features and
                   technical details. Additionally, this article
                   presents a comprehensive procedure for conducting
                   molecular dynamics as a representative application,
                   benchmarks the accuracy and efficiency of different
                   models, and discusses ongoing developments.}},
  doi =           {10.1063/5.0155600},
  issn =          {0021-9606},
  url =           {https://doi.org/10.1063/5.0155600},
}

@article{AcunLanger2016-power,
  author =        {Acun, Bilge and Langer, Akhil and Meneses, Esteban and
                   Menon, Harshitha and Sarood, Osman and Totoni, Ehsan and
                   Kalé, Laxmikant V.},
  journal =       {Computer},
  number =        {10},
  pages =         {30-37},
  title =         {{Power, Reliability, and Performance: One System to
                   Rule them All}},
  volume =        {49},
  year =          {2016},
  doi =           {10.1109/MC.2016.310},
}

@inproceedings{chasapis2016runtime,
  author =        {Chasapis, Dimitrios and Casas, Marc and
                   Moret\'{o}, Miquel and Schulz, Martin and
                   Ayguad\'{e}, Eduard and Labarta, Jesus and
                   Valero, Mateo},
  booktitle =     {{Proceedings of the 2016 International Conference on
                   Supercomputing}},
  series =        {ICS '16},
  title =         {{Runtime-Guided Mitigation of Manufacturing
                   Variability in Power-Constrained Multi-Socket NUMA
                   Nodes}},
  year =          {2016},
}

@inproceedings{ChasapisMoreto2019-powerEfficJobSched,
  author =        {Chasapis, Dimitrios and Moret\'{o}, Miquel and
                   Schulz, Martin and Rountree, Barry and Valero, Mateo and
                   Casas, Marc},
  booktitle =     {{Proceedings of the ACM International Conference on
                   Supercomputing}},
  pages =         {296–307},
  series =        {ICS '19},
  title =         {{Power Efficient Job Scheduling by Predicting the
                   Impact of Processor Manufacturing Variability}},
  year =          {2019},
  abstract =      {Modern CPUs suffer from performance and power
                   consumption variability due to the manufacturing
                   process. As a result, systems that do not consider
                   such variability caused by manufacturing issues lead
                   to performance degradations and wasted power. In
                   order to avoid such negative impact, users and system
                   administrators must actively counteract any
                   manufacturing variability.In this work we show that
                   parallel systems benefit from taking into account the
                   consequences of manufacturing variability when making
                   scheduling decisions at the job scheduler level. We
                   also show that it is possible to predict the impact
                   of this variability on specific applications by using
                   variability-aware power prediction models. Based on
                   these power models, we propose two job scheduling
                   policies that consider the effects of manufacturing
                   variability for each application and that ensure that
                   power consumption stays under a system-wide power
                   budget. We evaluate our policies under different
                   power budgets and traffic scenarios, consisting of
                   both single- and multi-node parallel applications,
                   utilizing up to 4096 cores in total. We demonstrate
                   that they decrease job turnaround time, compared to
                   contemporary scheduling policies used on production
                   clusters, up to 31\% while saving up to 5.5\%
                   energy.},
  doi =           {10.1145/3330345.3330372},
  isbn =          {9781450360791},
  url =           {https://doi.org/10.1145/3330345.3330372},
}

@inproceedings{InadomiPatki2015-scVar,
  author =        {Inadomi, Yuichi and Patki, Tapasya and Inoue, Koji and
                   Aoyagi, Mutsumi and Rountree, Barry and
                   Schulz, Martin and Lowenthal, David and
                   Wada, Yasutaka and Fukazawa, Keiichiro and
                   Ueda, Masatsugu and Kondo, Masaaki and Miyoshi, Ikuo},
  booktitle =     {{Proceedings of the International Conference for High
                   Performance Computing, Networking, Storage and
                   Analysis}},
  series =        {SC},
  title =         {{Analyzing and Mitigating the Impact of Manufacturing
                   Variability in Power-Constrained Supercomputing}},
  year =          {2015},
  abstract =      {A key challenge in next-generation supercomputing is
                   to effectively schedule limited power resources.
                   Modern processors suffer from increasingly large
                   power variations due to the chip manufacturing
                   process. These variations lead to power inhomogeneity
                   in current systems and manifest into performance
                   inhomogeneity in power constrained environments,
                   drastically limiting supercomputing performance. We
                   present a first-of-its-kind study on manufacturing
                   variability on four production HPC systems spanning
                   four microarchitectures, analyze its impact on HPC
                   applications, and propose a novel variation-aware
                   power budgeting scheme to maximize effective
                   application performance. Our low-cost and scalable
                   budgeting algorithm strives to achieve performance
                   homogeneity under a power constraint by deriving
                   application-specific, module-level power allocations.
                   Experimental results using a 1,920 socket system show
                   up to 5.4X speedup, with an average speedup of 1.8X
                   across all benchmarks when compared to a
                   variation-unaware power allocation scheme.},
  doi =           {10.1145/2807591.2807638},
  isbn =          {9781450337236},
  url =           {https://doi.org/10.1145/2807591.2807638},
}

@inproceedings{PatelWagenhauser2020-hpcPowerConsump,
  author =        {Patel, Tirthak and Wagenhäuser, Adam and
                   Eibel, Christopher and Hönig, Timo and
                   Zeiser, Thomas and Tiwari, Devesh},
  booktitle =     {{IEEE International Parallel and Distributed
                   Processing Symposium}},
  pages =         {799-809},
  series =        {IPDPS},
  title =         {{What does Power Consumption Behavior of HPC Jobs
                   Reveal? : Demystifying, Quantifying, and Predicting
                   Power Consumption Characteristics}},
  year =          {2020},
  doi =           {10.1109/IPDPS47924.2020.00087},
}

@inproceedings{SkinnerKramer2005-perfVarCauses,
  author =        {Skinner, D. and Kramer, W.},
  booktitle =     {{Proceedings of the IEEE Workload Characterization
                   Symposium}},
  pages =         {137-149},
  series =        {IISWC},
  title =         {{Understanding the causes of performance variability
                   in HPC workloads}},
  year =          {2005},
  doi =           {10.1109/IISWC.2005.1526010},
}

@inproceedings{Scogland2015-pwrPerspectives,
  address =       {New York, NY, USA},
  author =        {Scogland, Thomas and Azose, Jonathan and Rohr, David and
                   Rivoire, Suzanne and Bates, Natalie and
                   Hackenberg, Daniel},
  booktitle =     {{Proceedings of the International Conference for High
                   Performance Computing, Networking, Storage and
                   Analysis}},
  publisher =     {Association for Computing Machinery},
  series =        {SC '15},
  title =         {{Node Variability in Large-Scale Power Measurements:
                   Perspectives from the Green500, Top500 and EEHPCWG}},
  year =          {2015},
  abstract =      {The last decade has seen power consumption move from
                   an afterthought to the foremost design constraint of
                   new supercomputers. Measuring the power of a
                   supercomputer can be a daunting proposition, and as a
                   result, many published measurements are extrapolated.
                   This paper explores the validity of these
                   extrapolations in the context of inter-node power
                   variability and power variations over time within a
                   run. We characterize power variability across nodes
                   in systems at eight supercomputer centers across the
                   globe. This characterization shows that the current
                   requirement for measurements submitted to the
                   Green500 and others is insufficient, allowing
                   variations of up to 20\% due to measurement timing
                   and a further 10--15\% due to insufficient sample
                   sizes. This paper proposes new power and energy
                   measurement requirements for supercomputers, some of
                   which have been accepted for use by the Green500 and
                   Top500, to ensure consistent accuracy.},
  doi =           {10.1145/2807591.2807653},
  isbn =          {9781450337236},
  url =           {https://doi.org/10.1145/2807591.2807653},
}

@inproceedings{DeBardeleben-LBNL-EuroPar13,
  author =        {Nathan DeBardeleben and Sean Blanchard and
                   Laura Monroe and Philip Romero and Daryl Grunau and
                   Craig Idler and Cornell Wright},
  booktitle =     {Euro-Par 2013: Parallel Processing Workshops -
                   BigDataCloud, DIHC, FedICI, HeteroPar, HiBB, LSDVE,
                   MHPC, OMHI, PADABS, PROPER, Resilience, ROME, and
                   {UCHPC} 2013, Aachen, Germany, August 26-27, 2013.
                   Revised Selected Papers},
  editor =        {Dieter an Mey and Michael Alexander and
                   Paolo Bientinesi and Mario Cannataro and
                   Carsten Clauss and Alexandru Costan and
                   Gabor Kecskemeti and Christine Morin and Laura Ricci and
                   Julio Sahuquillo and Martin Schulz and
                   Vittorio Scarano and Stephen L. Scott and
                   Josef Weidendorfer},
  pages =         {680--689},
  publisher =     {Springer},
  series =        {Lecture Notes in Computer Science},
  title =         {{{GPU} Behavior on a Large {HPC} Cluster}},
  volume =        {8374},
  year =          {2013},
  biburl =        {https://dblp.org/rec/conf/europar/DeBardelebenBMRGIW13.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  doi =           {10.1007/978-3-642-54420-0\_66},
  timestamp =     {Wed, 19 Feb 2020 14:52:57 +0100},
  url =           {https://doi.org/10.1007/978-3-642-54420-0_66},
}

@article{DeSensiDeMatteis2022-cloudPerfVar,
  address =       {New York, NY, USA},
  author =        {De Sensi, Daniele and De Matteis, Tiziano and
                   Taranov, Konstantin and Di Girolamo, Salvatore and
                   Rahn, Tobias and Hoefler, Torsten},
  month =         dec,
  number =        {3},
  publisher =     {Association for Computing Machinery},
  title =         {{Noise in the Clouds: Influence of Network
                   Performance Variability on Application Scalability}},
  volume =        {6},
  year =          {2022},
  doi =           {10.1145/3570609},
  url =           {https://doi.org/10.1145/3570609},
}

@article{Fraternali-EEHPCVar-2018,
  author =        {Fraternali, Francesco and Bartolini, Andrea and
                   Cavazzoni, Carlo and Benini, Luca},
  journal =       {IEEE Transactions on Parallel and Distributed
                   Systems},
  number =        {7},
  pages =         {1575-1588},
  title =         {{Quantifying the Impact of Variability and
                   Heterogeneity on the Energy Efficiency for a
                   Next-Generation Ultra-Green Supercomputer}},
  volume =        {29},
  year =          {2018},
  doi =           {10.1109/TPDS.2017.2766151},
}

@inproceedings{SencanKulkarni2025-gpuHPCUtil,
  address =       {New York, NY, USA},
  author =        {Sencan, Efe and Kulkarni, Dhruva and Coskun, Ayse and
                   Konate, Kadidia},
  booktitle =     {{Practice and Experience in Advanced Research
                   Computing 2025: The Power of Collaboration}},
  publisher =     {Association for Computing Machinery},
  series =        {PEARC},
  title =         {{Analyzing GPU Utilization in HPC Workloads: Insights
                   from Large-Scale Systems}},
  year =          {2025},
  abstract =      {Efficient resource utilization is a critical yet
                   challenging problem in high-performance computing
                   (HPC) systems, particularly in GPU-accelerated
                   workloads. GPUs have become indispensable in modern
                   datacenters, powering applications in AI, scientific
                   computing, and large-scale simulations, but their
                   potential is often hindered by imbalances in resource
                   allocation and utilization. These inefficiencies
                   across GPUs and nodes can lead to significant
                   performance degradation and energy wastage. While
                   existing works analyze resource utilization at a
                   coarse level, they often fail to capture the
                   intricate temporal and spatial imbalances present in
                   multi-node GPU jobs. In this work, we analyze GPU
                   jobs executed on the Perlmutter supercomputer, a
                   GPU-accelerated system at the National Energy
                   Research Scientific Computing Center (NERSC), using
                   telemetry data from one month of operation in 2024.
                   We identify inefficiencies in how resources are
                   allocated and utilized across GPU nodes and GPUs
                   within nodes. We propose novel methodologies to
                   quantify these imbalances using refined metrics that
                   capture both spatial and temporal variations in
                   resource utilization. Our analysis reveals that GPU
                   utilization is generally well-balanced temporally for
                   most jobs, with no significant temporal imbalances
                   observed. However, GPU memory utilization remains
                   consistently low across many jobs, highlighting
                   opportunities to optimize resource allocation and
                   improve memory usage efficiency.},
  doi =           {10.1145/3708035.3736010},
  isbn =          {9798400713989},
  url =           {https://doi.org/10.1145/3708035.3736010},
}

@inproceedings{sinha2022notall,
  author =        {Sinha, Prasoon and Guliani, Akhil and Jain, Rutwik and
                   Tran, Brandon and Sinclair, Matthew D and
                   Venkataraman, Shivaram},
  booktitle =     {{Proceedings of the International Conference on High
                   Performance Computing, Networking, Storage and
                   Analysis}},
  pages =         {1--15},
  series =        {SC},
  title =         {{Not All GPUs Are Created Equal: Characterizing
                   Variability in Large-Scale, Accelerator-Rich
                   Systems}},
  year =          {2022},
}

@article{TopcuKarabacak2025-gpuPowerPerfVar,
  author =        {Topcu, Burak and Karabacak, Deniz and Oz, Isil},
  journal =       {{Computer Science and Information Systems}},
  number =        {2},
  pages =         {533-561},
  title =         {{Demystifying Power and Performance Variations in GPU
                   Systems through Microarchitectural Analysis}},
  volume =        {22},
  year =          {2025},
  doi =           {https://doi.org/10.2298/CSIS240722021T},
}

@inproceedings{YouXuan2024-gvarp,
  author =        {You, Xin and Xuan, Zhibo and Yang, Hailong and
                   Luan, Zhongzhi and Liu, Yi and Qian, Depei},
  booktitle =     {{Proceedings of the International Conference for High
                   Performance Computing, Networking, Storage, and
                   Analysis}},
  series =        {SC},
  title =         {{GVARP: Detecting Performance Variance on Large-Scale
                   Heterogeneous System}},
  year =          {2024},
}

@inproceedings{ZhongSultanov2025-uncorePowerWaste,
  author =        {Zhong, Zheng and Sultanov, Seyfal and Papka, Michael and
                   Lan, Zhiling},
  booktitle =     {{Proceedings of the International Conference on High
                   Performance Computing, Networking, Storage and
                   Analysis}},
  series =        {SC},
  title =         {{Minimizing Power Waste in Heterogenous Computing via
                   Adaptive Uncore Scaling}},
  year =          {2025},
}

@inproceedings{stanzione2020frontera,
  author =        {Stanzione, Dan and West, John and Evans, R Todd and
                   Minyard, Tommy and Ghattas, Omar and
                   Panda, Dhabaleswar K},
  booktitle =     {Practice and Experience in Advanced Research
                   Computing},
  pages =         {106--111},
  title =         {Frontera: The Evolution of Leadership Computing at
                   the {National Science Foundation}},
  year =          {2020},
}

@inproceedings{paszke2017-pytorch,
  author =        {Paszke, Adam and Gross, Sam and Chintala, Soumith and
                   Chanan, Gregory and Yang, Edward and DeVito, Zachary and
                   Lin, Zeming and Desmaison, Alban and Antiga, Luca and
                   Lerer, Adam},
  booktitle =     {NIPS-W},
  title =         {{Automatic differentiation in PyTorch}},
  year =          {2017},
}

@inproceedings{JainTran2024-pal,
  author =        {Jain, Rutwik and Tran, Brandon and Chen, Keting and
                   Sinclair, Matthew D. and Venkataraman, Shivaram},
  booktitle =     {{Proceedings of the International Conference for High
                   Performance Computing, Networking, Storage, and
                   Analysis}},
  month =         {November},
  series =        {SC},
  title =         {{PAL: A Variability-Aware Policy for Scheduling ML
                   Workloads in GPU Clusters}},
  year =          {2024},
}

@article{Guerreiro-appClasses,
  author =        {João Guerreiro and Aleksandar Ilic and Nuno Roma and
                   Pedro Tomás},
  journal =       {Parallel Computing},
  pages =         {93-117},
  title =         {{DVFS-aware application classification to improve
                   GPGPUs energy efficiency}},
  volume =        {83},
  year =          {2019},
  doi =           {https://doi.org/10.1016/j.parco.2018.02.001},
  issn =          {0167-8191},
  url =           {https://www.sciencedirect.com/science/article/pii/
                  S0167819118300243},
}

@article{kaplan2020scaling,
  author =        {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and
                   Brown, Tom B. and Chess, Benjamin and Child, Rewon and
                   Gray, Scott and Radford, Alec and Wu, Jeffrey and
                   Amodei, Dario},
  journal =       {arXiv preprint arXiv:2001.08361},
  title =         {{Scaling Laws for Neural Language Models}},
  year =          {2020},
  url =           {https://arxiv.org/abs/2001.08361},
}

@article{WECEnergy,
  author =        {{World Economic Forum}},
  journal =       {NA},
  month =         jul,
  note =          {(Accessed on 09/07/2025)},
  title =         {AI and energy: Will AI help reduce emissions or
                   increase power demand? Here's what to know},
  year =          {2024},
  url =           {https://www.weforum.org/stories/2024/07/generative-ai-
                  energy-emissions/},
}

@article{patterson2021carbon,
  author =        {Patterson, David and Gonzalez, Joseph and Le, Quoc V. and
                   Liang, Chen and Munguia, Lluis and Rothchild, Daniel and
                   So, David R. and Texier, Maud and Dean, Jeff},
  journal =       {arXiv preprint arXiv:2104.10350},
  title =         {{The Carbon Footprint of Large Neural Network
                   Training}},
  year =          {2021},
  url =           {https://arxiv.org/abs/2104.10350},
}

@inproceedings{BlandRogers2009-jaguar,
  author =        {Bland, Arthur S. and Rogers II, James H and
                   Kendall, Ricky A and Kothe, Douglas B and
                   Shipman, Galen M},
  booktitle =     {{35th Cray User Group Meeting}},
  month =         {5},
  series =        {CUG},
  title =         {{Jaguar: The World's Most Powerful Computer}},
  year =          {2009},
}

@inproceedings{Bland2012-titan,
  author =        {Bland, Buddy},
  booktitle =     {{SC Companion: High Performance Computing, Networking
                   Storage and Analysis}},
  number =        {},
  pages =         {2189-2211},
  title =         {{Titan - Early experience with the Titan system at
                   Oak Ridge National Laboratory}},
  volume =        {},
  year =          {2012},
  doi =           {10.1109/SC.Companion.2012.356},
}

@article{WombleShankar2019-summit,
  author =        {Womble, D. E. and Shankar, M. and Joubert, W. and
                   Johnston, J. T. and Wells, J. C. and Nichols, J. A.},
  journal =       {{IBM Journal of Research and Development}},
  number =        {6},
  pages =         {2:1-2:9},
  title =         {{Early Experiences on Summit: Data Analytics and AI
                   Applications}},
  volume =        {63},
  year =          {2019},
  doi =           {10.1147/JRD.2019.2944146},
}

@inproceedings{AtchleyZimmer2023-frontier,
  author =        {Atchley, Scott and Zimmer, Chris and Lange, John R. and
                   Bernholdt, David E. and Melesse Vergara, Verónica G. and
                   Beck, Thomas and Brim, Michael J. and
                   Budiardja, Reuben and Chandrasekaran, Sunita and
                   Eisenbach, Markus and Evans, Thomas and
                   Ezell, Matthew and Frontiere, Nicholas and
                   Georgiadou, Antigoni and Glenski, Joe and
                   Grete, Philipp and Hamilton, Steven and Holmen, John and
                   Huebl, Axel and Jacobson, Daniel and Joubert, Wayne and
                   McMahon, Kim and Merzari, Elia and Moore, Stan G and
                   Myers, Andrew and Nichols, Stephen and Oral, Sarp and
                   Papatheodore, Thomas and Perez, Danny and
                   Rogers, David M. and Schneider, Evan and
                   Vay, Jean-Luc and Yeung, P.K.},
  booktitle =     {{International Conference for High Performance
                   Computing, Networking, Storage and Analysis}},
  number =        {},
  pages =         {1-16},
  series =        {SC},
  title =         {{Frontier: Exploring Exascale The System Architecture
                   of the First Exascale Supercomputer}},
  volume =        {},
  year =          {2023},
  doi =           {10.1145/3581784.3607089},
}

@inproceedings{Scogland11Green500,
  author =        {Scogland, Thomas R. W. and Subramaniam, Balaji and
                   Feng, Wu{-}Chun},
  booktitle =     {Proc.\ IEEE IPDPS Workshops},
  pages =         {889--895},
  title =         {{Emerging Trends on the Evolving Green500: Year
                   Three}},
  year =          {2011},
  doi =           {10.1109/IPDPS.2011.229},
}

@inproceedings{Tschand24MLPerfPower,
  author =        {Tschand, Arya and Rajan, Arun Tejusve Raghunath and
                   Idgunji, Sachin and Ghosh, Anirban and
                   Holleman, Jeremy and Kiraly, Csaba and
                   Ambalkar, Pawan and Borkar, Ritika and Chukka, Ramesh and
                   Cockrell, Trevor and Curtis, Oliver and
                   Fursin, Grigori and Hodak, Miro and Kassa, Hiwot and
                   Lokhmotov, Anton and Miskovic, Dejan and Pan, Yuechao and
                   Manmathan, Manu Prasad and Raymond, Liz and
                   John, Tom St. and Suresh, Arjun and Taubitz, Rowan and
                   Zhan, Sean and Wasson, Scott and Kanter, David and
                   Reddi, Vijay Janapa},
  booktitle =     {{IEEE International Symposium on High Performance
                   Computer Architecture}},
  number =        {},
  pages =         {1201-1216},
  series =        {HPCA},
  title =         {{MLPerf Power: Benchmarking the Energy Efficiency of
                   Machine Learning Systems from $\mu$Watts to MWatts
                   for Sustainable AI}},
  volume =        {},
  year =          {2025},
  doi =           {10.1109/HPCA61900.2025.00092},
}

@inproceedings{cosmoflow2019,
  author =        {Prabhat and others},
  booktitle =     {Proceedings of the International Conference for High
                   Performance Computing, Networking, Storage and
                   Analysis (SC19)},
  note =          {Includes CosmoFlow‑Power joules/epoch data},
  title =         {Scaling {CosmoFlow} to ~15,000 {GPUs} and achieving
                   {43} PFLOPS},
  year =          {2019},
  doi =           {10.1145/3295500.3356175},
}

@article{hacc2020power,
  author =        {Heitmann, Katrin and others},
  journal =       {Computing in Science \& Engineering},
  note =          {Adds HACC Energy Add‑on joules/particle metric},
  title =         {The HACC Framework: Energy and Performance
                   Characterization},
  year =          {2020},
  doi =           {10.1109/MCSE.2020.3033659},
}

@inproceedings{deepcam2020power,
  author =        {Kurth, Thorsten and others},
  booktitle =     {International Conference for High Performance
                   Computing (SC20)},
  note =          {DeepCAM‑Energy joules/epoch results},
  title =         {Exascale Deep Learning for Climate Analytics},
  year =          {2020},
  doi =           {10.5555/3433701.3433712},
}

@techreport{openifsenergy2023,
  author =        {Wedi, Nils and others},
  institution =   {ECMWF},
  note =          {kWh per model‑day for full weather physics},
  title =         {OpenIFS Energy Benchmark Report},
  year =          {2023},
  url =           {https://www.ecmwf.int/en/publications/openifs/energy-
                  benchmark},
}

@inproceedings{gromacsee2024,
  author =        {P\'{a}ll, Szil\'{a}rd and others},
  booktitle =     {GPU Technology Conference (GTC)},
  note =          {Introduces Joules/ns metric},
  title =         {GROMACS-EE: Energy‑Efficient Molecular Dynamics on
                   GPUs},
  year =          {2024},
  url =           {https://developer.nvidia.com/gtc},
}

@article{namdpower2019,
  author =        {Rodriguez, A. and others},
  journal =       {Journal of Computational Chemistry},
  note =          {Energy‑Delay Product results for ApoA1},
  title =         {Energy Delay Product Optimization of NAMD on Summit},
  year =          {2019},
  doi =           {10.1002/jcc.25785},
}

@inproceedings{qeenergy2022,
  author =        {Giannozzi, Paolo and others},
  booktitle =     {International Workshop on Performance Modeling,
                   Benchmarking and Simulation of High Performance
                   Computing Systems (PMBS)},
  title =         {Energy‑Aware Quantum ESPRESSO: Joules per SCF Step},
  year =          {2022},
  url =           {https://ieeexplore.ieee.org/document/9955431},
}

@techreport{vasppower2023,
  author =        {Kresse, Georg and others},
  institution =   {Vienna University of Technology},
  title =         {VASP Power Harness: Energy Profiling of DFT MD},
  year =          {2023},
  url =           {https://vasp.at/energy-harness},
}

@inproceedings{openfoamenergy2021,
  author =        {Jain, R. and others},
  booktitle =     {Workshop on Energy Efficient Supercomputing},
  title =         {Characterizing Energy Consumption of OpenFOAM on
                   Modern HPC Systems},
  year =          {2021},
  doi =           {10.1145/3489059.3494181},
}

@article{insarpower2024,
  author =        {Farr, Tom and others},
  journal =       {IEEE Journal of Selected Topics in Applied Earth
                   Observations},
  note =          {Joules per satellite scene},
  title =         {InSAR-AI: Power Characterization of Satellite Image
                   Unwrapping},
  year =          {2024},
  doi =           {10.1109/JSTARS.2024.1234567},
}

@inproceedings{h3denergy2023,
  author =        {Fox, Geoffrey and others},
  booktitle =     {International Conference on Computational Science
                   (ICCS)},
  note =          {Joules per timestep metric},
  title =         {H3D: Hydrology 3D Energy Benchmark},
  year =          {2023},
  url =           {https://iccs2023.org},
}

@inproceedings{KhairyShen2021-accelSim,
  author =        {Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor M. and
                   Rogers, Timothy G.},
  booktitle =     {{2020 ACM/IEEE 47th Annual International Symposium on
                   Computer Architecture}},
  pages =         {473-486},
  series =        {ISCA},
  title =         {{Accel-Sim: An Extensible Simulation Framework for
                   Validated GPU Modeling}},
  year =          {2020},
  doi =           {10.1109/ISCA45697.2020.00047},
}

@article{binkert2011gem5,
  author =        {Binkert, Nathan and Beckmann, Bradford and
                   Black, Gabriel and Reinhardt, Steven K. and
                   Saidi, Ali and Basu, Arkaprava and Hestness, Joel and
                   Hower, Derek R. and Krishna, Tushar and
                   Sardashti, Somayeh and Sen, Rathijit and
                   Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and
                   Hill, Mark D. and Wood, David A.},
  journal =       {ACM SIGARCH Computer Architecture News},
  number =        {2},
  pages =         {1--7},
  publisher =     {ACM},
  title =         {The gem5 simulator},
  volume =        {39},
  year =          {2011},
}

@article{LowePowerAhmad2020-gem520,
  author =        {Jason Lowe-Power and Abdul Mutaal Ahmad and
                   Ayaz Akram and Mohammad Alian and Rico Amslinger and
                   Matteo Andreozzi and Adrià Armejach and
                   Nils Asmussen and Srikant Bharadwaj and Gabe Black and
                   Gedare Bloom and Bobby R. Bruce and
                   Daniel Rodrigues Carvalho and Jeronimo Castrillon and
                   Lizhong Chen and Nicolas Derumigny and
                   Stephan Diestelhorst and Wendy Elsasser and
                   Marjan Fariborz and Amin Farmahini-Farahani and
                   Pouya Fotouhi and Ryan Gambord and Jayneel Gandhi and
                   Dibakar Gope and Thomas Grass and Bagus Hanindhito and
                   Andreas Hansson and Swapnil Haria and Austin Harris and
                   Timothy Hayes and Adrian Herrera and Matthew Horsnell and
                   Syed Ali Raza Jafri and Radhika Jagtap and
                   Hanhwi Jang and Reiley Jeyapaul and Timothy M. Jones and
                   Matthias Jung and Subash Kannoth and
                   Hamidreza Khaleghzadeh and Yuetsu Kodama and
                   Tushar Krishna and Tommaso Marinelli and
                   Christian Menard and Andrea Mondelli and Tiago Mück and
                   Omar Naji and Krishnendra Nathella and Hoa Nguyen and
                   Nikos Nikoleris and Lena E. Olson and Marc Orr and
                   Binh Pham and Pablo Prieto and Trivikram Reddy and
                   Alec Roelke and Mahyar Samani and Andreas Sandberg and
                   Javier Setoain and Boris Shingarov and
                   Matthew D. Sinclair and Tuan Ta and Rahul Thakur and
                   Giacomo Travaglini and Michael Upton and Nilay Vaish and
                   Ilias Vougioukas and Zhengrong Wang and Norbert Wehn and
                   Christian Weis and David A. Wood and Hongil Yoon and
                   Éder F. Zulian},
  howpublished =  {arXiv},
  journal =       {CoRR},
  title =         {The gem5 Simulator: Version 20.0+},
  volume =        {abs/2007.03152},
  year =          {2020},
}

@article{RodriguesHemmert2011-sst,
  address =       {New York, NY, USA},
  author =        {Rodrigues, A. F. and Hemmert, K. S. and
                   Barrett, B. W. and Kersey, C. and Oldfield, R. and
                   Weston, M. and Risen, R. and Cook, J. and
                   Rosenfeld, P. and Cooper-Balis, E. and Jacob, B.},
  journal =       {SIGMETRICS Perform. Eval. Rev.},
  month =         mar,
  number =        {4},
  pages =         {37–42},
  publisher =     {Association for Computing Machinery},
  title =         {{The Structural Simulation Toolkit}},
  volume =        {38},
  year =          {2011},
  abstract =      {As supercomputers grow, understanding their behavior
                   and performance has become increasingly challenging.
                   New hurdles in scalability, programmability, power
                   consumption, reliability, cost, and cooling are
                   emerging, along with new technologies such as 3D
                   integration, GP-GPUs, silicon-photonics, and other
                   "game changers". Currently, they HPC community lacks
                   a unified toolset to evaluate these technologies and
                   design for these challenges.To address this problem,
                   a number of institutions have joined together to
                   create the Structural Simulation Toolkit (SST), an
                   open, modular, parallel, multi-criteria, multi-scale
                   simulation framework. The SST includes a number of
                   processor, memory, and network models. The SST has
                   been used in a variety of network, memory, and
                   application studies and aims to become the standard
                   simulation framework for designing and procuring HPC
                   systems.},
  doi =           {10.1145/1964218.1964225},
  issn =          {0163-5999},
  url =           {https://doi.org/10.1145/1964218.1964225},
}

@article{SST,
  author =        {Nema, Shubham and Razdan, Rohin and Rodrigues, Arun and
                   Hemmert, Karl and Voskuilen, Gwendolyn and
                   Adak, Debratim and Hammond, Simon and Awad, Amro and
                   Hughes, Clayton},
  journal =       {Sandia National Labs Tech Report},
  month =         {9},
  number =        {},
  title =         {{ERAS: Enabling the Integration of Real-World
                   Intellectual Properties (IPs) in Architectural
                   Simulators}},
  volume =        {},
  year =          {2021},
  doi =           {10.2172/1854734},
  url =           {https://www.osti.gov/biblio/1854734},
}

@inproceedings{brewer2024digital,
  author =        {Brewer, Wesley and Maiterth, Matthias and
                   Kumar, Vineet and Wojda, Rafal and Bouknight, Sedrick and
                   Hines, Jesse and Shin, Woong and Greenwood, Scott and
                   Grant, David and Williams, Wesley and Wang, Feiyi},
  booktitle =     {Proceedings of the International Conference for High
                   Performance Computing, Networking, Storage and
                   Analysis ({SC})},
  title =         {A digital twin framework for liquid-cooled
                   supercomputers as demonstrated at exascale},
  year =          {2024},
}

@inproceedings{BakhodaYuan2009-gpgpuSim,
  author =        {Bakhoda, Ali and Yuan, George L. and
                   Fung, Wilson W. L. and Wong, Henry and
                   Aamodt, Tor M.},
  booktitle =     {{2009 IEEE International Symposium on Performance
                   Analysis of Systems and Software}},
  month =         {April},
  pages =         {163-174},
  series =        {ISPASS},
  title =         {{Analyzing CUDA workloads using a detailed GPU
                   simulator}},
  year =          {2009},
  doi =           {10.1109/ISPASS.2009.4919648},
  issn =          {null},
}

@inproceedings{KandiahPeverelle2021-accelWattch,
  author =        {Kandiah, Vijay and Peverelle, Scott and
                   Khairy, Mahmoud and Manjunath, Amogh and Pan, Junrui and
                   Rogers, Timothy G. and Aamodt, Tor M. and
                   Hardavellas, Nikos},
  booktitle =     {{Proceedings of the 54th IEEE/ACM International
                   Symposium on Microarchitecture}},
  month =         {October},
  series =        {MICRO},
  title =         {{AccelWattch: A Power Modeling Framework for Modern
                   GPUs}},
  year =          {2021},
}

@article{LewShah2018-gpgpusimML,
  author =        {Jonathan Lew and Deval Shah and Suchita Pati and
                   Shaylin Cattell and Mengchi Zhang and
                   Amruth Sandhupatla and Christopher Ng and Negar Goli and
                   Matthew D. Sinclair and Timothy G. Rogers and
                   Tor M. Aamodt},
  howpublished =  {arXiv},
  journal =       {CoRR},
  title =         {{Analyzing Machine Learning Workloads Using a
                   Detailed GPU Simulator}},
  volume =        {abs/1811.08933},
  year =          {2018},
  biburl =        {https://dblp.org/rec/bib/journals/corr/abs-1811-08933},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  timestamp =     {Fri, 30 Nov 2018 12:44:28 +0100},
  url =           {http://arxiv.org/abs/1811.08933},
}

@inproceedings{AvalosKhairy2021-pka,
  address =       {New York, NY, USA},
  author =        {Avalos Baddouh, Cesar and Khairy, Mahmoud and
                   Green, Roland N. and Payer, Mathias and
                   Rogers, Timothy G.},
  booktitle =     {{54th Annual IEEE/ACM International Symposium on
                   Microarchitecture}},
  pages =         {724–737},
  publisher =     {Association for Computing Machinery},
  series =        {MICRO '21},
  title =         {{Principal Kernel Analysis: A Tractable Methodology
                   to Simulate Scaled GPU Workloads}},
  year =          {2021},
  abstract =      {Simulating all threads in a scaled GPU workload
                   results in prohibitive simulation cost. Cycle-level
                   simulation is orders of magnitude slower than native
                   silicon, the only solution is to reduce the amount of
                   work simulated while accurately representing the
                   program. Existing solutions to simulate GPU programs
                   either scale the input size, simulate the first
                   several billion instructions, or simulate a portion
                   of both the GPU and the workload. These solutions
                   lack validation against scaled systems, produce
                   unrealistic contention conditions and frequently miss
                   critical code sections. Existing CPU sampling
                   mechanisms, like SimPoint, reduce per-thread
                   workload, and are ill-suited to GPU programs where
                   reducing the number of threads is critical. Sampling
                   solutions on GPUs space lack silicon validation,
                   require per-workload parameter tuning, and do not
                   scale. A tractable solution, validated on
                   contemporary scaled workloads, is needed to provide
                   credible simulation results. By studying scaled
                   workloads with centuries-long simulation times, we
                   uncover practical and algorithmic limitations of
                   existing solutions and propose Principal Kernel
                   Analysis: a hierarchical program sampling methodology
                   that concisely represents GPU programs by selecting
                   representative kernel portions using a scalable
                   profiling methodology, tractable clustering algorithm
                   and detection of intra-kernel IPC stability. We
                   validate Principal Kernel Analysis across 147
                   workloads and three GPU generations using the
                   Accel-Sim simulator, demonstrating a better
                   performance/error tradeoff than prior work and that
                   century-long MLPerf simulations are reduced to hours
                   with an average cycle error of 27\% versus silicon.},
  doi =           {10.1145/3466752.3480100},
  isbn =          {9781450385572},
  url =           {https://doi.org/10.1145/3466752.3480100},
}

@inproceedings{GutierrezBeckmann2018-gem5GPU,
  author =        {Gutierrez, Anthony and Beckmann, Bradford M. and
                   Dutu, Alexandru and Gross, Joseph and
                   LeBeane, Michael and Kalamatianos, John and
                   Kayiran, Onur and Poremba, Matthew and
                   Potter, Brandon and Puthoor, Sooraj and
                   Sinclair, Matthew D. and Wyse, Michael and
                   Yin, Jieming and Zhang, Xianwei and Jain, Akshay and
                   Rogers, Timothy},
  booktitle =     {{24th IEEE International Symposium on High
                   Performance Computer Architecture}},
  month =         {Feb},
  pages =         {608-619},
  series =        {HPCA},
  title =         {{Lost in Abstraction: Pitfalls of Analyzing GPUs at
                   the Intermediate Language Level}},
  year =          {2018},
  doi =           {10.1109/HPCA.2018.00058},
  issn =          {2378-203X},
}

@inproceedings{RogersSlycord2020-gem5Salam,
  author =        {Rogers, Samuel and Slycord, Joshua and
                   Baharani, Mohammadreza and Tabkhi, Hamed},
  booktitle =     {{53rd Annual IEEE/ACM International Symposium on
                   Microarchitecture}},
  number =        {},
  pages =         {471-482},
  series =        {MICRO},
  title =         {{gem5-SALAM: A System Architecture for LLVM-based
                   Accelerator Modeling}},
  volume =        {},
  year =          {2020},
  doi =           {10.1109/MICRO50266.2020.00047},
}

@article{SpencerRogers2024-gem5Salam2,
  author =        {Zephaniah Spencer and Samuel Rogers and
                   Joshua Slycord and Hamed Tabkhi},
  journal =       {{Journal of Systems Architecture}},
  pages =         {103211},
  title =         {{Expanding Hardware Accelerator System Design Space
                   Exploration with gem5-SALAMv2}},
  volume =        {154},
  year =          {2024},
  doi =           {https://doi.org/10.1016/j.sysarc.2024.103211},
  issn =          {1383-7621},
  url =           {https://www.sciencedirect.com/science/article/pii/
                  S1383762124001486},
}

@inproceedings{ChaudhariSinclair2025-gem5Accel,
  author =        {Chaudhari, Akanksha and Sinclair, Matthew D.},
  booktitle =     {6th gem5 Users' Workshop},
  month =         {6},
  title =         {{Toward Full-System Heterogeneous Simulation: Merging
                   gem5-SALAM with Mainline gem5}},
  year =          {2025},
}

@inproceedings{SmithBruce2024-gem5Power,
  author =        {Smith, Alex and Bruce, Bobby and Lowe-Power, Jason and
                   Sinclair, Matthew D.},
  booktitle =     {{3rd Open-Source Computer Architecture Research
                   Workshop}},
  series =        {OSCAR},
  title =         {{Designing Generalizable Power Models For Open-Source
                   Architecture Simulators}},
  year =          {2024},
}

@inproceedings{RamadasPoremba2023-gem5GPUFS,
  author =        {Ramadas, Vishnu and Poremba, Matthew and
                   Beckmann, Bradford M. and Sinclair, Matthew D.},
  booktitle =     {{The 5th gem5 Users’ Workshop}},
  month =         {6},
  title =         {{Improving gem5’s GPU FS Support}},
  year =          {2023},
}

@inproceedings{RamadasPoremba2024-gem5MLSim,
  author =        {Ramadas, Vishnu and Poremba, Matthew and
                   Beckmann, Bradford M. and Sinclair, Matthew D.},
  booktitle =     {{3rd Open-Source Computer Architecture Research
                   Workshop}},
  series =        {OSCAR},
  title =         {{Simulation Support for Fast and Accurate Large-Scale
                   GPGPU and Accelerator Workloads}},
  year =          {2024},
}

@inproceedings{RamadasSinclair2024-gem5MLSim,
  author =        {Ramadas, Vishnu and Sinclair, Matthew D.},
  booktitle =     {{SRC TECHCON}},
  month =         {9},
  title =         {{Simulating Machine Learning Models at Scale}},
  year =          {2024},
}

@article{sandia_2,
  author =        {Hughes, Clayton and Hammond, Simon David and
                   Hoekstra, Robert J. and Zhang, Mengchi and
                   Liu, Yechen and Rogers, Tim},
  journal =       {Sandia National Lab},
  month =         {1},
  title =         {{SST-GPU: A Scalable SST GPU Component for
                   Performance Modeling and Profiling}},
  year =          {2021},
  doi =           {10.2172/1762830},
  url =           {https://www.osti.gov/biblio/1762830},
}

@article{sandia_3,
  author =        {Hughes, Clayton and Hammond, Simon David and
                   Khairy, Mahmoud and Zhang, Mengchi and Green, Roland and
                   Rogers, Timothy and Hoekstra, Robert J.},
  journal =       {Sandia National Lab},
  month =         {9},
  title =         {{Balar: A SST GPU Component for Performance Modeling
                   and Profiling}},
  year =          {2019},
  doi =           {10.2172/1560919},
  url =           {https://www.osti.gov/biblio/1560919},
}

@inproceedings{hsieh2012gem5sst,
  address =       {Brussels, BEL},
  author =        {Hsieh, Mingyu and Pedretti, Kevin and Meng, Jie and
                   Coskun, Ayse and Levenhagen, Michael and
                   Rodrigues, Arun},
  booktitle =     {{Proceedings of the 5th International ICST Conference
                   on Simulation Tools and Techniques}},
  pages =         {196–201},
  publisher =     {ICST (Institute for Computer Sciences,
                   Social-Informatics and Telecommunications
                   Engineering)},
  series =        {SIMUTOOLS '12},
  title =         {{SST + Gem5 = a Scalable Simulation Infrastructure
                   for High Performance Computing}},
  year =          {2012},
  isbn =          {9781450315104},
}

@inproceedings{nguyen2022gem5sst,
  author =        {Hoa Nguyen and Jason Lowe-Power},
  booktitle =     {The 4th gem5 Users’ Workshop with ISCA},
  title =         {{gem5/SST Integration 2021: Scaling Full-system
                   Simulations}},
  year =          {2022},
}

@inproceedings{maiterth2025hpc,
  author =        {Maiterth, Matthias and Brewer, Wesley H. and
                   Kuruvella, Jaya S. and Dey, Arunavo and
                   Islam, Tanzima Z. and Menear, Kevin and
                   Duplyakin, Dmitry and Kabir, Rashadul and
                   Patki, Tapasya and Jones, Terry and others},
  booktitle =     {SC25-W: Workshops of the International Conference for
                   High Performance Computing, Networking, Storage and
                   Analysis},
  organization =  {IEEE},
  title =         {{HPC} Digital Twins for Evaluating Scheduling
                   Policies, Incentive Structures and their Impact on
                   Power and Cooling},
  year =          {2025},
}

@inproceedings{brewer2025trace,
  author =        {Brewer, Wesley and Maiterth, Matthias and
                   Fay, Damien},
  booktitle =     {2025 IEEE High Performance Extreme Computing
                   Conference (HPEC)},
  organization =  {IEEE},
  title =         {Trace Replay Simulation of {MIT SuperCloud} for
                   Studying Optimal Sustainability Policies},
  year =          {2025},
}

@inproceedings{kalepu2025virtual,
  author =        {Kalepu, Srishti and Brewer, Wesley H. and
                   Maiterth, Matthias and Vuduc, Richard},
  booktitle =     {2025 IEEE High Performance Extreme Computing
                   Conference (HPEC)},
  organization =  {IEEE},
  title =         {Virtual Benchmarking for {HPC} Systems Using
                   {ExaDigiT} and {Calculon}},
  year =          {2025},
}

