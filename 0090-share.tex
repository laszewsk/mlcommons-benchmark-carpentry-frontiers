\section{Sharing Benchmarks}
\label{sec:share}

Beyond the creation of new benchmarks, \textit{sharing} benchmakrs is an essential aspect of benchmark carpentry. To this end, integrating the  FAIR principles is of paramount importance.

Benchmark sharing is best supported through hosting the code in a public repository that provides well-documented, executable workflows, thereby enabling others to reproduce the benchmark and compare results. Standard development practices, such as using Python Notebooks or scripts in other programming languages, as well as standard libraries, are recommended. More complex benchmarks may benefit from formal build processes (e.g., using makefiles) and dependency management through package managers. Containerization offers additional advantages, simplifying configuration and improving portability across environments.

To further support FAIRness, benchmark results should include standardized metadata, facilitating consistent comparison and analysis across studies.

While existing platforms such as Hugging Face and Kaggle provide mechanisms for sharing benchmarks, results, and leaderboards, fostering community capacity to host them independently remains valuable. Initiatives such as MLCommons illustrate how communities can maintain open, transparent benchmarking ecosystems. Educational efforts could be developed to train researchers and practitioners in these practices.

Finally, with the growing prominence of agentic AI, it is worth exploring its potential for automating the benchmarking lifecycleâ€”including benchmark execution, result generation, and report synthesis. For example, the MLCommons Science Working Group is investigating how agentic AI can be applied to scientific benchmarks, particularly those involving time series analysis.