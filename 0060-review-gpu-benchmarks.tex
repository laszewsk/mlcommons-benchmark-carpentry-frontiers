\subsection{GPU Benchmarking and its Variability}
\label{sec:gpu}

%\subsubsection{Performance Variability}
%\label{sec:gpu-var}

% Measurement variability: often much lower for GPUs (Matt Sinclair can help with this)

Modern scientific applications frequently require peta- or exascale levels of compute to model topics with high fidelity. To meet these demands in reasonable timeframes, scientists and researchers typically run these workloads on massively parallel systems such as GPUs. For example, workloads such as graph analytics~\cite{CheBeckmann2013,WangPan2017-gunrock}, scientific computing~\cite{coral2,olcf6-bmks, kim2018qmcpack, WuTaylor2019-candle}, ML~\cite{BanburyReddi2021-tinyMLPerf,BaruahShivdikar2021-gnnMark,DongKaeli2017-dnnmark,Narang2017-deepBench,MattsonCheng2019-mlperfTrain,MattsonReddi2020-mlPerf,Reddi2020mlperf-Infer,ReddiCheng2021-mlPerfVision} heavily utilize GPUs.
Increasingly, ML is also impacting scientific applications~\cite{fan2021predicting,jumper2021highly,kates2019predicting,ThiyagalingamShankar2022-mlSci,ThiyagalingamVonLaszewski2022-aiForSciMLCommons} by replacing or supplementing traditional computing methods in application domains like molecular dynamics (e.g., DeePMD~\cite{WangZhang2018-deepmd,ZengZhang2023-deepmd2}), protein folding (e.g., OpenFold2~\cite{openfold2}), and scientific AI models (e.g., AuroraGPT~\cite{Stevens2023-auroraGPT}).
However, given the scale of data these workloads operate on and the large size of the workloads themselves, they typically must partition their work across many GPUs.

% why this matters a lot
Given their widespread use and trend towards many GPU applications, it is desirable from a benchmark carpentry perspective to make GPU experiments repeatable and consistent.
% variability trends
For traditional HPC systems composed of multiple CPUs, prior work showed that this was difficult to achieve: application performance varied by up to 20\%, even for CPUs with the same architecture and vendor SKU (Stock-Keeping Unit)~\cite{AcunLanger2016-power,chasapis2016runtime, ChasapisMoreto2019-powerEfficJobSched, InadomiPatki2015-scVar, PatelWagenhauser2020-hpcPowerConsump, SkinnerKramer2005-perfVarCauses}. This variation occurs due to the manufacturing process and the chip's power constraints~\cite{ChasapisMoreto2019-powerEfficJobSched,Scogland2015-pwrPerspectives}.
Such dynamic behavior makes it challenging for %applications to achieve
repeatable experiments, and can lead to resource underutilization.
Unfortunately, similar issues also arise in modern systems composed of many GPUs. Recent work has demonstrated that GPU-rich systems suffer from significant performance variability~\cite{DeBardeleben-LBNL-EuroPar13, DeSensiDeMatteis2022-cloudPerfVar, Fraternali-EEHPCVar-2018, Scogland2015-pwrPerspectives, SencanKulkarni2025-gpuHPCUtil, sinha2022notall, TopcuKarabacak2025-gpuPowerPerfVar, YouXuan2024-gvarp, ZhongSultanov2025-uncorePowerWaste}.

For example, Sinha, et al. examined variability across five modern GPU-rich clusters with a variety of sizes, cooling approaches, and GPU vendors~\cite{sinha2022notall}.
They found that applications exhibited performance variability of 8\% on average (max 22\%) with outliers up to 1.5$\times$ slower than the median GPU.
Moreover, these results were consistent over time (i.e., not transient) and were unaffected by GPU vendors or cooling type.
Interestingly, this performance variability was also application-specific: the more compute-intensive the application was, the more performance variability the application observed due to effects of the GPU's power management algorithm (e.g., Dynamic Voltage \& Frequency Scaling---DVFS).
%This performance variability is application-specific, not vendor specific, not cluster size specific, not resolved by cooling, not transient
%Gets worse as transistors scale, especially for more compute-intensive workloads.
Furthermore, performance variability is getting worse as transistors continue scaling~\cite{DRAMthermalissues}.

Although the impact of performance variability is significant for single-GPU workloads, it is even larger for multi-GPU workloads.
Currently, GPU-rich systems focus on scheduling work to minimize the number of nodes an application requests, without considering variability.
In the five clusters from this prior work, users asking for 4 GPUs for a given application would get a slower GPU allocated to them between 22\% (Sandia's Vortex cluster) and 50\% (TACC's Longhorn cluster~\cite{stanzione2020frontera, tacc}) of the time.
Thus, users are likely to get a slow GPU frequently, especially since modern scientific workloads often request 64 or more GPUs for a given experiment.
This can lead to significant resource under-utilization for multi-GPU jobs since all of them must wait for the slowest one to complete due to the bulk synchronous programming (BSP) model used in many data-parallel workloads~\cite{paszke2017-pytorch}.
Accordingly, it is imperative for users to be aware of the impact of performance variability on their experiments, and for benchmark carpentry to propose solutions to minimize its effects.

% what can users do from carpentry perspective
Although GPU-rich systems are likely to suffer from performance variability for the foreseeable future, there are several steps various stakeholders, such as users, maintainers, and system designers, can take to reduce the impact on obtaining statistically significant results in existing systems.
First, cluster operators can perform periodic performance-variability benchmarking to identify underperforming GPUs and perform targeted maintenance on them. Likewise, users can perform similar benchmarking to identify GPUs that behave similarly, and then use blacklisting or other scheduling approaches to attempt to schedule work on GPUs with similar performance variability profiles. However, doing so can be time- and labor-intensive for clusters with thousands or more GPUs (though it is a one-time cost, since a GPU's performance variability is consistent over time).
Thus, a more scalable, dynamic approach is to redesign job-scheduling policies for GPU clusters to account for performance variability when making scheduling decisions. Recent work has shown that embracing performance variability can transparently and significantly improve job completion time, makespan, and GPU utilization~\cite{JainTran2024-pal}.
Finally, since performance variability is application-specific, we recommend that new, unprofiled applications either rely on other applications with similar profiles as proxies ~\cite{Guerreiro-appClasses} or be profiled during their first execution on a new cluster to determine their sensitivity to performance variability.

In terms of democratizing the availability of multi-GPU systems there are several barriers to overcome, these are the cost, access, skills and complexity The cost barrier means that the large-scale systems are affordable only to national labs and major corporations. Consequently, the access is usually restricted to the staff of these organizations. Using multi‑GPU systems effectively requires specialized knowledge. Users must be trained in containerization technologies, distributed libraries, and orchestration tools that allow applications to scale across many GPUs. There is also a barrier on the conceptual level. The performance of a multi‑GPU system is the result of interactions between hardware, interconnects, and software stacks. At present, we lack high‑level performance prediction model that can reliably describe how applications behave when running on GPUs. This makes it difficult to plan experiments, determine the required resources and generalize findings. 

% before running final results