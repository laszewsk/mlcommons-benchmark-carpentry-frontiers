\begin{abstract}
 
Benchmarks are one cornerstone of modern machine learning practice, providing standardized evaluations that enable reproducibility, comparison, and scientific progress. 
However, AI benchmarks are becoming increasingly complex, requiring special care, including AI focused dynamic workflows. This is evident by the rapid evolution of 
AI models in architecture, scale, and capability; the evolution of datasets; and deployment contexts continuously change, creating a moving target for evaluation.
Large language models in particular are known for their memorization of static benchmarks, which causes a drastic difference between benchmark results and real-world performance.
Beyond the accepted static benchmarks we know from the traditional computing community, we need to develop and evolve continuous adaptive benchmarking frameworks, as scientific assessment is increasingly misaligned with real-world deployment risks.
This requires the development of skills and education focused on benchmarks in the scientific community: {\em AI Benchmark Carpentry}. 

Drawing on our experience from MLCommons, educational initiatives, and government programs such as the DOE's Trillion Parameter Consortium, we identify key barriers that hinder the broader adoption, utility, and evolution of benchmarking in AI. These include substantial resource demands, limited access to specialized hardware, lack of expertise in benchmark design, and uncertainty among practitioners about how to relate benchmark results to their own application domains. Moreover, current benchmarks often emphasize peak performance on leadership-class hardware, offering limited guidance for more diverse, real-world deployment scenarios. This may include applications to smaller compute resources, but also to larger systems such as LLMs deployed by commercial entities.

We argue that benchmarking itself must become dynamic in order to incorporate evolving models, updated data, and heterogeneous computational platforms while maintaining transparency, reproducibility, and interpretability. Democratizing this process requires not only technical innovation, but also systematic educational efforts as part of AI benchmark carpentry offerings, spanning undergraduate to professional levels, in order to develop sustained expertise in benchmark design and use. Finally, benchmarks should be framed and used to support application-relevant comparisons, enabling both developers and users to make informed, context-sensitive decisions. Advancing dynamic and inclusive benchmarking practices will be essential to ensure that evaluation keeps pace with the evolving AI landscape and supports responsible, reproducible, and accessible AI deployment. Furthermore, we believe that it is timely to provide a solid foundation for designing, using, and evolving benchmarks through community efforts that allows us to enable the concept of {\em AI benchmark carpentry.}

\begin{comment}
\TODO{OLD: In this document we describe the importance of benchmarks in relationship to AI and especially deep learning. Our experience is gained from working with MLCommons but also while working in educational settings as well as government settings such as the million parameter consortium headed by DOE. What we observed is that in many cases benchmarking in this domain is significantly more challenging due to a number of reasons. This includes the scale in time and space of the benchmarks, the expertise and access to hardware to run them, and the lack of experience in benchmarks in general. On the other hand it is often not yet clear to scientists how to use the benchmarks or apply them to their own applications. Therefore, efforts need to be projected to democratize such benchmark endeavors to make the more available and transparent to the users. Preparedness to use and develop such benchmarks must be accompanied by educational activities starting from the undergraduate level to the professional developers. The goal is to develop and contribute to repeatable benchmarks that can be adopted to a wide variety of applications with various scales in algorithmic complexity but also hardware requirements. The benchmarks must be communicated and collected in a fashion allowing meaningful comparisons for application users that go beyond the demonstration of them on a leadership class hardware platform.}
\end{comment}

\end{abstract}

%\twocolumn

\begin{IEEEkeywords}
benchmark, AI benchmark, AI benchmark carpentry, AI benchmark democratization, MLCommons
\end{IEEEkeywords}

\bigskip
