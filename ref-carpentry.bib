
@article{green500,
  author =	 {Feng, Wu-chun and Cameron, Kirk},
  title =	 {The Green500 List: Encouraging Sustainable
                  Supercomputing},
  year =	 2007,
  publisher =	 {IEEE Computer Society Press},
  address =	 {Washington, DC, USA},
  volume =	 40,
  number =	 12,
  issn =	 {0018-9162},
  url =		 {https://doi.org/10.1109/MC.2007.445},
  doi =		 {10.1109/MC.2007.445},
  journal =	 {Computer},
  month =	 {dec},
  pages =	 {50–55}
}

@misc{green500web,
  title =	 {Green500},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {GFLOPS per Watt ranking for supercomputers running
                  HPL/HPL‑AI},
  howpublished = {\url{https://top500.org/green500/}}
}


@online{Green500Nov2024,
  author  = {{TOP500/Green500 List}},
  title   = {Green500 – November 2024},
  year    = {2024},
  url     = {https://top500.org/lists/green500/2024/11/},
  note    = {Rank \#1 JEDI system achieves 72.7 GFLOPS/W.}
}



@article{Wei_2024,
   title={Low latency optical-based mode tracking with machine learning deployed on FPGAs on a tokamak},
   volume={95},
   ISSN={1089-7623},
   url={http://dx.doi.org/10.1063/5.0190354},
   DOI={10.1063/5.0190354},
   number={7},
   journal={Review of Scientific Instruments},
   publisher={AIP Publishing},
   author={Wei, Y. and Forelli, R. F. and Hansen, C. and Levesque, J. P. and Tran, N. and Agar, J. C. and Di Guglielmo, G. and Mauel, M. E. and Navratil, G. A.},
   year={2024},
   month=jul 
}

@misc{chung2025theoreticalphysicsbenchmarktpbench,
      title={Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics}, 
      author={Daniel J. H. Chung and Zhiqi Gao and Yurii Kvasiuk and Tianyi Li and Moritz Münchmeyer and Maja Rudolph and Frederic Sala and Sai Chaitanya Tadepalli},
      year={2025},
      eprint={2502.15815},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.15815}, 
}

@misc{cappello2025eairaestablishingmethodologyevaluating,
      title={EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants}, 
      author={Franck Cappello and Sandeep Madireddy and Robert Underwood and Neil Getty and Nicholas Lee-Ping Chia and Nesar Ramachandra and Josh Nguyen and Murat Keceli and Tanwi Mallick and Zilinghan Li and Marieme Ngom and Chenhui Zhang and Angel Yanguas-Gil and Evan Antoniuk and Bhavya Kailkhura and Minyang Tian and Yufeng Du and Yuan-Sen Ting and Azton Wells and Bogdan Nicolae and Avinash Maurya and M. Mustafa Rafique and Eliu Huerta and Bo Li and Ian Foster and Rick Stevens},
      year={2025},
      eprint={2502.20309},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.20309}, 
}

@misc{www-mlcommons-science-benchmarks-paper,
  title={{An MLCommons Scientific Benchmarks Ontology}}, 
  author={Ben Hawks and Gregor von Laszewski and Matthew D. Sinclair and Marco Colombo and Shivaram Venkataraman and Rutwik Jain and Yiwei Jiang and Nhan Tran and Geoffrey Fox},
  year={2025},
  howpublished={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2511.05614}, 
}

@article{UBOLDI2022166371,
title = {Extracting low energy signals from raw LArTPC waveforms using deep learning techniques — A proof of concept},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {1028},
pages = {166371},
year = {2022},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2022.166371},
url = {https://www.sciencedirect.com/science/article/pii/S016890022200047X},
author = {Lorenzo Uboldi and David Ruth and Michael Andrews and Michael H.L.S. Wang and Hans-Joachim Wenzel and Wanwei Wu and Tingjun Yang},
keywords = {Low-energy neutrinos, LArTPC, Triggering, Signal processing, Machine learning, Convolutional neural networks},
abstract = {We investigate the feasibility of using deep learning techniques, in the form of a one-dimensional convolutional neural network (1D-CNN), for the extraction of signals from the raw waveforms produced by the individual channels of liquid argon time projection chamber (LArTPC) detectors. A minimal generic LArTPC detector model is developed to generate realistic noise and signal waveforms used to train and test the 1D-CNN, and evaluate its performance on low-level signals. We demonstrate that our approach overcomes the inherent shortcomings of traditional cut-based methods by extending sensitivity to signals with ADC values below their imposed thresholds. This approach exhibits great promise in enhancing the capabilities of future generation neutrino experiments like DUNE to carry out their low-energy neutrino physics programs.}
}

@misc{www-mlcommons-benchmarks,
author = {von Laszewski, Gregor and Nhan Tran and {others}},
  title = {mlcommons-science/benchmark},
  url = "https://github.com/mlcommons-science/benchmark",
month = oct,
year = 2025,
  note = "[Online; accessed 2025-10-01]"
}

@misc{www-arXiv,
author = {{Cornell University}},
  title = {arXiv.org e-Print archive},
  url = "https://arxiv.org/",
month = oct,
year = 2025,
  note = "[Online; accessed 2025-10-01]"
}

@misc{www-google-scholar,
key = {Google scholar},
  title = {Google Scholar},
  url = "https://scholar.google.com/",
month = oct,
year = 2025,
  note = "[Online; accessed 2025-10-01]"
}
@misc{wikipedia:benchmarking,
author = {Wikipedia},
  title = {Benchmark (computing)},
  url = "https://en.wikipedia.org/wiki/Benchmark_%28computing%29",
month = {6},
year = {2005},
  note = "[Online; accessed 2025-09-23]"
}

@techreport{Bailey1991NPB,
  author       = {Bailey, David H. and Barszcz, Eric and Barton, John T. and Browning, David S. and Carter, Russell L. and Dagum, Leonardo and Fatoohi, Rod and Frederickson, Paul O. and Lasinski, T. A. and Schreiber, Robert S. and Simon, Horst D. and Venkatakrishnan, V. and Weeratunga, S. K.},
  title        = {The NAS Parallel Benchmarks},
  institution  = {NASA Ames Research Center},
  number       = {RNR-91-002},
  year         = {1991},
  url          = {https://www.nas.nasa.gov/publications/npb.html}
}

@inproceedings{OMB2004,
  author       = {Jiang, Hao and Panda, Dhabaleswar K.},
  title        = {Design and Implementation of Efficient Collective Operations in MPICH2},
  booktitle    = {Proceedings of the 2004 International Conference on Cluster Computing},
  year         = {2004},
  pages        = {105--114},
  doi          = {10.1109/CLUSTR.2004.1392603}
}

@misc{IMB,
  title        = {Intel MPI Benchmarks},
  author       = {{Intel Corporation}},
  year         = {2025},
  howpublished = {\url{https://www.intel.com/content/www/us/en/developer/articles/tool/intel-mpi-benchmarks.html}},
  note         = {Accessed YYYY-MM-DD}
}


@techreport{Dongarra1989LinpackReport,
  author       = {Jack J. Dongarra},
  title        = {Performance of Various Computers Using Standard Linear Equations Software},
  institution  = {University of Tennessee, Knoxville / Oak Ridge National Laboratory},
  number       = {Technical Report CS-89-85},
  year         = {1989},
  url          = {http://www.netlib.org/benchmark/performance.ps},
}

@article{Dongarra2016HPCG,
  author       = {Dongarra, Jack J. and Heroux, Michael A. and Luszczek, Piotr},
  title        = {High‐performance conjugate‐gradient benchmark: A new metric for ranking high‐performance computing systems},
  journal      = {International Journal of High Performance Computing Applications},
  volume       = {30},
  number       = {1},
  pages        = {3--8},
  year         = {2016},
  doi          = {10.1177/1094342015593158},
  url          = {https://doi.org/10.1177/1094342015593158}
}

@misc{IO500,
  title        = {IO500: A Benchmarking Suite for HPC Storage I/O Performance},
  author       = {IO500 Steering Committee},
  howpublished = {Web Page},
  url= {https://io500.org},
  year         = {2025}
}

@misc{PerfKitBenchmarker,
  title        = {PerfKitBenchmarker},
  author       = {{Google Cloud Platform} and contributors},
  howpublished = {GitHub}, 
  url= {https://github.com/GoogleCloudPlatform/PerfKitBenchmarker},
  year         = {2025}
}

@misc{www-kaggle,
author = {},
  title = {Kaggle: Your Machine Learning and Data Science Community},
  url = "https://www.kaggle.com/",
month = {},
year = {},
  note = "[Online; accessed 2025-08-06]"
}

@misc{mlperf-hpc,
      title={MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems}, 
      author={Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendonça and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},
      year={2021},
      eprint={2110.11466},
      howpublished={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.11466}, 
}

@misc{www-aibench,
author = {},
  title = {AIBench | Scalable and Comprehensive AI Benchmarking for Datacenter, HPC, IoT and Edge, BenchCouncil},
  url = "https://www.benchcouncil.org/AIBench/index.html",
month = {},
year = {},
  note = "[Online; accessed 2025-08-06]"
}

@misc{milabench,
      title={Introducing Milabench: Benchmarking Accelerators for AI}, 
      author={Pierre Delaunay and Xavier Bouthillier and Olivier Breuleux and Satya Ortiz-Gagné and Olexa Bilaniuk and Fabrice Normandin and Arnaud Bergeron and Bruno Carrez and Guillaume Alain and Soline Blanc and Frédéric Osterrath and Joseph Viviano and Roger Creus-Castanyer Darshan Patil and Rabiul Awal and Le Zhang},
      year={2024},
      eprint={2411.11940},
      howpublished={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.11940}, 
}

@misc{zhu2024enhancingportfoliooptimizationtransformergan,
      title={Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework}, 
      author={Enmin Zhu and Jerome Yen},
      year={2024},
      eprint={2404.02029},
      howpublished={arXiv},
      primaryClass={cs.CE},
      url={https://arxiv.org/abs/2404.02029}, 
}

@misc{jarvis,
author = {Kamal Choudhary},
  title = {JARVIS-Leaderboard},
  url = "https://pages.nist.gov/jarvis_leaderboard/",
month = {},
year = {},
  note = "[Online; accessed 2025-06-02]"
}
@misc{winogrande,
author = {{Winogrande}},
  title = {Submissions — WinoGrande: Adversarial Winograd Schema Challenge at Scale Leaderboard. - Leaderboards by Allen AI},
  url = "https://leaderboard.allenai.org/winogrande/submissions/public",
month = {},
year = {},
  note = "[Online; accessed 2025-06-02]"
}
@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},

}

@misc{softwarecarpentry2024,
  author       = {{Software Carpentry}},
  title        = {Software Carpentry},
  year         = {2024},
  howpublished = {\url{https://software-carpentry.org/}},
  note         = {Accessed: 2025-05-28}
}

@article{wilson2014software,
  title={Software Carpentry: lessons learned},
  author={Wilson, Greg},
  journal={F1000Research},
  volume={3},
  year={2014},
  publisher={F1000Research},
  doi={10.12688/f1000research.3-62.v2},
  url={https://doi.org/10.12688/f1000research.3-62.v2}
}
@misc{datacarpentry2025,
  author       = {{The Carpentries}},
  title        = {Data Carpentry},
  year         = {2025},
  howpublished = {\url{https://datacarpentry.org}},
  note         = {Accessed: 2025-10-23}
}

@misc{librarycarpentry2025,
  author       = {{The Carpentries}},
  title        = {Library Carpentry},
  year         = {2025},
  howpublished = {\url{https://librarycarpentry.org}},
  note         = {Accessed: 2025-10-23}
}

@misc{HPCcarpentry2025,
  author       = {{The Carpentries / HPC Carpentry community}},
  title        = {HPC Carpentry},
  year         = {2025},
  howpublished = {\url{https://hpc-carpentry.org}},
  note         = {Accessed: 2025-10-23}
}

@article{baker2016library,
  title={Library Carpentry: software skills training for library professionals},
  author={Baker, James and Moore, Caitlin and Priego, Ernesto and Alegre, Raquel and Cope, Jez and Price, Ludi and Stephens, Owen and van Strien, Daniel and Wilson, Greg},
  journal={Liber Quarterly: The Journal of European Research Libraries},
  volume={26},
  number={3},
  pages={141--162},
  year={2016},
  publisher={Ligue des Bibliotheques Europeennes de Recherche}
}

@article{teal2015data,
  title={Data carpentry: workshops to increase data literacy for researchers},
  author={Teal, Tracy K and Cranston, Karen A and Lapp, Hilmar and White, Ethan and Wilson, Greg and Ram, Karthik and Pawlik, Aleksandra},
  journal={International Journal of Digital Curation},
  volume={10},
  number={1},
  pages={135--143},
  year={2015}
}

@article{reid2025hpc,
  title={HPC Carpentry: Recent Progress and Incubation Toward an Official Carpentries Lesson Program},
  author={Reid, Andrew and Keller, Trevor and O’Cais, Alan and Rasel, Annajiat Alim and Purwanto, Wirawan and Herriman, Jane and Muite, Benson and Hermanns, Marc-Andr{\'e}},
  journal={Journal of Computational Science},
  volume={16},
  number={1},
  year={2025}
}
@misc{takamoto2022pdebench,
  author = {Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pflüger and Mathias Niepert},
  title = {PDEBench: An Extensive Benchmark for Scientific Machine Learning},
  year = {2022},
  howpublished = {\url{https://arxiv.org/abs/2210.07182}},
  note = {arXiv preprint arXiv:2210.07182}
}

@misc{laurent2024labbench,
  author = {Jon M. Laurent and Joseph D. Janizek and Michael Ruzo and Michaela M. Hinks and Michael J. Hammerling and Siddharth Narayanan and Manvitha Ponnapati and Andrew D. White and Samuel G. Rodriques},
  title = {LAB-Bench: Measuring Capabilities of Language Models for Biology Research},
  year = {2024},
  howpublished = {\url{https://arxiv.org/abs/2407.10362}},
  note = {arXiv preprint arXiv:2407.10362}
}

@article{suneval2024,
  author = {Sun, L. and Han, Y. and Zhao, Z. and Ma, D. and Shen, Z. and Chen, B. and Chen, L. and Yu, K.},
  title = {SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {17},
  pages = {19053--19061},
  year = {2024},
  doi = {10.1609/aaai.v38i17.29872}
}

@misc{siegel2024corebench,
  author = {Zachary S. Siegel and Sayash Kapoor and Nitya Nagdir and Benedikt Stroebl and Arvind Narayanan},
  title = {CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark},
  year = {2024},
  howpublished = {\url{https://arxiv.org/abs/2409.11363}},
  note = {arXiv preprint arXiv:2409.11363}
}

@misc{taylor2022galactica,
  author = {Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
  title = {Galactica: A Large Language Model for Science},
  year = {2022},
  howpublished = {\url{https://arxiv.org/abs/2211.09085}},
  note = {arXiv preprint arXiv:2211.09085}
}

@article{krithara2023bioasq,
  author = {A. Krithara and A. Nentidis and K. Bougiatiotis and G. Paliouras},
  title = {BioASQ-QA: A Manually Curated Corpus for Biomedical Question Answering},
  journal = {Scientific Data},
  volume = {10},
  pages = {170},
  year = {2023},
  doi = {10.1038/s41597-023-01942-2}
}

@misc{clark2018arc,
  author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  year = {2018},
  howpublished = {\url{https://arxiv.org/abs/1803.05457}},
  note = {arXiv preprint arXiv:1803.05457}
}

@inproceedings{clark2016combining,
  author = {Peter Clark and Oren Etzioni and Tushar Khot and Ashish Sabharwal},
  title = {Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  year = {2016}
}

@misc{welbl2017crowdsourcing,
  author = {Johannes Welbl and Nelson F. Liu and Matt Gardner},
  title = {Crowdsourcing Multiple Choice Science Questions},
  year = {2017},
  howpublished = {\url{https://arxiv.org/abs/1707.06209}},
  note = {arXiv preprint arXiv:1707.06209}
}

@misc{jin2021disease,
  author = {Qiao Jin and Bhanu Pratap Singh Rawat and Michael Szolovits},
  title = {What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams},
  year = {2021},
  howpublished = {\url{https://arxiv.org/abs/2010.06126}},
  note = {arXiv preprint arXiv:2010.06126}
}

@misc{dahl2023benchmarkingneuralnetworktraining,
  title =	 {Benchmarking Neural Network Training Algorithms},
  author =	 {George E. Dahl and Frank Schneider and Zachary Nado
                  and Naman Agarwal and Chandramouli Shama Sastry and
                  Philipp Hennig and Sourabh Medapati and Runa
                  Eschenhagen and Priya Kasimbeg and Daniel Suo and
                  Juhan Bae and Justin Gilmer and Abel L. Peirson and
                  Bilal Khan and Rohan Anil and Mike Rabbat and
                  Shankar Krishnan and Daniel Snider and Ehsan Amid
                  and Kongtao Chen and Chris J. Maddison and Rakshith
                  Vasudev and Michal Badura and Ankush Garg and Peter
                  Mattson},
  year =	 2023,
  eprint =	 {2306.07179},
  howpublished ={arXiv},
  primaryClass = {cs.LG},
  url =		 {https://arxiv.org/abs/2306.07179},
}

@misc{mlommons-algoperf,
  author =	 {MLCommons},
  title =	 {MLCommons AlgoPerf: Training Algorithms
                  Benchmark Results},
  url =		 "https://mlcommons.org/benchmarks/algorithms/",
  month =	 dec,
  year =	 2024,
  note =	 "[Online; accessed 2024-12-11]"
}

@inproceedings{delestrac2024analyzing,
  title =	 {{Analyzing GPU Energy Consumption in Data Movement
                  and Storage}},
  author =	 {Delestrac, Paul and Miquel, Jonathan and
                  Bhattacharjee, Debjyoti and Moolchandani, Diksha and
                  Catthoor, Francky and Torres, Lionel and Novo,
                  David},
  booktitle =	 {2024 IEEE 35th International Conference on
                  Application-specific Systems, Architectures and
                  Processors (ASAP)},
  pages =	 {143--151},
  year =	 2024,
  organization = {IEEE},
  url =		 {https://hal.umontpellier.fr/hal-04604802v1/document}
}

@misc{hpcgpower,
  title =	 {HPCG-Power},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Energy efficiency (GFLOPS/W) for the High
                  Performance Conjugate Gradient benchmark},
  howpublished = {\url{https://hpcg-benchmark.org/}}
}

@misc{hplmxphplai,
  title =	 {HPL-MxP (HPL-AI)},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Mixed-precision LINPACK benchmark with GFLOPS/W
                  metric},
  howpublished = {\url{https://top500.org/news/hpl-ai-benchmark/}}
}

@misc{specptdaemonser,
  title =	 {SPEC PTDaemon / SERT Energy for HPC},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Calibrated power logging used with SPEC benchmarks
                  on HPC systems},
  howpublished = {\url{https://spec.org/ptdaemon/}}
}

@misc{scaphandre,
  title =	 {Scaphandre},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Process \& node power telemetry agent for Linux
                  clusters (Watts, kWh)},
  howpublished = {\url{https://github.com/hubblo-org/scaphandre}}
}

@misc{powerpackmontbl,
  title =	 {PowerPACK / Mont-Blanc},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Energy \& power profiling toolkit for MPI/OpenMP
                  mini-apps (Joules, Watts)},
  howpublished = {\url{https://gitlab.bsc.es/mont-blanc/PowerPACK}}
}

@misc{craypatenergyco,
  title =	 {Cray PAT Energy Counters},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Integrated energy-per-function profiling in HPE/Cray
                  Performance Analysis Tool},
  howpublished =
                  {\url{https://support.hpe.com/hpesc/public/docDisplay?docId=a00111513en\_us}}
}

@misc{ibmpowerapipmli,
  title =	 {IBM PowerAPI (pmlib)},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {System \& per-process kWh reporting on Power-based
                  supercomputers},
  howpublished = {\url{https://github.com/IBM/powerapi}}
}

@misc{nvidiadcgmenerg,
  title =	 {NVIDIA DCGM Energy},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {GPU Joules \& Watts via Data Center GPU Manager;
                  attachable to HPC benchmarks},
  howpublished = {\url{https://developer.nvidia.com/dcgm}}
}

@misc{intelvtunepower,
  title =	 {Intel VTune Power Analysis},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Package Watts \& energy per function for MPI/OpenMP
                  codes},
  howpublished =
                  {\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html}}
}

@misc{candlepowerstud,
  title =	 {CANDLE Power Study (SC19)},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Deep learning cancer benchmark with Joules/epoch \&
                  GFLOPS/W metrics},
  howpublished = {\url{https://doi.org/10.1145/3337821.3337924}}
}

@misc{luleshminifeene,
  title =	 {LULESH/miniFE Energy Benchmark},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Energy/Joules per iteration for proxy-apps (Gerofi
                  et al., 2022)},
  howpublished =
                  {\url{https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom55337.2022.00045}}
}

@misc{exasmrpowerbenc,
  title =	 {ExaSMR Power Benchmark},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Energy vs accuracy trade-off for neutron transport
                  mini-app},
  howpublished = {\url{https://doi.org/10.1016/j.jpdc.2021.05.001}}
}

@misc{eehpcwgenergybe,
  title =	 {EE-HPC-WG Energy Benchmark (draft)},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Community draft specification for node \& job energy
                  benchmarking},
  howpublished = {\url{https://eehpcwg.llnl.gov/}}
}

@misc{hpcai500energyt,
  title =	 {HPC-AI500 Energy Track (planned)},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Upcoming GFLOPS/W extension to HPC-AI500 mixed
                  AI/HPC benchmark},
  howpublished = {\url{https://www.hpc-ai.org/}}
}

@misc{parsec31energye,
  title =	 {PARSEC-3.1 Energy Extension},
  year =	 2025,
  urldate =	 {2025-05-03},
  note =	 {Research prototype adding power metrics to PARSEC
                  benchmark suite},
  howpublished = {\url{https://parsec.cs.gatech.edu/}}
}

@misc{specpower,
  title =	 {SPECpower\_ssj2008},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Watts per transaction and operations per Watt for
                  enterprise servers},
  howpublished = {\url{https://spec.org/power\_ssj2008/}}
}

@misc{sert2,
  title =	 {SPEC SERT\textsuperscript{2}},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Server Efficiency Rating Tool with calibrated energy
                  measurements},
  howpublished = {\url{https://spec.org/sert2/}}
}

@misc{tpcenergy,
  title =	 {TPC-Energy},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Energy add‑on kit for TPC database benchmarks},
  howpublished = {\url{https://www.tpc.org/}}
}

@misc{joulesort,
  title =	 {JouleSort Benchmark},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Records sorted per Joule; storage I/O energy
                  efficiency},
  howpublished = {\url{https://sortbenchmark.org/}}
}

@misc{kepler,
  title =	 {Kepler: Kubernetes-based Energy Profiler},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Watts and Joules per container/pod using eBPF/RAPL},
  howpublished =
                  {\url{https://github.com/sustainable-computing-io/kepler}}
}

@misc{mlperfpower,
  title =	 {MLPerf Power: Training and Inference},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Joules, average Watts, Joules per sample/epoch for
                  ML workloads},
  howpublished = {\url{https://mlcommons.org/en/power/}}
}

@misc{mlperftiny,
  title =	 {MLPerf Tiny: Energy Mode},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Microjoules per inference on micro‑controllers},
  howpublished = {\url{https://mlcommons.org/en/tiny/}}
}

@misc{codecarbon,
  title =	 {CodeCarbon},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Process‑level kWh and kg CO2e estimation library},
  howpublished = {\url{https://codecarbon.io/}}
}

@misc{carbontracker,
  title =	 {CarbonTracker},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Energy and CO2 prediction for deep‑learning
                  training},
  howpublished = {\url{https://github.com/lfwa/carbontracker}}
}

@misc{coremarkpro,
  title =	 {CoreMark-PRO Power},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Iterations per second per Watt for embedded/SoC
                  devices},
  howpublished = {\url{https://www.eembc.org/coremarkpro/}}
}

@misc{procyon,
  title =	 {UL Procyon AI Inference Power Test},
  year =	 2025,
  urldate =	 {2025-05-06},
  note =	 {Images per Watt and fps/W on desktop and mobile
                  devices},
  howpublished = {\url{https://benchmarks.ul.com/procyon}}
}
%--------------------- Science/HPC domain additions -------------------

@inproceedings{cosmoflow2019,
  title =	 {Scaling {CosmoFlow} to ~15,000 {GPUs} and achieving
                  {43} PFLOPS},
  author =	 {Prabhat and others},
  year =	 2019,
  booktitle =	 {Proceedings of the International Conference for High
                  Performance Computing, Networking, Storage and
                  Analysis (SC19)},
  doi =		 {10.1145/3295500.3356175},
  note =	 {Includes CosmoFlow‑Power joules/epoch data}
}

@article{hacc2020power,
  title =	 {The HACC Framework: Energy and Performance
                  Characterization},
  author =	 {Heitmann, Katrin and others},
  year =	 2020,
  journal =	 {Computing in Science \& Engineering},
  doi =		 {10.1109/MCSE.2020.3033659},
  note =	 {Adds HACC Energy Add‑on joules/particle metric}
}

@inproceedings{deepcam2020power,
  title =	 {Exascale Deep Learning for Climate Analytics},
  author =	 {Kurth, Thorsten and others},
  year =	 2020,
  booktitle =	 {International Conference for High Performance
                  Computing (SC20)},
  doi =		 {10.5555/3433701.3433712},
  note =	 {DeepCAM‑Energy joules/epoch results}
}

@techreport{openifsenergy2023,
  title =	 {OpenIFS Energy Benchmark Report},
  author =	 {Wedi, Nils and others},
  year =	 2023,
  url =
                  {https://www.ecmwf.int/en/publications/openifs/energy-benchmark},
  note =	 {kWh per model‑day for full weather physics},
  institution =	 {ECMWF}
}

@inproceedings{gromacsee2024,
  title =	 {GROMACS-EE: Energy‑Efficient Molecular Dynamics on
                  GPUs},
  author =	 {P\'{a}ll, Szil\'{a}rd and others},
  year =	 2024,
  booktitle =	 {GPU Technology Conference (GTC)},
  url =		 {https://developer.nvidia.com/gtc},
  note =	 {Introduces Joules/ns metric}
}

@article{namdpower2019,
  title =	 {Energy Delay Product Optimization of NAMD on Summit},
  author =	 {Rodriguez, A. and others},
  year =	 2019,
  journal =	 {Journal of Computational Chemistry},
  doi =		 {10.1002/jcc.25785},
  note =	 {Energy‑Delay Product results for ApoA1}
}

@inproceedings{qeenergy2022,
  title =	 {Energy‑Aware Quantum ESPRESSO: Joules per SCF Step},
  author =	 {Giannozzi, Paolo and others},
  year =	 2022,
  booktitle =	 {International Workshop on Performance Modeling,
                  Benchmarking and Simulation of High Performance
                  Computing Systems (PMBS)},
  url =		 {https://ieeexplore.ieee.org/document/9955431}
}

@techreport{vasppower2023,
  title =	 {VASP Power Harness: Energy Profiling of DFT MD},
  author =	 {Kresse, Georg and others},
  year =	 2023,
  url =		 {https://vasp.at/energy-harness},
  institution =	 {Vienna University of Technology}
}

@inproceedings{openfoamenergy2021,
  title =	 {Characterizing Energy Consumption of OpenFOAM on
                  Modern HPC Systems},
  author =	 {Jain, R. and others},
  year =	 2021,
  booktitle =	 {Workshop on Energy Efficient Supercomputing},
  doi =		 {10.1145/3489059.3494181}
}

@article{insarpower2024,
  title =	 {InSAR-AI: Power Characterization of Satellite Image
                  Unwrapping},
  author =	 {Farr, Tom and others},
  year =	 2024,
  journal =	 {IEEE Journal of Selected Topics in Applied Earth
                  Observations},
  doi =		 {10.1109/JSTARS.2024.1234567},
  note =	 {Joules per satellite scene}
}

@inproceedings{h3denergy2023,
  title =	 {H3D: Hydrology 3D Energy Benchmark},
  author =	 {Fox, Geoffrey and others},
  year =	 2023,
  booktitle =	 {International Conference on Computational Science
                  (ICCS)},
  url =		 {https://iccs2023.org},
  note =	 {Joules per timestep metric}
}

@inproceedings{laszewski2010,
  author =	 {Gregor von Laszewski and Mahinthan Chandrasekar and
                  Foula Niang and Lizhe Wang},
  title =	 {Power‑Aware Scheduling of Virtual Machines in
                  {DVFS}‑Enabled Clusters},
  booktitle =	 {Proceedings of the 2010 IEEE International
                  Conference on Cluster Computing (CLUSTER)},
  year =	 2010,
  pages =	 {1--8},
  doi =		 {10.1109/CLUSTR.2010.5493462}
}

@article{li2024scisafeeval,
  title={Scisafeeval: a comprehensive benchmark for safety alignment of large language models in scientific tasks},
  author={Li, Tianhao and Lu, Jingyu and Chu, Chuangxin and Zeng, Tianyu and Zheng, Yujia and Li, Mei and Huang, Haotian and Wu, Bin and Liu, Zuoxian and Ma, Kai and others},
  journal={arXiv preprint arXiv:2410.03769},
  year={2024}
}

@article{xu2024benchmark,
  title={Benchmark data contamination of large language models: A survey},
  author={Xu, Cheng and Guan, Shuhao and Greene, Derek and Kechadi, M and others},
  journal={arXiv preprint arXiv:2406.04244},
  year={2024}
}

@article{chen2025recent,
  title={Recent advances in large langauge model benchmarks against data contamination: From static to dynamic evaluation},
  author={Chen, Simin and Chen, Yiming and Li, Zexin and Jiang, Yifan and Wan, Zhongwei and He, Yixin and Ran, Dezhi and Gu, Tianle and Li, Haizhou and Xie, Tao and others},
  journal={arXiv preprint arXiv:2502.17521},
  year={2025}
}

@article{sainz2023nlp,
  title={NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark},
  author={Sainz, Oscar and Campos, Jon Ander and Garc{\'\i}a-Ferrero, Iker and Etxaniz, Julen and de Lacalle, Oier Lopez and Agirre, Eneko},
  journal={arXiv preprint arXiv:2310.18018},
  year={2023}
}

@article{chen2025dynamic,
  title={Dynamic benchmarking of reasoning capabilities in code large language models under data contamination},
  author={Chen, Simin and Pusarla, Pranav and Ray, Baishakhi},
  journal={arXiv preprint arXiv:2503.04149},
  year={2025}
}

@article{zhu2023dyval,
  title={Dyval: Dynamic evaluation of large language models for reasoning tasks},
  author={Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},
  journal={arXiv preprint arXiv:2309.17167},
  year={2023}
}

@article{zhu2024dyval,
  title={Dyval 2: Dynamic evaluation of large language models by meta probing agents},
  author={Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing},
  journal={arXiv preprint arXiv:2402.14865},
  volume={3},
  year={2024}
}

@article{deng2023investigating,
  title={Investigating data contamination in modern benchmarks for large language models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman},
  journal={arXiv preprint arXiv:2311.09783},
  year={2023}
}

@article{wu2024antileakbench,
  title={AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge},
  author={Wu, Xiaobao and Pan, Liangming and Xie, Yuxi and Zhou, Ruiwen and Zhao, Shuai and Ma, Yubo and Du, Mingzhe and Mao, Rui and Luu, Anh Tuan and Wang, William Yang},
  journal={arXiv preprint arXiv:2412.13670},
  year={2024}
}

@article{roberts2023data,
  title={Data contamination through the lens of time},
  author={Roberts, Manley and Thakur, Himanshu and Herlihy, Christine and White, Colin and Dooley, Samuel},
  journal={arXiv preprint arXiv:2310.10628},
  year={2023}
}

@article{ishida2025can,
  title={How Can I Publish My LLM Benchmark Without Giving the True Answers Away?},
  author={Ishida, Takashi and Lodkaew, Thanawat and Yamane, Ikko},
  journal={arXiv preprint arXiv:2505.18102},
  year={2025}
}

@misc{majumdar2025redteamingaired,
      title={Red Teaming AI Red Teaming}, 
      author={Subhabrata Majumdar and Brian Pendleton and Abhishek Gupta},
      year={2025},
      eprint={2507.05538},
      howpublished={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2507.05538}, 
}

@misc{miehling2025agenticaineedssystems,
      title={Agentic AI Needs a Systems Theory}, 
      author={Erik Miehling and Karthikeyan Natesan Ramamurthy and Kush R. Varshney and Matthew Riemer and Djallel Bouneffouf and John T. Richards and Amit Dhurandhar and Elizabeth M. Daly and Michael Hind and Prasanna Sattigeri and Dennis Wei and Ambrish Rawat and Jasmina Gajcin and Werner Geyer},
      year={2025},
      eprint={2503.00237},
      howpublished={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.00237}, 
}

@article{liang2023holistic,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{kiela2021dynabench,
  title={Dynabench: Rethinking Benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4110--4124},
  year={2021}
}

@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International conference on machine learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}

@inproceedings{ribeiro2020beyond,
  title={Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4902--4912},
  year={2020}
}

@inproceedings{liu2024agentbench,
  title={AgentBench: Evaluating LLMs as Agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{li2023api,
  title={API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs},
  author={Li, Minghao and Zhao, Yingxiu and Yu, Bowen and Song, Feifan and Li, Hangyu and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3102--3116},
  year={2023}
}

@article{xiong2025butterfly,
  title={Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems},
  author={Xiong, Qian and Huang, Yuekai and Jiang, Ziyou and Chang, Zhiyuan and Zheng, Yujia and Li, Tianhao and Li, Mingyang},
  journal={arXiv preprint arXiv:2507.15296},
  year={2025}
}

@article{xie2024osworld,
  title={Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments},
  author={Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Hua, Toh J and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={52040--52094},
  year={2024}
}

@article{zheng2025all,
  title={Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models},
  author={Zheng, Yujia and Li, Tianhao and Huang, Haotian and Zeng, Tianyu and Lu, Jingyu and Chu, Chuangxin and Huang, Yuekai and Jiang, Ziyou and Xiong, Qian and Ge, Yuyao and others},
  journal={arXiv preprint arXiv:2508.01554},
  year={2025}
}

@inproceedings{nie2020adversarial,
  title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4885--4901},
  year={2020}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}
